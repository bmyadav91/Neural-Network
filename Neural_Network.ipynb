{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Introduction to Deep Learning Assignment questions."
      ],
      "metadata": {
        "id": "YQ-av3HL433o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Explain what deep learning is and discuss its significance in the broader field of artificial intelligence.\n",
        "# Ans: Deep learning is a subset of machine learning and artificial intelligence (AI) that focuses on using artificial neural networks with many layers (hence \"deep\") to model complex patterns in data. These neural networks are designed to mimic the way the human brain processes information, learning hierarchies of features from raw data through multiple layers of abstraction.\n",
        "\n",
        "# Key characteristics of deep learning include:\n",
        "\n",
        "# Hierarchical Feature Learning: Automatically extracts low- to high-level features (e.g., edges to shapes to objects).\n",
        "# Data-Driven: Requires large datasets for effective training.\n",
        "# End-to-End Learning: Trains the system directly from input to output, reducing the need for feature engineering.\n",
        "# Significance of Deep Learning in Artificial Intelligence\n",
        "\n",
        "# Automated Feature Extraction:\n",
        "# Unlike traditional machine learning, deep learning eliminates the need for manual feature engineering. This has been transformative for tasks like image recognition, where feature design was previously challenging.\n",
        "# State-of-the-Art Performance:\n",
        "\n",
        "# Deep learning models consistently achieve or exceed human-level performance in tasks such as image classification (e.g., ImageNet), speech recognition, and natural language processing.\n",
        "# Handling Complex and Unstructured Data:\n",
        "\n",
        "# Excels in domains like vision, audio, and text, which are challenging for traditional algorithms.\n",
        "# Examples: Detecting objects in images, understanding spoken language, or translating text.\n",
        "\n",
        "# Scalability with Data:\n",
        "# The performance of deep learning systems improves significantly with more data and computational power, making them ideal for large-scale problems.\n",
        "\n",
        "# Applications Across Industries:\n",
        "# Healthcare: Diagnosing diseases from medical images, drug discovery.\n",
        "\n",
        "# Finance: Fraud detection, algorithmic trading.\n",
        "# Autonomous Vehicles: Object detection, path planning, and decision-making.\n",
        "# Entertainment: Content recommendation, gaming, and special effects generation.\n",
        "# Customer Service: Chatbots, sentiment analysis, and customer support automation.\n",
        "\n",
        "# Enabling AI Advancements:\n",
        "# Powers breakthroughs in fields like robotics (e.g., learning to grasp objects), generative models (e.g., creating realistic images, text, and videos), and scientific discovery (e.g., protein structure prediction).\n",
        "# Challenges of Deep Learning\n",
        "# Data Dependency:\n",
        "# Requires large, labeled datasets, which may be expensive or difficult to obtain.\n",
        "# Computational Resource Requirements:\n",
        "# High demands for GPU/TPU computing power, storage, and energy.\n",
        "# Interpretability:\n",
        "# Deep learning models are often seen as \"black boxes,\" making their decisions hard to explain.\n",
        "# Overfitting:\n",
        "# Risk of overfitting when training on small or biased datasets.\n",
        "# Ethical Concerns:\n",
        "# Misuse (e.g., deepfakes) and bias in training data leading to unfair decisions."
      ],
      "metadata": {
        "id": "raoutu4E48z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. List and explain the fundamental components of artificial neural networks.\n",
        "# Ans: Artificial Neural Networks (ANNs) are computational models inspired by biological neural networks. They consist of interconnected nodes (neurons) organized in layers that process data by passing it through weighted connections. Here are the key components:\n",
        "\n",
        "# 1. Neurons (Nodes)\n",
        "# Description: The basic computational unit of a neural network.\n",
        "# Functionality: Each neuron receives inputs, processes them using a weighted sum, applies an activation function, and produces an output.\n",
        "\n",
        "# 2. Layers\n",
        "# Neurons are organized into layers, which define the structure of the network:\n",
        "\n",
        "# Input Layer:\n",
        "# First layer, takes the raw data as input.\n",
        "# No computation is performed; it simply forwards the data.\n",
        "# Hidden Layers:\n",
        "# Intermediate layers between input and output layers.\n",
        "# Perform most of the computation by extracting features through learned weights and activation functions.\n",
        "# Networks can have one or multiple hidden layers; deeper networks are referred to as \"deep neural networks.\"\n",
        "# Output Layer:\n",
        "# Produces the final result of the network.\n",
        "# The number of neurons in this layer corresponds to the number of output classes or values.\n",
        "\n",
        "# 3. Weights\n",
        "# Description: Parameters associated with connections between neurons.\n",
        "# Role:\n",
        "# Determine the importance of each input to the neuron.\n",
        "# Learned during training to minimize the error in predictions.\n",
        "# Update Rule: Adjusted using optimization techniques (e.g., gradient descent) during backpropagation.\n",
        "\n",
        "# 4. Bias\n",
        "# Description: A trainable parameter added to the weighted sum of inputs to shift the activation function.\n",
        "# Role: Helps the network fit the data better by allowing the activation function to shift left or right.\n",
        "\n",
        "# 5. Activation Functions\n",
        "# Description: Functions applied to the weighted sum of inputs in a neuron to introduce non-linearity.\n",
        "\n",
        "# 6. Forward Propagation\n",
        "# Description: The process of passing input data through the layers of the network to compute the output.\n",
        "# Mechanism:\n",
        "# Compute the weighted sum for each neuron.\n",
        "# Apply the activation function to produce outputs layer by layer.\n",
        "\n",
        "# 7. Loss Function\n",
        "# Description: A metric to evaluate the difference between predicted outputs and the actual target values.\n",
        "# Common Types:\n",
        "# Mean Squared Error (MSE): For regression tasks.\n",
        "# Cross-Entropy Loss: For classification tasks.\n",
        "# Role: Guides the learning process by quantifying the error.\n",
        "\n",
        "# 8. Backpropagation\n",
        "# Description: An algorithm to compute gradients of the loss function with respect to the network's weights and biases.\n",
        "# Role: Enables the adjustment of weights during training to minimize the loss.\n",
        "\n",
        "# 9. Optimizers\n",
        "# Description: Algorithms that update weights and biases based on computed gradients.\n",
        "# Common Types:\n",
        "# Gradient Descent: Updates weights in the direction of decreasing loss.\n",
        "# Adam: Combines momentum and adaptive learning rates for faster convergence.\n",
        "# RMSProp: Adjusts learning rates for efficient optimization.\n",
        "\n",
        "# 10. Learning Rate\n",
        "# Description: A hyperparameter that determines the step size in the weight update process.\n",
        "# Role: Balances convergence speed and stability.\n",
        "\n",
        "# 11. Epochs and Batches\n",
        "# Epoch:\n",
        "# One complete pass of the entire training dataset through the network.\n",
        "# Batch:\n",
        "# Subset of the dataset used for training in one iteration.\n",
        "# Mini-batch Gradient Descent:\n",
        "# Updates weights using a batch of data instead of the entire dataset.\n",
        "\n",
        "# 12. Regularization Techniques\n",
        "# Description: Methods to prevent overfitting and improve generalization.\n",
        "# Types:\n",
        "# Dropout: Randomly disables neurons during training.\n",
        "# L1/L2 Regularization: Adds penalties to the loss function based on the magnitude of weights.\n",
        "\n",
        "# 13. Hyperparameters\n",
        "# Description: Parameters that are not learned during training but need to be set beforehand.\n",
        "# Examples:\n",
        "# Number of layers and neurons.\n",
        "# Learning rate.\n",
        "# Activation function."
      ],
      "metadata": {
        "id": "vKvftZG358IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Discuss the roles of neurons, connections, weights, and biases.\n",
        "# Ans: These components work in tandem to mimic the way biological neural networks process information. Below is a detailed explanation of their roles and interconnections:\n",
        "\n",
        "\n",
        "# Roles of Neurons, Connections, Weights, and Biases in Artificial Neural Networks\n",
        "# These components work in tandem to mimic the way biological neural networks process information. Below is a detailed explanation of their roles and interconnections:\n",
        "\n",
        "# 1. Neurons (Nodes)\n",
        "# Role:\n",
        "# Neurons are the basic computational units of a neural network.\n",
        "# Each neuron processes input data, applies a transformation, and sends the result to connected neurons.\n",
        "# Functionality:\n",
        "# Input Reception:\n",
        "# A neuron receives input signals (data or outputs from other neurons).\n",
        "# Weighted Sum:\n",
        "# Combines these inputs into a single value using weights and biases.\n",
        "# ð‘§\n",
        "# =\n",
        "# âˆ‘\n",
        "# ð‘–\n",
        "# =\n",
        "# 1\n",
        "# ð‘›\n",
        "# ð‘¤\n",
        "# ð‘–\n",
        "# ð‘¥\n",
        "# ð‘–\n",
        "# +\n",
        "# ð‘\n",
        "# z=\n",
        "# i=1\n",
        "# âˆ‘\n",
        "# n\n",
        "# â€‹\n",
        "#  w\n",
        "# i\n",
        "# â€‹\n",
        "#  x\n",
        "# i\n",
        "# â€‹\n",
        "#  +b\n",
        "# Activation:\n",
        "# Applies an activation function to introduce non-linearity, enabling the network to model complex relationships.\n",
        "# Output:\n",
        "# Produces an output that is passed to the next layer.\n",
        "# 2. Connections\n",
        "# Role:\n",
        "# Define how neurons are linked to one another across layers in the network.\n",
        "# Connections transfer the output of one neuron as input to another, facilitating data flow.\n",
        "# Types:\n",
        "# Feedforward Connections:\n",
        "# Data flows from input to output without loops.\n",
        "# Recurrent Connections:\n",
        "# Data can flow backward, enabling networks to retain memory (e.g., in RNNs).\n",
        "# Significance:\n",
        "# Connections form the network's architecture, determining its complexity and capacity.\n",
        "# 3. Weights\n",
        "# Role:\n",
        "# Weights determine the importance or influence of an input signal on the neuron's output.\n",
        "# They are the primary trainable parameters of the network.\n",
        "# Functionality:\n",
        "# Scaling Inputs:\n",
        "# Each input is multiplied by its corresponding weight.\n",
        "# Large weights emphasize certain inputs, while small weights reduce others' impact.\n",
        "# Learning:\n",
        "# During training, weights are adjusted through optimization techniques (e.g., gradient descent) to minimize the error between predictions and actual outputs.\n",
        "# Significance:\n",
        "# Properly learned weights allow the network to make accurate predictions by capturing relationships in the data.\n",
        "# 4. Biases\n",
        "# Role:\n",
        "# Biases shift the weighted sum of inputs before applying the activation function.\n",
        "# They ensure that a neuron can activate (or produce a non-zero output) even if all input values are zero.\n",
        "# Functionality:\n",
        "# Offset Adjustment:\n",
        "# Adds flexibility to the model by allowing the activation threshold to change.\n",
        "# Prevents dependence solely on input values.\n",
        "# Learning:\n",
        "# Like weights, biases are adjusted during training to improve performance.\n",
        "# Significance:\n",
        "# Biases enhance the model's ability to fit the data by enabling it to learn a wider range of functions."
      ],
      "metadata": {
        "id": "meGbHuZj7Fuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Illustrate the architecture of an artificial neural network. Provide an example to explain the flow of information through the network.\n",
        "# Ans: An Artificial Neural Network (ANN) consists of layers of interconnected nodes (neurons). These layers are typically structured as follows:\n",
        "\n",
        "# Input Layer:\n",
        "\n",
        "# Receives raw input data.\n",
        "# Number of neurons corresponds to the number of input features.\n",
        "# Hidden Layers:\n",
        "\n",
        "# Perform intermediate computations.\n",
        "# Comprise multiple neurons, each connected to neurons in the previous and next layers.\n",
        "# Number of hidden layers and neurons varies depending on the problem's complexity.\n",
        "# Output Layer:\n",
        "\n",
        "# Produces the final result (e.g., classification or regression output).\n",
        "# Number of neurons depends on the type of problem:\n",
        "# 1 neuron for binary classification. N neurons for N-class classification.\n",
        "# 1 neuron for regression.\n",
        "# Flow of Information Through the Network\n",
        "# Example: Predicting a Binary Outcome (Spam/Not Spam)\n",
        "\n",
        "# Architecture:\n",
        "# Input Layer: 3 neurons (features: email length, keyword count, number of links).\n",
        "# Hidden Layer: 2 neurons.\n",
        "# Output Layer: 1 neuron (Spam or Not Spam).\n",
        "\n",
        "# Diagram:\n",
        "# Input Layer       Hidden Layer       Output Layer\n",
        "#    x1   oâ”€â”€â”€w1â”€â”€â”€o          oâ”€â”€â”€w5â”€â”€â”€o (Spam/Not Spam)\n",
        "#    x2   oâ”€â”€â”€w2â”€â”€â”€o          oâ”€â”€â”€w6â”€â”€â”€o\n",
        "#    x3   oâ”€â”€â”€w3â”€â”€â”€o\n",
        "#            â”‚\n",
        "#            b1\n"
      ],
      "metadata": {
        "id": "URHPfsKI7dll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Outline the perceptron learning algorithm. Describe how weights are adjusted during the learning process.\n",
        "# Ans: The perceptron learning algorithm is a foundational supervised learning method for binary classification. It adjusts the weights of the perceptron iteratively based on the errors made in predictions.\n",
        "\n",
        "# Key Components\n",
        "# Input Features (ð‘¥): Represent the data in numerical form.\n",
        "# Weights (ð‘¤): Learnable parameters associated with each input feature.\n",
        "# Bias (ð‘): A constant added to the weighted sum of inputs.\n",
        "# Activation Function:\n",
        "# A step function that determines the output: y = {1 if z > 0 | 0 if z <= 0}\n",
        "# z=âˆ‘wi*xi+b\n",
        "\n",
        "# Steps of the Perceptron Learning Algorithm\n",
        "# Initialization:\n",
        "\n",
        "# Initialize weights (ð‘¤) and bias (ð‘) to small random values or zeros.\n",
        "# For Each Training Sample (ð‘¥):\n",
        "# Compute the weighted sum.\n",
        "# Determine the predicted output (ð‘¦pred) using the activation function.\n",
        "# Update Weights and Bias.\n",
        "# Repeat:\n",
        "# Iterate over the dataset multiple times (epochs) until convergence or a stopping criterion is met."
      ],
      "metadata": {
        "id": "jdwL3tb08A7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Discuss the importance of activation functions in the hidden layers of a multi-layer perceptron. Provide examples of commonly used activation functions\n",
        "# Ans: Activation functions play a crucial role in determining the output of each neuron in the hidden layers of a Multi-Layer Perceptron (MLP). They introduce non-linearity into the model, which is essential for enabling the network to learn complex patterns and relationships in the data. Without activation functions, an MLP would behave like a linear model, regardless of the number of hidden layers.\n",
        "\n",
        "# Key Roles of Activation Functions:\n",
        "# Non-linearity:\n",
        "\n",
        "# Real-world data often has complex, non-linear relationships. Activation functions allow the MLP to model such relationships by applying non-linear transformations to the weighted sum of inputs.\n",
        "# Without non-linearity, the entire MLP (even with multiple layers) would collapse into a single-layer perceptron, essentially performing linear regression.\n",
        "# Enabling Deep Networks:\n",
        "\n",
        "# By adding non-linearity, activation functions enable deeper architectures (i.e., networks with multiple hidden layers) to model increasingly abstract representations of the data.\n",
        "# They allow the model to learn hierarchical featuresâ€”low-level features in the initial layers and more complex features in deeper layers.\n",
        "# Control of Output Range:\n",
        "\n",
        "# Activation functions help control the range of the output from each neuron, which can impact the convergence during training and the networkâ€™s ability to generalize.\n",
        "# For example, some activation functions output values between a certain range, which can make training more stable.\n",
        "# Gradient Propagation:\n",
        "\n",
        "# In backpropagation, activation functions determine the gradient flow through the network, which influences how weights are updated during training.\n",
        "# If the gradient vanishes (as in the case of certain activation functions like sigmoid), the network can have trouble learning long-range dependencies. Conversely, certain activation functions like ReLU help mitigate this issue.\n",
        "\n",
        "# Choosing an Activation Function\n",
        "# The choice of activation function depends on several factors:\n",
        "\n",
        "# Task Type: For binary classification, sigmoid is often used in the output layer. For multi-class classification, softmax is used in the output layer. For regression tasks, no activation or linear activation is often preferred.\n",
        "# Training Stability: ReLU is commonly used in hidden layers because it mitigates the vanishing gradient problem. However, variations like Leaky ReLU and Parametric ReLU can be used to address issues like the dying ReLU problem.\n",
        "# Convergence Speed: ReLU tends to lead to faster convergence due to its simplicity and non-saturating nature."
      ],
      "metadata": {
        "id": "YL9pIa_s98Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Various Neural Network Architect Overview Assignments"
      ],
      "metadata": {
        "id": "flESqXIz_GxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Describe the basic structure of a Feedforward Neural Network (FNN). What is the purpose of the activation function?\n",
        "# Ans: A Feedforward Neural Network (FNN) is one of the simplest types of artificial neural networks (ANNs). It consists of layers of neurons where information moves in one direction, from input to output, without any feedback loops. Each neuron in one layer is connected to every neuron in the subsequent layer. The basic structure includes the following components:\n",
        "\n",
        "# 1. Input Layer:\n",
        "# The input layer consists of neurons that represent the features of the dataset.\n",
        "# Each input neuron corresponds to one feature in the input data. For example, if you have a dataset with 3 features (height, weight, age), the input layer will have 3 neurons.\n",
        "# The input layer does not perform any computation; it just passes the input data to the next layer.\n",
        "# 2. Hidden Layers:\n",
        "# These are the intermediate layers between the input and output layers.\n",
        "# A typical FNN may contain one or more hidden layers, each consisting of several neurons.\n",
        "# Neurons in the hidden layers apply weighted sums of the inputs and a bias term, then pass the result through an activation function (discussed later).\n",
        "# Hidden layers allow the network to learn complex patterns and features.\n",
        "# 3. Output Layer:\n",
        "# The output layer produces the final prediction or classification.\n",
        "# The number of neurons in the output layer depends on the specific task. For example:\n",
        "# Binary classification: 1 output neuron with a sigmoid activation function.\n",
        "# Multi-class classification: One output neuron for each class, with a softmax activation function.\n",
        "# Regression: A single output neuron, often with a linear activation function.\n",
        "# 4. Connections (Weights):\n",
        "# Each connection between neurons has an associated weight. The weight represents the strength of the connection between two neurons.\n",
        "# During training, these weights are adjusted to minimize the error in predictions.\n",
        "# 5. Biases:\n",
        "# Each neuron in the hidden and output layers has a bias term.\n",
        "# The bias allows the model to shift the activation function, helping to model more complex patterns.\n",
        "# Purpose of the Activation Function\n",
        "# The activation function is a crucial component in the structure of a Feedforward Neural Network. It serves several important purposes:\n",
        "\n",
        "# 1. Non-linearity:\n",
        "# The most important purpose of the activation function is to introduce non-linearity into the network.\n",
        "# Without activation functions, the entire network would only be able to model linear relationships, even if it had many layers. This is because a series of linear transformations (weighted sums) is still a linear transformation.\n",
        "# Activation functions allow the network to learn and model complex, non-linear relationships in the data, which is essential for tasks like image recognition, language processing, and more.\n",
        "# 2. Enable Learning of Complex Patterns:\n",
        "# By applying a non-linear transformation to the weighted sum of inputs, the activation function allows the network to learn more complex patterns and interactions in the data. Each hidden layer with an activation function represents a transformation of the data into a higher-dimensional space, helping the network solve complex problems.\n",
        "# 3. Control Output Range:\n",
        "# Activation functions can also control the range of the output, which can be beneficial for different tasks. For example:\n",
        "# Sigmoid or tanh functions ensure the output is within a specific range (e.g., 0â‰¤f(x)â‰¤1 for sigmoid), which is useful for probabilities in classification tasks.\n",
        "# ReLU (Rectified Linear Unit) outputs values from 0 to infinity, which can speed up training and help the network learn faster.\n",
        "# 4. Help with Gradient Flow:\n",
        "# During backpropagation, activation functions determine how the gradient (the derivative of the error with respect to the weights) flows through the network. Some activation functions, like ReLU, help avoid the vanishing gradient problem, making the learning process more efficient."
      ],
      "metadata": {
        "id": "MoC-_3Ms_KDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Explain the role of convolutional layers in CNN. Why are pooling layers commonly used, and what do they achieve?\n",
        "# Ans: Convolutional layers are the core building blocks of Convolutional Neural Networks (CNNs) and play a pivotal role in feature extraction from input data, especially in the context of image processing. They work by applying a set of filters (also known as kernels) to the input data to capture local patterns such as edges, textures, and shapes.\n",
        "\n",
        "# How Convolutional Layers Work:\n",
        "# Filters/Kernels:\n",
        "\n",
        "# A filter is a small matrix (e.g., 3x3, 5x5) that slides across the input image (or feature map in deeper layers), performing element-wise multiplication followed by summation to produce a single value in the output.\n",
        "# Each filter detects a specific feature of the image, such as horizontal edges, vertical edges, or more complex patterns in deeper layers.\n",
        "# Stride:\n",
        "\n",
        "# The stride determines how far the filter moves in each step as it slides over the image. A stride of 1 means the filter moves one pixel at a time, while a stride of 2 means it moves two pixels at a time.\n",
        "# Padding:\n",
        "\n",
        "# Padding is used to control the spatial dimensions of the output. By adding extra pixels around the border of the input (usually zeros), the filter can fully process the input at the edges. This helps preserve the size of the feature map, or control the reduction in size as the network deepens.\n",
        "# Feature Map:\n",
        "\n",
        "# The output of a convolutional layer is a feature map, which is a spatial representation of the features detected by the filters. As you stack convolutional layers, the network captures increasingly abstract and complex features of the input data.\n",
        "# Why Convolutional Layers Are Important:\n",
        "# Local Receptive Fields: Convolutional layers look at small regions of the input at a time (local receptive fields), allowing the network to learn spatial hierarchies of features.\n",
        "# Weight Sharing: Instead of learning a unique weight for each pixel, filters are shared across the entire image, which reduces the number of parameters, making CNNs computationally efficient and easier to train.\n",
        "# Translation Invariance: By detecting features like edges regardless of where they appear in the image, convolutional layers help CNNs achieve translation invariance, meaning the model can recognize objects even if they are moved around in the image.\n",
        "# Why Pooling Layers Are Commonly Used in CNNs\n",
        "# Pooling layers are typically used after convolutional layers to downsample the spatial dimensions of the feature maps, which helps reduce the computational complexity and the number of parameters in the model. They also help make the network more robust to small translations or distortions in the input data.\n",
        "\n",
        "# Types of Pooling:\n",
        "# Max Pooling:\n",
        "\n",
        "# Max pooling selects the maximum value from a region (usually a 2x2 or 3x3 window) in the feature map and discards the rest.\n",
        "# This operation retains the most important feature (the strongest activation) from each region, making the representation more compact and preserving the most dominant features.\n",
        "# Average Pooling:\n",
        "\n",
        "# Average pooling computes the average value in a region instead of the maximum.\n",
        "# This can be used when a smoother feature map is desired, though it is less common than max pooling because max pooling generally performs better in preserving important features.\n",
        "# Why Pooling Layers Are Important:\n",
        "# Dimensionality Reduction:\n",
        "\n",
        "# Pooling reduces the size of the feature maps, which helps lower the computational cost and memory requirements of the network. This is especially important when working with high-resolution images.\n",
        "# Translation Invariance:\n",
        "\n",
        "# Pooling adds a form of translation invariance to the model. Since the network focuses on the most important features (max pooling), small translations, distortions, or rotations in the input image are less likely to affect the final output.\n",
        "# Reduction of Overfitting:\n",
        "\n",
        "# By reducing the spatial dimensions, pooling layers help prevent the network from overfitting to the fine details in the training data. This encourages the model to focus on more global features that are essential for classification or recognition tasks.\n",
        "# Faster Computation:\n",
        "\n",
        "# Pooling reduces the number of computations in the network. After applying pooling, the feature maps are smaller, meaning fewer parameters to process in the subsequent layers, which speeds up both the training and inference processes."
      ],
      "metadata": {
        "id": "gB-HLr_UAKLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. What is the key characteristic that differentiates Recurrent Neural Networks (RNNs) from other neural networks? How does an RNN handle sequential data?\n",
        "# Ans: The key characteristic that differentiates Recurrent Neural Networks (RNNs) from other types of neural networks (such as Feedforward Neural Networks or CNNs) is their ability to process sequential data and maintain memory of previous inputs. While traditional neural networks process input data in isolation (independently of previous inputs), RNNs have a feedback loop within their architecture that allows information to persist and influence future computations.\n",
        "\n",
        "# This feedback loop enables RNNs to retain a form of memory, which is particularly useful for tasks where the order of inputs is important, such as time series analysis, natural language processing (NLP), and speech recognition. The recurrent nature of RNNs allows them to take into account previous time steps' outputs when processing the current input.\n",
        "\n",
        "# How an RNN Handles Sequential Data\n",
        "# Recurrent Connection:\n",
        "\n",
        "# In an RNN, each neuron in the hidden layer has a connection that loops back to itself, forming a cycle. This connection allows the network to use the information from the previous step to influence the current step. The hidden state\n",
        "\n",
        "# Handling Sequential Data in RNNs\n",
        "# RNNs are specifically designed to handle sequential data by:\n",
        "\n",
        "# Capturing Temporal Dependencies:\n",
        "\n",
        "# RNNs maintain a memory of past inputs through their hidden states, enabling them to capture temporal dependencies within the sequence. This makes RNNs effective for tasks like time series prediction, language modeling, and speech recognition, where the context from previous steps is crucial.\n",
        "# Modeling Context:\n",
        "\n",
        "# As RNNs process sequential data, they can retain the context of the entire sequence in their hidden states. This context is used to make predictions or decisions based on the entire sequence rather than just the current input.\n",
        "# Dynamic Input Length:\n",
        "\n",
        "# Unlike traditional neural networks, RNNs can handle sequences of varying lengths. Whether the sequence is short (e.g., a few words) or long (e.g., an entire document or time series data), the RNN processes each element sequentially and maintains context."
      ],
      "metadata": {
        "id": "SEhJcTiJAcLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Discuss the components of a Long Short-Term Memory (LSTM) network. How does it address the vanishing gradient problem?\n",
        "# Ans: Long Short-Term Memory (LSTM) is a specialized type of Recurrent Neural Network (RNN) designed to address the limitations of traditional RNNs, especially the vanishing gradient problem. LSTMs have a more complex architecture than basic RNNs, incorporating gates that regulate the flow of information. These gates help LSTMs maintain long-term dependencies and manage the cell state more effectively.\n",
        "\n",
        "# The key components of an LSTM are:\n",
        "\n",
        "# Cell State (ð¶ð‘¡):\n",
        "\n",
        "# The cell state is the memory of the LSTM unit. It carries information across time steps and is modified by various gates at each time step.\n",
        "# The cell state can be thought of as the long-term memory of the LSTM, and it is crucial for learning long-term dependencies.\n",
        "# Hidden State (â„Žð‘¡):\n",
        "\n",
        "# The hidden state carries the output of the LSTM unit, which is passed to the next time step and the output layer. The hidden state captures both short-term and long-term information from the sequence.\n",
        "# Gates:\n",
        "\n",
        "# Forget Gate: Decides what portion of the previous cell state should be forgotten. It takes the current input (ð‘¥ð‘¡) and the previous hidden state (h tâˆ’1) and passes them through a sigmoid activation function to produce a value between 0 and 1. A value close to 0 means \"forget,\" and a value close to 1 means \"retain.\"\n",
        "\n",
        "# How LSTMs Address the Vanishing Gradient Problem\n",
        "# The vanishing gradient problem arises in traditional RNNs when gradients are propagated backward through many time steps during training. The gradients become very small, making it difficult for the network to learn long-term dependencies.\n",
        "\n",
        "# LSTMs address this problem in the following ways:\n",
        "\n",
        "# Cell State as a \"Conduit\":\n",
        "\n",
        "# The cell state in LSTMs serves as a long-term memory that is modified in a controlled manner by the gates. It has the ability to \"carry\" information across many time steps without drastic changes, allowing the network to retain important long-term dependencies.\n",
        "# The gradients that flow through the cell state remain largely unaffected by the vanishing gradient problem because of the gating mechanisms that control how much the cell state is updated. As a result, the cell state can be passed down the network with little attenuation over many time steps.\n",
        "# Forget Gate:\n",
        "\n",
        "# The forget gate allows the LSTM to \"forget\" irrelevant information selectively, preventing the cell state from accumulating noise over long sequences. This selective forget mechanism ensures that only relevant information is retained, reducing the chances of long-term dependencies becoming irrelevant or forgotten.\n",
        "# Gradients Propagation through Gates:\n",
        "\n",
        "# The LSTM gates (forget, input, and output) are carefully designed to enable the gradients to flow more effectively during backpropagation. These gates are trained to regulate the flow of information, preventing the gradients from becoming too small (vanishing) or too large (exploding).\n",
        "# Since the sigmoid and tanh activations are used in the gates, they ensure that the gradients are not vanishing too quickly, unlike in vanilla RNNs where gradients tend to decay exponentially.\n",
        "# Preservation of Information:\n",
        "\n",
        "# The cell state is updated in a manner that allows it to retain important features from previous time steps without becoming overwhelmed by minor fluctuations in the input. The forget and input gates allow the LSTM to preserve significant information over long sequences while ignoring less useful data."
      ],
      "metadata": {
        "id": "SJmwMT6xA0_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Describe the roles of the generator and discriminator in a Generative Adversarial Network (GAN). What is the training objective for each?\n",
        "# Ans: A Generative Adversarial Network (GAN) consists of two main components: the generator and the discriminator. These two networks work in opposition to each other, and their interaction drives the learning process.\n",
        "\n",
        "# 1. Generator\n",
        "# Role: The generator is responsible for generating synthetic data that resembles the real data distribution. It takes random noise as input (often from a uniform or normal distribution) and produces data that mimics the real data.\n",
        "# Objective: The generatorâ€™s objective is to create realistic data (e.g., images, audio, text) that is indistinguishable from real data. It tries to fool the discriminator into thinking that the generated data is real.\n",
        "# Training Objective: The generator is trained to minimize the ability of the discriminator to differentiate between real and fake data. Specifically, it aims to maximize the probability of the discriminator classifying the fake data as real. This is achieved by maximizing the discriminator's error.\n",
        "# 2. Discriminator\n",
        "# Role: The discriminator is a classifier that evaluates whether a given input is real (from the training dataset) or fake (generated by the generator). It provides feedback to the generator about the quality of its generated data.\n",
        "# Objective: The discriminatorâ€™s objective is to accurately classify data as real or fake, improving its ability to distinguish between the two.\n",
        "# Training Objective: The discriminator is trained to maximize its ability to correctly classify real and fake data. Specifically, it aims to assign high probability to real data and low probability to fake data."
      ],
      "metadata": {
        "id": "Oap8LX37BoCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation functions assignment questions"
      ],
      "metadata": {
        "id": "Y9sUnhaSCEXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Explain the role of activation functions in neural networks. Compare and contrast linear and nonlinear activation functions. Why are nonlinear activation functions preferred in hidden layers?\n",
        "# Ans: Activation functions play a crucial role in neural networks by introducing non-linearity into the network. This non-linearity allows neural networks to learn and model complex patterns in data, making them suitable for a wide range of tasks, such as classification, regression, and image recognition.\n",
        "# The activation function determines the output of each neuron based on its input, which is typically a weighted sum of the neuronâ€™s inputs passed through an activation function. Without activation functions, a neural network would be a linear model, which limits its capacity to learn complex relationships in the data.\n",
        "\n",
        "# Types of Activation Functions\n",
        "# 1. Linear Activation Function\n",
        "# A linear activation function simply outputs a weighted sum of the inputs. Mathematically,\n",
        "\n",
        "# 2. Nonlinear Activation Functions\n",
        "# Nonlinear activation functions introduce non-linearity into the model, which enables neural networks to learn and approximate complex, real-world data distributions.\n",
        "\n",
        "# Tanh (Hyperbolic Tangent): Often used in the hidden layers as it centers the data around zero, improving training stability compared to sigmoid.\n",
        "# ReLU (Rectified Linear Unit): Very popular for hidden layers due to its simplicity and ability to mitigate the vanishing gradient problem.\n",
        "\n",
        "# Why Nonlinear Activation Functions are Preferred in Hidden Layers\n",
        "# Modeling Complex Patterns: Nonlinear activation functions enable the network to approximate complex, non-linear relationships in the data. This is especially important for tasks like image recognition, language modeling, and other tasks with highly complex data distributions.\n",
        "\n",
        "# Layer Stacking and Hierarchical Learning: Without non-linearity, even if you stack multiple layers in a network, the entire network would still behave as a linear function. Nonlinear activation functions allow each layer to learn a more complex transformation of the input data, enabling the network to learn hierarchical features. For example, in deep convolutional networks, lower layers might learn edges and textures, while deeper layers might learn object parts or complete objects.\n",
        "\n",
        "# Overcoming Linear Limitations: As mentioned earlier, a network with only linear activations, regardless of the depth, would have the same expressive power as a single-layer model. Nonlinear activations allow deep networks to learn more abstract representations of the data, making them much more powerful for real-world applications.\n",
        "\n",
        "# Gradient-Based Optimization: Nonlinear activation functions allow gradients to be backpropagated through the network during training. While some nonlinear functions like sigmoid and tanh may suffer from issues like vanishing gradients, others like ReLU and its variants help mitigate such problems, allowing more efficient training and deeper networks.\n",
        "\n",
        "# Sparsity and Efficiency: Functions like ReLU introduce sparsity (outputting zero for negative inputs), which helps the network focus on the most important features and makes the model more efficient."
      ],
      "metadata": {
        "id": "V3PVkGSDCHIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Describe the Sigmoid activation function. What are its characteristics, and in what type of layers is it commonly used? Explain the Rectified Linear Unit (ReLU) activation function. Discuss its advantages and potential challenges.What is the purpose of the Tanh activation function? How does it differ from the Sigmoid activation function?\n",
        "# Ans: The Sigmoid activation function is one of the earliest activation functions used in neural networks. It produces an output that maps any input to a value between 0 and 1, making it useful for binary classification tasks.\n",
        "\n",
        "# Characteristics:\n",
        "# Output Range: (0,1), meaning it squashes the output to this range.\n",
        "# Smooth and Differentiable: The function is smooth and differentiable, which makes it suitable for backpropagation.\n",
        "# Monotonic: Sigmoid is a monotonically increasing function, meaning as the input increases, the output also increases.\n",
        "# Output Interpretation: The output can be interpreted as a probability, which is why the sigmoid is commonly used in the output layer for binary classification tasks.\n",
        "# Gradient: The gradient of the sigmoid function is f(x)â‹…(1âˆ’f(x)), which is the derivative of the sigmoid function.\n",
        "# Common Usage:\n",
        "# Output Layer of Binary Classification Models: Sigmoid is typically used in the output layer of models where the task is to predict a binary class (0 or 1). For example, in a neural network for spam detection, the sigmoid activation would output a probability of the email being spam (1) or not spam (0).\n",
        "# Limitations:\n",
        "# Vanishing Gradient Problem: When the input to the sigmoid function becomes very large or very small, the gradient becomes very small, leading to vanishing gradients during backpropagation. This can hinder the training of deep networks.\n",
        "# Not Zero-Centered: The sigmoid functionâ€™s output is always positive, which can cause inefficient weight updates during training because gradients are always positive. This may lead to slower convergence.\n",
        "# 2. Rectified Linear Unit (ReLU) Activation Function\n",
        "# The Rectified Linear Unit (ReLU) is one of the most widely used activation functions, especially in deep learning models. It offers a simple yet effective way to introduce non-linearity.\n",
        "\n",
        "\n",
        "# 1. Sigmoid Activation Function\n",
        "# The Sigmoid activation function is one of the earliest activation functions used in neural networks. It produces an output that maps any input to a value between 0 and 1, making it useful for binary classification tasks.\n",
        "\n",
        "# Formula:\n",
        "# ð‘“\n",
        "# (\n",
        "# ð‘¥\n",
        "# )\n",
        "# =\n",
        "# 1\n",
        "# 1\n",
        "# +\n",
        "# ð‘’\n",
        "# âˆ’\n",
        "# ð‘¥\n",
        "# f(x)=\n",
        "# 1+e\n",
        "# âˆ’x\n",
        "\n",
        "# 1\n",
        "# â€‹\n",
        "\n",
        "# Where\n",
        "# ð‘¥\n",
        "# x is the input to the function.\n",
        "\n",
        "# Characteristics:\n",
        "# Output Range:\n",
        "# (\n",
        "# 0\n",
        "# ,\n",
        "# 1\n",
        "# )\n",
        "# (0,1), meaning it squashes the output to this range.\n",
        "# Smooth and Differentiable: The function is smooth and differentiable, which makes it suitable for backpropagation.\n",
        "# Monotonic: Sigmoid is a monotonically increasing function, meaning as the input increases, the output also increases.\n",
        "# Output Interpretation: The output can be interpreted as a probability, which is why the sigmoid is commonly used in the output layer for binary classification tasks.\n",
        "# Gradient: The gradient of the sigmoid function is\n",
        "# ð‘“\n",
        "# (\n",
        "# ð‘¥\n",
        "# )\n",
        "# â‹…\n",
        "# (\n",
        "# 1\n",
        "# âˆ’\n",
        "# ð‘“\n",
        "# (\n",
        "# ð‘¥\n",
        "# )\n",
        "# )\n",
        "# f(x)â‹…(1âˆ’f(x)), which is the derivative of the sigmoid function.\n",
        "# Common Usage:\n",
        "# Output Layer of Binary Classification Models: Sigmoid is typically used in the output layer of models where the task is to predict a binary class (0 or 1). For example, in a neural network for spam detection, the sigmoid activation would output a probability of the email being spam (1) or not spam (0).\n",
        "# Limitations:\n",
        "# Vanishing Gradient Problem: When the input to the sigmoid function becomes very large or very small, the gradient becomes very small, leading to vanishing gradients during backpropagation. This can hinder the training of deep networks.\n",
        "# Not Zero-Centered: The sigmoid functionâ€™s output is always positive, which can cause inefficient weight updates during training because gradients are always positive. This may lead to slower convergence.\n",
        "# 2. Rectified Linear Unit (ReLU) Activation Function\n",
        "# The Rectified Linear Unit (ReLU) is one of the most widely used activation functions, especially in deep learning models. It offers a simple yet effective way to introduce non-linearity.\n",
        "\n",
        "# Formula:\n",
        "# ð‘“\n",
        "# (\n",
        "# ð‘¥\n",
        "# )\n",
        "# =\n",
        "# max\n",
        "# â¡\n",
        "# (\n",
        "# 0\n",
        "# ,\n",
        "# ð‘¥\n",
        "# )\n",
        "# f(x)=max(0,x)\n",
        "# Where\n",
        "# ð‘¥\n",
        "# x is the input to the function.\n",
        "\n",
        "# Characteristics:\n",
        "# Output Range: [0,âˆž), meaning it outputs the input if it is positive and zero if it is negative.\n",
        "# Non-linear: Despite being linear for positive inputs, it is a non-linear activation function that enables neural networks to approximate complex functions.\n",
        "# Computationally Efficient: ReLU is computationally efficient as it only requires a comparison operation.\n",
        "# Sparsity: The ReLU function introduces sparsity in the network because it outputs zero for negative values. This can improve efficiency and performance by focusing on important features.\n",
        "# Advantages:\n",
        "# Prevents Vanishing Gradients: Unlike sigmoid and tanh, ReLU does not suffer from the vanishing gradient problem in the positive domain, as its gradient is constant (\n",
        "# 1\n",
        "# 1) for positive inputs.\n",
        "# Faster Training: Due to its simplicity and lack of complex mathematical operations, ReLU leads to faster convergence and efficient training.\n",
        "# Sparse Activations: ReLU helps in sparsity, meaning fewer neurons are active at any given time, which can make the model more efficient.\n",
        "# Challenges:\n",
        "# Dying ReLU Problem: For inputs that are less than zero, ReLU outputs zero, which means neurons can \"die\" and never recover. This is especially problematic when a large fraction of neurons become inactive during training.\n",
        "# Unbounded Output: Since ReLU has no upper bound, it can produce very large outputs that may cause instability in some networks.\n",
        "# Variants:\n",
        "# Leaky ReLU: Introduces a small slope for negative values instead of outputting zero, addressing the dying ReLU problem.\n",
        "# Parametric ReLU (PReLU): Similar to Leaky ReLU, but with learnable parameters for the slope in the negative domain."
      ],
      "metadata": {
        "id": "XjK41RqyCwvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Discuss the significance of activation functions in the hidden layers of a neural network.\n",
        "# Ans: Activation functions are essential components in hidden layers of a neural network because they introduce non-linearity into the network. Without activation functions, the entire neural network would behave like a single-layer linear model, regardless of how many layers it contains. This would severely limit the ability of the network to learn complex patterns in data. Let's explore the reasons why activation functions are crucial in hidden layers:\n",
        "\n",
        "# 1. Introduction of Non-linearity\n",
        "# The primary function of an activation function is to introduce non-linearity into the model. Real-world data often exhibit non-linear relationships, and a neural network must be able to model these relationships to make accurate predictions. If a network only had linear transformations (i.e., no activation function), it would essentially be equivalent to a linear model, regardless of the depth of the network. This means it would only be able to solve problems where relationships between inputs and outputs are linear.\n",
        "\n",
        "# Non-linear transformation enables the neural network to approximate complex, non-linear mappings between the inputs and outputs. This is critical for tasks like image recognition, speech recognition, natural language processing, etc., where the relationships in data are highly complex and non-linear.\n",
        "# 2. Learning Complex Patterns\n",
        "# By using non-linear activation functions, each layer can learn different representations of the data. In a deep neural network, early layers might learn low-level features like edges in images or simple phonemes in speech, while deeper layers can combine those low-level features to recognize complex structures like faces or words.\n",
        "\n",
        "# Hierarchical learning: Activation functions help the network learn progressively more abstract features in the data. Without them, the network would struggle to capture and combine these features effectively.\n",
        "# 3. Enhancing the Expressive Power of the Network\n",
        "# Activation functions enhance the networkâ€™s capacity to represent complex functions. For example, a deep network with activation functions can approximate any continuous function, as described by the universal approximation theorem. This makes neural networks highly versatile, able to learn and approximate virtually any pattern in the data given enough training data and layers.\n",
        "\n",
        "# Increased capacity: By introducing non-linearity, neural networks can capture a broader range of functions, making them suitable for diverse tasks, including regression, classification, and generative modeling.\n",
        "# 4. Backpropagation and Gradient-Based Optimization\n",
        "# Activation functions are differentiable, allowing for the effective use of backpropagation, a key algorithm for training neural networks. Backpropagation computes the gradients of the loss function with respect to each weight by applying the chain rule. The gradient is influenced by the derivative of the activation function, so the choice of activation function impacts how efficiently the model learns.\n",
        "\n",
        "# Smooth gradients: A differentiable activation function ensures smooth gradients, facilitating the optimization process. If the gradients are too steep or too flat (as with the vanishing or exploding gradient problems), learning can become very slow or fail entirely.\n",
        "# 5. Preventing the Vanishing Gradient Problem\n",
        "# The vanishing gradient problem occurs when gradients become very small during backpropagation, especially in deep networks. This makes weight updates very slow and can stop the network from learning altogether. Activation functions like ReLU (Rectified Linear Unit) and its variants help to address this issue.\n",
        "\n",
        "# ReLU and variants: ReLU provides a gradient of 1 for positive inputs, which helps to maintain stronger gradients, preventing them from vanishing. This leads to faster training and better convergence in deeper networks.\n",
        "# 6. Controlling the Output\n",
        "# In some cases, the output of a hidden layer needs to be scaled or normalized within a certain range. For example:\n",
        "\n",
        "# Tanh: Outputs values between âˆ’ 1 1, which can help center data around zero, leading to more efficient learning in some cases.\n",
        "# Sigmoid: Outputs values between 0 and 1, which is useful in specific situations where probabilities or binary outputs are needed.\n",
        "# 7. Sparsity\n",
        "# Some activation functions, like ReLU, induce sparsity in the network. This means that a large number of neurons in the network may have an output of zero for certain inputs. This can be useful for several reasons:\n",
        "\n",
        "# Efficiency: Sparsity reduces the number of neurons actively contributing to the computation, which can improve computational efficiency.\n",
        "# Feature selection: Sparsity may act as a form of feature selection, helping the network focus on the most relevant inputs."
      ],
      "metadata": {
        "id": "mD3a0NoiDU9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Explain the choice of activation functions for different types of problems (e.g., classification, regression) in the output layer.\n",
        "# Ans: The output layer of a neural network is responsible for producing the final prediction or output based on the features learned by the network. The activation function in the output layer plays a crucial role in determining the type of output (e.g., continuous values or probabilities) and how the model will interpret or predict that output. The choice of activation function depends on the type of problem you are solving, such as classification, regression, or other specialized tasks. Below are the common choices of activation functions for different types of problems.\n",
        "\n",
        "# 1. Binary Classification\n",
        "# In binary classification, the goal is to predict one of two classes (e.g., \"yes\" vs. \"no\", \"spam\" vs. \"not spam\"). The output layer typically has one neuron, and the output needs to represent a probability of belonging to one class.\n",
        "\n",
        "# 2. Multi-Class Classification\n",
        "# In multi-class classification, the goal is to assign an input to one of several classes (e.g., classifying images of animals into categories like \"cat,\" \"dog,\" and \"rabbit\"). The output layer typically has one neuron per class, and the output should represent the probability distribution over the classes.\n",
        "\n",
        "# 3. Regression (Continuous Values)\n",
        "# In regression tasks, the goal is to predict continuous values rather than discrete classes. The output layer typically has one neuron (for predicting a single value), and the output should be a real number, which may not be constrained within a specific range.\n",
        "\n",
        "# 4. Multi-Label Classification\n",
        "# In multi-label classification, each input can belong to multiple classes simultaneously (e.g., a movie can belong to both \"comedy\" and \"drama\" genres). The output layer typically has one neuron per class, and each neuron outputs a binary value (0 or 1), indicating whether the input belongs to that class."
      ],
      "metadata": {
        "id": "gKUTZA-WD5lW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Experiment with different activation functions (e.g., ReLU, Sigmoid, Tanh) in a simple neural network architecture. Compare their effects on convergence and performance.\n",
        "# Ans: To experiment with different activation functions in a simple neural network architecture, we can set up a neural network for a classification task using the Keras library with TensorFlow backend. We'll create a model and compare how different activation functions like ReLU, Sigmoid, and Tanh affect the network's convergence and performance.\n",
        "\n",
        "# Steps:\n",
        "# Dataset Selection: We'll use a simple dataset such as the Iris dataset for multi-class classification.\n",
        "# Model Setup: We'll build a neural network with a few hidden layers.\n",
        "# Training and Comparison: We'll experiment with different activation functions and compare the model's training loss, accuracy, and convergence.\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# One-hot encoding the labels\n",
        "y = to_categorical(y)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define a function to create and train the model\n",
        "def create_model(activation_function):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(16, input_dim=X_train.shape[1], activation=activation_function))  # First hidden layer\n",
        "    model.add(Dense(16, activation=activation_function))  # Second hidden layer\n",
        "    model.add(Dense(3, activation='softmax'))  # Output layer (softmax for multi-class classification)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define activation functions to experiment with\n",
        "activation_functions = ['relu', 'sigmoid', 'tanh']\n",
        "\n",
        "# Dictionary to store results\n",
        "history_dict = {}\n",
        "\n",
        "# Train and evaluate the model for each activation function\n",
        "for activation_function in activation_functions:\n",
        "    print(f\"Training model with {activation_function} activation function:\")\n",
        "\n",
        "    # Create and train the model\n",
        "    model = create_model(activation_function)\n",
        "    history = model.fit(X_train, y_train, epochs=100, batch_size=10, validation_data=(X_test, y_test), verbose=0)\n",
        "\n",
        "    # Store the training history\n",
        "    history_dict[activation_function] = history\n",
        "    print(f\"Final accuracy with {activation_function}: {history.history['val_accuracy'][-1]:.4f}\")\n",
        "\n",
        "# Plot training accuracy and loss\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "for activation_function, history in history_dict.items():\n",
        "    plt.plot(history.history['val_accuracy'], label=f'{activation_function} (val_accuracy)')\n",
        "plt.title('Validation Accuracy for Different Activation Functions')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "for activation_function, history in history_dict.items():\n",
        "    plt.plot(history.history['val_loss'], label=f'{activation_function} (val_loss)')\n",
        "plt.title('Validation Loss for Different Activation Functions')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 772
        },
        "id": "9wrVK7tLEYdh",
        "outputId": "6d1e3f76-5f33-4635-f9e5-fd5d4ff6a6cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with relu activation function:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final accuracy with relu: 0.9667\n",
            "Training model with sigmoid activation function:\n",
            "Final accuracy with sigmoid: 0.9667\n",
            "Training model with tanh activation function:\n",
            "Final accuracy with tanh: 1.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAJOCAYAAABm7rQwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd3hTZfsH8G+SZnXTli4otCAgYKGAgFAVVLQMQUSm/GSIoCICwiuCC0SRV5Eh7vEqiEVZ4kRkCIqIyFb2pkAXq3skTZ7fHyfnZJ2sNmnW/bmuXtCTk3Oek5ykJ3fu+34kjDEGQgghhBBCCCGEEELqkdTbAyCEEEIIIYQQQgghwYeCUoQQQgghhBBCCCGk3lFQihBCCCGEEEIIIYTUOwpKEUIIIYQQQgghhJB6R0EpQgghhBBCCCGEEFLvKChFCCGEEEIIIYQQQuodBaUIIYQQQgghhBBCSL2joBQhhBBCCCGEEEIIqXcUlCKEEEIIIYQQQggh9Y6CUkHm/PnzkEgkWLZsmbBszpw5kEgkTt1fIpFgzpw5bh1Tz5490bNnT7duk9SfgoICDB48GLGxsZBIJFiyZIm3h2RG7JwHgI0bNyIjIwMqlQoSiQRFRUUAgBUrVuDmm2+GXC5HdHR0vY83EHjzNT1mzBikpqZ6Zd/+KDU1FWPGjPH2MAgJOnQ95p/27NmD7t27IywsDBKJBAcPHvT2kMwsW7YMEokE58+fN1u+YMECNGvWDDKZDBkZGQCAmpoazJgxAykpKZBKpRg4cGC9jzcQeOK16Cz6G+48W58HiG+goJQPGzBgAEJDQ1FaWmpznZEjR0KhUODatWv1ODLXHT16FHPmzLH6I+krNmzYAIlEguTkZOj1em8Px68888wz+OWXXzBr1iysWLECvXv39uj+JBKJ8BMSEoKYmBh06tQJU6ZMwdGjR53axrVr1zB06FCo1Wq89957WLFiBcLCwnD8+HGMGTMGzZs3xyeffIKPP/7Yo8dSF3V5Tc2YMQMSiQTDhg3zyv7rKjc3F3PmzPGpDwP8xY7Yz2233ebVsf3555+YM2eOEHglhLiGrsc8a/v27ZBIJFi7dq23h2KXVqvFkCFDcP36dSxevBgrVqxA06ZNPbY//nHhf5RKJRISEtCzZ0+8/vrruHLlilPb2bRpE2bMmIHMzEx8/vnneP311wEAn332GRYsWIDBgwdj+fLleOaZZzx2LHW1YcOGWgd+unTpAolEgg8++MAr+68rX/wbzgc/xX5mzpzp1bGtXLnS574gJ05gxGd9/fXXDABbvny56O3l5eUsLCyM9e/f3+ltnjt3jgFgn3/+ubBMq9WyyspKp+4PgM2ePdvp/fHWrFnDALBt27ZZ3VZdXc2qq6td3qY7Pfzwwyw1NZUBYJs3b/bqWPxNQkICGzlyZL3tDwC799572YoVK9gXX3zB3nnnHfbYY4+xqKgoFhISwhYuXGi2vl6vZ5WVlaympkZY9vPPP4s+1x988AEDwE6dOlUvx1IX9l5T9uj1eta4cWOWmprK1Go1Kykpcfv+Pf2a3rNnj9X7GE+j0bCqqiqP7dsW/r11xIgRbMWKFWY/GzdurPfxmFqwYAEDwM6dO2d1W1VVFdNoNPU/KEL8CF2Peda2bdsYALZmzZp637crjh07xgCwTz75pF72xz8ukydPZitWrGDLli1jCxYsYA8++CALCQlhsbGxbOvWrWb3qampYZWVlUyv1wvLnnvuOSaVSq2e22HDhrFGjRrVy7HU1VNPPcVq87H15MmTDABLTU1lmZmZHtl/ZWUl02q1td62I774N/zzzz9nANjcuXOtrnkOHDhQ7+Mx1a9fP9a0aVOr5WKfB4jvCKm36Bdx2YABAxAREYGVK1di1KhRVrd/9913KC8vx8iRI+u0n5CQEISEeO9UUCgUXts3AJSXl+O7777D/Pnz8fnnnyM7Oxu9evXy6phsKS8vR1hYmLeHYaawsNCtZW5VVVVQKBSQSm0ncrZs2RL/93//Z7bsv//9L/r374/p06fj5ptvRt++fQFwmVUqlcpqzACsxm1reV342nO2fft2XLp0Cb/++iuysrLwzTffYPTo0W7dhzdf03K53Gv7BoCOHTtanZu+TKlUensIhPg8uh4jgPeuEe644w4MHjzYbNmhQ4dw33334aGHHsLRo0eRlJQEAJDJZJDJZFbjVqvVVs+vu6/fGGOoqqqCWq122zbr6ssvv0R8fDwWLlyIwYMH4/z5824v8be8xqxP3v4b3qdPH9x6661eHYOzxD4PEB/i7agYsW/06NEsJCSEFRQUWN12//33s4iICFZRUcGuXbvGpk+fzm655RYWFhbGIiIiWO/evdnBgwfN7iP2zdzs2bOtov9VVVVs6tSpLC4ujoWHh7P+/fuzixcvWn0zd/78efbkk0+yli1bMpVKxWJiYtjgwYPNovl8NN3yh/+WrkePHqxHjx5m+y8oKGCPPvooi4+PZ0qlkrVr144tW7ZM9FgWLFjAPvroI9asWTOmUCjYrbfeyv7++2+nH+MVK1YwqVTK8vLy2BtvvMEiIyNFv6msrKxks2fPZi1atGBKpZIlJiayBx98kJ0+fVpYR6fTsSVLlrBbbrmFKZVKFhcXx7KystiePXtsPv48y8eWf16OHDnCRowYwaKjo1lGRgZjjLFDhw6x0aNHs7S0NKZUKllCQgIbO3Ysu3r1qtV2L126xB599FGWlJTEFAoFS01NZU888QSrrq5mZ86cYQDYokWLrO63c+dOBoCtXLlS9HGz9bzyzpw5wwYPHswaNGjA1Go169q1K/vxxx/NtsF/C/jVV1+xF154gSUnJzOJRMJu3Lghuk/+cXrqqadEb7tw4QILCQlh3bt3F5ZZPuY9evSwGvPo0aNZ06ZNrZabPh8bNmxgt99+OwsNDWXh4eGsb9++7PDhw2b7Hz16NAsLC2OnT59mffr0YeHh4eyBBx5gjHHnxuLFi1mbNm2YUqlk8fHxbMKECez69etm22jatCnr168f27FjB+vcuTNTKpUsLS3N7Bt6R68pe8aNG8fatGnDGGOsT58+7N577xVdz95548prOj8/n8lkMjZnzhyrfRw/fpwBYO+88w5jjDn1PsafM5Y//PPLP5emysrK2LRp01jjxo2ZQqFgLVu2ZAsWLDD7Jpkx47m1fv161rZtW6ZQKFibNm3Yzz//7PBxNX0/skXsvU5szK6+tx07dowNGTKExcXFMZVKxVq2bMmef/55xpjxfcTyh3+Pbtq0KRs9erTZ9lx57a5atYq99tprrFGjRkypVLK7777bKtPw5MmTbNCgQSwhIYEplUrWqFEjNmzYMFZUVGTnESXEt9D1mOeux5zNlHLmvYkxxpYuXcratGnD1Go1i46OZp06dWLZ2dnC7SUlJWzKlCmsadOmTKFQsIYNG7JevXqxffv22dz36NGjrR4308dq69atwjVCVFQUGzBgADt69KjZNuxd19XmcVm5ciUDILzfM2Z8jvnn3dbfS3vngavXKxs3bmSdOnViSqWSLV68mDHG2I0bN9iUKVOEv7vNmzdn//3vf5lOpxPu7+x5I/bYO/sR9qabbmITJ05k1dXVLDo6ms2bN090vb/++ov16dOHRUdHs9DQUJaens6WLFni1P5NX4t8JuL27dut9vHhhx8yAOzff/9ljDl3Le8Lf8PF8OcQ//lGjOV7FM9yzPy2/vjjD/bMM8+wuLg4FhoaygYOHMgKCwut7r9hwwZ25513svDwcBYREcFuvfVW4fUtdo3PX1/Z+gzmymv31KlTbPTo0SwqKopFRkayMWPGsPLycrN1N23axDIzM1lUVBQLCwtjLVu2ZLNmzbLzaBLGKFPK540cORLLly/H6tWrMWnSJGH59evX8csvv2DEiBFQq9U4cuQIvv32WwwZMgRpaWkoKCjARx99hB49euDo0aNITk52ab+PPfYYvvzySzz88MPo3r07fv31V/Tr189qvT179uDPP//E8OHD0bhxY5w/fx4ffPABevbsiaNHjyI0NBR33nknJk+ejKVLl+L5559H69atAUD411JlZSV69uyJ06dPY9KkSUhLS8OaNWswZswYFBUVYcqUKWbrr1y5EqWlpXj88cchkUjw5ptvYtCgQTh79qxTWRPZ2dm46667kJiYiOHDh2PmzJn44YcfMGTIEGEdnU6H+++/H1u3bsXw4cMxZcoUlJaWYvPmzTh8+DCaN28OABg3bhyWLVuGPn364LHHHkNNTQ127NiBv/76q9bfJAwZMgQtWrTA66+/DsYYAGDz5s04e/Ysxo4di8TERBw5cgQff/wxjhw5gr/++ktolJqbm4suXbqgqKgIEyZMwM0334zLly9j7dq1qKioQLNmzZCZmYns7GyrXgLZ2dmIiIjAAw88IDquO++8EytWrMAjjzyCe++91+zb44KCAnTv3h0VFRWYPHkyYmNjsXz5cgwYMABr167Fgw8+aLatV199FQqFAv/5z39QXV1d629rmzRpgh49emDbtm0oKSlBZGSk1TovvPACWrVqhY8//hhz585FWloamjdvjoEDB+KLL77A+vXr8cEHHyA8PBzt2rUDwDU/Hz16NLKysvDGG2+goqICH3zwAW6//XYcOHDA7Fu3mpoaZGVl4fbbb8dbb72F0NBQAMDjjz+OZcuWYezYsZg8eTLOnTuHd999FwcOHMDOnTvNztXTp09j8ODBGDduHEaPHo3PPvsMY8aMQadOndC2bVuXX1O86upqrFu3DtOnTwcAjBgxAmPHjkV+fj4SExOF9RydN67sPyEhAT169MDq1asxe/Zss9tWrVoFmUwmvNbOnj3r8H2sdevWmDt3Ll5++WVMmDABd9xxBwCge/fuosfMGMOAAQOwbds2jBs3DhkZGfjll1/w7LPP4vLly1i8eLHZ+n/88Qe++eYbTJw4EREREVi6dCkeeugh5OTkIDY21u7jCwAVFRW4evWq2bKoqKhaZXA58972zz//4I477oBcLseECROQmpqKM2fO4IcffsC8efMwaNAgnDx5El999RUWL16MuLg4AEDDhg1F9+nqa/e///0vpFIp/vOf/6C4uBhvvvkmRo4cid27dwMANBoNsrKyUF1djaeffhqJiYm4fPkyfvzxRxQVFSEqKsrlx4UQb6DrMc9fj9nj7HvTJ598gsmTJ2Pw4MGYMmUKqqqq8M8//2D37t14+OGHAQBPPPEE1q5di0mTJqFNmza4du0a/vjjDxw7dgwdO3YU3f/jjz+ORo0a4fXXX8fkyZPRuXNnJCQkAAC2bNmCPn36oFmzZpgzZw4qKyvxzjvvIDMzE/v377fKzBG7rqsN/jph06ZNmDdvnug6K1aswMcff4y///4bn376KQCgQ4cOWLFiBebNm4eysjLMnz8fgPE8cOV65cSJExgxYgQef/xxjB8/Hq1atUJFRQV69OiBy5cv4/HHH0eTJk3w559/YtasWcjLy7Pq9+PovHn88ceRm5uLzZs3Y8WKFU4/Prt378bp06fx+eefQ6FQYNCgQcjOzsbzzz9vtt7mzZtx//33IykpCVOmTEFiYiKOHTuGH3/8EVOmTHFp//369UN4eDhWr16NHj16mN22atUqtG3bFrfccouwX0fX8t7+G+5IcXGx1TUPP0ZXPf3002jQoAFmz56N8+fPY8mSJZg0aRJWrVolrLNs2TI8+uijaNu2LWbNmoXo6GgcOHAAGzduxMMPP4wXXngBxcXFuHTpknB9Fx4ebnOfrr52hw4dirS0NMyfPx/79+/Hp59+ivj4eLzxxhsAgCNHjuD+++9Hu3btMHfuXCiVSpw+fRo7d+6s1WMSVLwcFCMO1NTUsKSkJNatWzez5Xy0/ZdffmGMcd+kmX77wBgXEVYqlWzu3Llmy+Dgm7mDBw8yAGzixIlm23v44Yetot4VFRVWY961axcDwL744gthmb0eBpbfzC1ZsoQBYF9++aWwTKPRsG7durHw8HChBw5/LLGxsWbf3nz33XcMAPvhhx+s9mWpoKCAhYSEmPUH6N69u5Ddwvvss89sZhTx2Ra//vorA7jaf1vr1CZTasSIEVbrij3uX331FQPAfv/9d2HZqFGjmFQqFf0mgx/TRx99xACwY8eOCbdpNBoWFxdn9e2LGIhkLk2dOpUBYDt27BCWlZaWsrS0NJaamiqcq/w3Nc2aNRM9Jmf3Z2rKlCkMADt06BBjTPwxt/UND/+YX7lyxWzc0dHRbPz48Wbr5ufns6ioKLPl/LdpM2fONFt3x44dDIDZN7WMMbZx40ar5XzGlunzWFhYyJRKJZs+fbqwrDY9pdauXSt808MY922xSqUSvtnkOXPeuPKa5s8x/ttBXps2bdjdd98t/O7s+5i9nlKWWUfffvstA8Bee+01s/UGDx7MJBKJWaYjAKZQKMyWHTp0yCybyxb+PBP7sZeFIDZmV97b7rzzThYREcEuXLhgtk3TLDB7/Sgsv7F09bXbunVrsz4lb7/9ttlzfeDAAacyIAjxdXQ9xvHE9ZgzmVLOvjc98MADrG3btnb3FxUVZfc6wtVxZmRksPj4eHbt2jVh2aFDh5hUKmWjRo0Sltm7rnNlf6bat2/PGjRoIPxumSnFmDGL21KPHj2sHqvaXK9Y9k189dVXWVhYGDt58qTZ8pkzZzKZTMZycnIYY66dN7XpKTVp0iSWkpIi/D3ctGkTA2DW86impoalpaWxpk2bWmXpm/4dtbd/y9fiiBEjWHx8vFnvory8PCaVSs3eA5y9lvfm33BbbGXbmT5Glo+LrTHz2+rVq5fZY/7MM88wmUwmZFUXFRWxiIgI1rVrV6uKFtP72eopJfae6+pr99FHHzXb5oMPPshiY2OF3xcvXmz1OYI4h2bf83EymQzDhw/Hrl27zGZKWblyJRISEnDPPfcA4GqK+R48Op0O165dQ3h4OFq1aoX9+/e7tM8NGzYAACZPnmy2fOrUqVbrmtaNa7VaXLt2DTfddBOio6Nd3q/p/hMTEzFixAhhmVwux+TJk1FWVobffvvNbP1hw4ahQYMGwu985sTZs2cd7uvrr7+GVCrFQw89JCwbMWIEfv75Z9y4cUNYtm7dOsTFxeHpp5+22gaflbRu3TpIJBKrbBDTdWrjiSeesFpm+rhXVVXh6tWrwgxf/OOu1+vx7bffon///qJZWvyYhg4dCpVKhezsbOG2X375BVevXq11b5wNGzagS5cuuP3224Vl4eHhmDBhAs6fP281S97o0aPd1oOA/0bE3ixJrti8eTOKioowYsQIXL16VfiRyWTo2rUrtm3bZnWfJ5980uz3NWvWICoqCvfee6/ZNjp16oTw8HCrbbRp00Y4jwHuG7FWrVo5dU7bk52djVtvvRU33XQTACAiIgL9+vUze+6dPW9cMWjQIISEhJh923X48GEcPXrUbAZAd76P8TZs2ACZTGb1fjZ9+nQwxvDzzz+bLe/Vq5eQ+QgA7dq1Q2RkpNOP/YQJE7B582azn/bt29dq7I7e265cuYLff/8djz76KJo0aWJ239q+57j62h07dqxZZqPlGPlMqF9++QUVFRW1GhMhvoCuxzieuh5zZizOvDdFR0fj0qVL2LNnj81tRUdHY/fu3cjNza3zuPLy8nDw4EGMGTMGMTExwvJ27drh3nvvFZ5DU2LXdbUVHh7utusdwPXrlbS0NGRlZVlt44477kCDBg3MttGrVy/odDr8/vvvZut74rypqanBqlWrMGzYMOHv4d133434+Hiza54DBw7g3LlzmDp1qlV/rdr+HR02bBgKCwuxfft2YdnatWuh1+vNrnmcuZZ3lbv/hjvy3nvvWV3z1NaECRPMHvM77rgDOp0OFy5cAMBdj5eWlmLmzJlWvaFq81y547V7xx134Nq1aygpKQFg7Df33Xff0WzuLqKglB/gG2euXLkSAHDp0iXs2LEDw4cPF5oZ6vV6LF68GC1atIBSqURcXBwaNmyIf/75B8XFxS7t78KFC5BKpWYfzACgVatWVutWVlbi5ZdfRkpKitl+i4qKXN6v6f5btGhh1eiaTyvm35x4lh/G+D9spkElW7788kt06dIF165dw+nTp3H69Gl06NABGo0Ga9asEdY7c+YMWrVqZbcB6ZkzZ5CcnGz2xuYOaWlpVsuuX7+OKVOmICEhAWq1Gg0bNhTW4x/3K1euoKSkREgTtiU6Ohr9+/cXzi+AC140atQId999d63GfOHCBdHzxdZzKHaMtVVWVgaAC7i4w6lTpwBwFzMNGzY0+9m0aZPQ+JQXEhKCxo0bW22juLgY8fHxVtsoKyuz2oblOQ1w57Uz57QtRUVF2LBhA3r06CGc66dPn0ZmZib27t2LkydPAnD+vHFFXFwc7rnnHqxevVpYtmrVKoSEhGDQoEHCMne+j/EuXLiA5ORkq/PB2fcTwLXHvkWLFujVq5fZj+nFtiscvbfxF43ufK5cfe06GmNaWhqmTZuGTz/9FHFxccjKysJ7771X6+eTEG+i6zGOJ67HnBmLM+9Nzz33HMLDw9GlSxe0aNECTz31lFXpzJtvvonDhw8jJSUFXbp0wZw5c2odAOH3a2tsV69eRXl5udlyd1/zuOt6B3D9ekXsWE6dOoWNGzda3Z+fRMjRNY87zptNmzbhypUr6NKli3C9c+7cOdx111346quvhIDBmTNnALj372jv3r0RFRVl9kXcqlWrkJGRgZYtWwrLnLmWd5W7/4Y70qVLF6trntpyNBZ3P1e1ee06GuOwYcOQmZmJxx57DAkJCRg+fDhWr15NASonUE8pP9CpUyfcfPPN+Oqrr/D888/jq6++AmPMbJaX119/HS+99BIeffRRvPrqq4iJiYFUKsXUqVM9+kJ4+umn8fnnn2Pq1Kno1q0boqKiIJFIMHz48Hp7AVrOMsJjDur0T506JXyT1qJFC6vbs7OzMWHChLoP0IStSL5Op7N5H7EMoqFDh+LPP//Es88+i4yMDISHh0Ov16N37961etxHjRqFNWvW4M8//0R6ejq+//57TJw40e4MeO7kzplaDh8+DJlM5raLPv7xXLFihVnfJZ5loNL0W3LTbVh+O2fKsjdAbc9pe9asWYPq6mosXLgQCxcutLo9Ozsbr7zySq2378jw4cMxduxYHDx4EBkZGVi9ejXuueces94D3nofM+WJx54nkUhEt2Pr9e/JsbiLM2NcuHAhxowZg++++w6bNm3C5MmTMX/+fPz1119WAVxCfBldj9nnC+9ZrVu3xokTJ/Djjz9i48aNWLduHd5//328/PLLwt+4oUOH4o477sD69euxadMmLFiwAG+88Qa++eYb9OnTx+NjdNc1j1arxcmTJ90aUHH1ekXsWPR6Pe69917MmDFDdBumgRnAM+cNP/6hQ4eK3v7bb7/hrrvuqvX27VEqlRg4cCDWr1+P999/HwUFBdi5cydef/11s/XcfS1fG954zQbyNY9arcbvv/+Obdu24aeffsLGjRuxatUq3H333di0aZPN+xMKSvmNkSNH4qWXXsI///yDlStXokWLFujcubNw+9q1a3HXXXfhf//7n9n9ioqKXG4417RpU+j1eiE7iHfixAmrddeuXYvRo0ebfcitqqpCUVGR2XqupFU2bdoU//zzD/R6vdmH++PHjwu3u0N2djbkcjlWrFhh9Sbxxx9/YOnSpcjJyUGTJk3QvHlz7N69G1qt1mazzubNm+OXX37B9evXbWZL8RF1y8fH8psLe27cuIGtW7filVdewcsvvyws5zN6eA0bNkRkZCQOHz7scJu9e/dGw4YNkZ2dja5du6KiogKPPPKI02Oy1LRpU9Hzxd3PoaWcnBz89ttv6Natm9u+OeS/oY6Pj6/1N0DNmzfHli1bkJmZ6baLUVdTlbOzs3HLLbeIlpd+9NFHWLlyJV555RWnzxtX9z9w4EA8/vjjwjeHJ0+exKxZs8zWcfZ9zNX3ky1btqC0tNTsnPD0uSimQYMGot/Gu/L6N9WsWTMAcOtz5anXbnp6OtLT0/Hiiy/izz//RGZmJj788EO89tprtdoeId5C12Peef905b0pLCwMw4YNw7Bhw6DRaDBo0CDMmzcPs2bNEsp+kpKSMHHiREycOBGFhYXo2LEj5s2b53JQit+vrbHFxcUhLCzMpW06a+3ataisrLQqn6sLd1yvNG/eHGVlZXXKmrHkynlbXl6O7777DsOGDcPgwYOtbp88ebIwyRF/jXf48GG743X1mmfYsGFYvnw5tm7dimPHjoExZla65+y1vKv79tb1t5gGDRpYvf9oNBrk5eXVanumzxXfhkKMs4+Xp167UqkU99xzD+655x4sWrQIr7/+Ol544QVs27bNra+JQEPle36C/xbu5ZdfxsGDB82+lQO4yK1lJHnNmjW4fPmyy/vi/yAvXbrUbLnlbBm29vvOO+9YRcH5F7Xlm5OYvn37Ij8/3yzttaamBu+88w7Cw8OtZrOorezsbNxxxx3CHy3Tn2effRYA8NVXXwEAHnroIVy9ehXvvvuu1Xb443/ooYfAGBPNNuHXiYyMRFxcnFU9/fvvv+/0uPkAmuXjbvn8SKVSDBw4ED/88AP27t1rc0wAl+0zYsQIrF69GsuWLUN6erow81xt9O3bF3///Td27dolLCsvL8fHH3+M1NRUtGnTptbbtuX69esYMWIEdDodXnjhBbdtNysrC5GRkXj99deh1Wqtbr9y5YrDbQwdOhQ6nQ6vvvqq1W01NTVOvS4sufKaunjxIn7//XcMHTrU6lwfPHgwxo4di9OnT2P37t1Onzeu7B/gykSzsrKwevVqfP3111AoFBg4cKDZOs6+j7n6fqLT6axeu4sXL4ZEIqmXb8V5zZs3x/Hjx83OmUOHDtV6VpaGDRvizjvvxGeffYacnByz20wfR1cfL3e+dktKSlBTU2O2LD09HVKpFNXV1S5tixBfQNdj7r8ec4az703Xrl0zu59CoUCbNm3AGINWq4VOp7MqjYqPj0dycnKt3pOSkpKQkZGB5cuXmz2mhw8fxqZNm9C3b1+Xt+mMQ4cOYerUqWjQoAGeeuopt23XHdcrQ4cOxa5du/DLL79Y3VZUVGT1N8EZrpy369evR3l5OZ566inRa577778f69atQ3V1NTp27Ii0tDQsWbLEatu1/TsKcP0pY2JisGrVKqxatQpdunQxy+B39lre1X174/rblubNm1t93vn444/tVofYc9999yEiIgLz589HVVWV2W2Wz5Uz5Y+eeO1ev37dallGRgYA0DWPA5Qp5SfS0tLQvXt3fPfddwBgdRF0//33Y+7cuRg7diy6d++Of//9F9nZ2cI36a7IyMjAiBEj8P7776O4uBjdu3fH1q1bcfr0aat177//fqxYsQJRUVFo06YNdu3ahS1btlhNnZ6RkQGZTIY33ngDxcXFUCqVQsNBSxMmTMBHH32EMWPGYN++fUhNTcXatWuxc+dOLFmyxC0ZMPw0sabTOptq1KgROnbsiOzsbDz33HMYNWoUvvjiC0ybNg1///037rjjDpSXl2PLli2YOHEiHnjgAdx111145JFHsHTpUpw6dUpIv92xYwfuuusuYV+PPfYY/vvf/+Kxxx7Drbfeit9//13o5+OMyMhI3HnnnXjzzTeh1WrRqFEjbNq0CefOnbNa9/XXX8emTZvQo0cPTJgwAa1bt0ZeXh7WrFmDP/74w6yp46hRo7B06VJs27ZNmNq0tmbOnImvvvoKffr0weTJkxETE4Ply5fj3LlzWLduXZ3LAk+ePIkvv/wSjDGUlJTg0KFDWLNmDcrKyrBo0SL07t27Tts3FRkZiQ8++ACPPPIIOnbsiOHDh6Nhw4bIycnBTz/9hMzMTNFgpakePXrg8ccfx/z583Hw4EHcd999kMvlOHXqFNasWYO3335b9Ns8e1x5Ta1cuRKMMQwYMEB0W3379kVISIiQKefMeePK/nnDhg3D//3f/+H9999HVlaWVVNRZ9/HmjdvjujoaHz44YeIiIhAWFgYunbtKlqy2b9/f9x111144YUXcP78ebRv3x6bNm3Cd999h6lTp1r1avGkRx99FIsWLUJWVhbGjRuHwsJCfPjhh2jbtq3QJNNVS5cuxe23346OHTtiwoQJSEtLw/nz5/HTTz/h4MGDALiSIwB44YUXMHz4cMjlcvTv31/0G0B3v3Z//fVXTJo0CUOGDEHLli1RU1MjZKeaTjBBiL+g6zH3Xo+ZWrdunZDRYWr06NFOvzfdd999SExMRGZmJhISEnDs2DG8++676NevHyIiIlBUVITGjRtj8ODBaN++PcLDw7Flyxbs2bNHtLTdGQsWLECfPn3QrVs3jBs3TphWPioqCnPmzKnLQwIA2LFjB6qqqoTG+Tt37sT333+PqKgorF+/XrS1QG2543rl2Wefxffff4/7778fY8aMQadOnVBeXo5///0Xa9euxfnz513OGuT/jk2ePBlZWVnCxANisrOzERsbi+7du4vePmDAAHzyySf46aefMGjQIHzwwQfo378/MjIyMHbsWCQlJeH48eM4cuSIEFhzZf8ANyHAoEGD8PXXX6O8vBxvvfWW2e2uXMt78294XTz22GN44okn8NBDD+Hee+/FoUOH8Msvv7j83PMiIyOxePFiPPbYY+jcuTMefvhhNGjQAIcOHUJFRQWWL18OgHu8Vq1ahWnTpqFz584IDw9H//79Rbfp7tfu3Llz8fvvv6Nfv35o2rQpCgsL8f7776Nx48ZmzeeJCI/P70fc5r333mMAWJcuXaxuq6qqYtOnT2dJSUlMrVazzMxMtmvXLqvpfZ2ZgpgxxiorK9nkyZNZbGwsCwsLY/3792cXL160mt7zxo0bbOzYsSwuLo6Fh4ezrKwsdvz4cavpPhlj7JNPPmHNmjVjMpnM4TTpBQUFwnYVCgVLT0+3mv6dP5YFCxZYPR6W47T09NNPMwDszJkzNteZM2cOA8AOHTrEGOOmbn3hhRdYWloak8vlLDExkQ0ePNhsGzU1NWzBggXs5ptvZgqFgjVs2JD16dOH7du3T1inoqKCjRs3jkVFRbGIiAg2dOhQVlhYaDVm/nkRm1b00qVL7MEHH2TR0dEsKiqKDRkyhOXm5ooe94ULF9ioUaNYw4YNmVKpZM2aNWNPPfWU2RSwvLZt2zKpVMouXbpk83GxBEB0auUzZ86wwYMHs+joaKZSqViXLl3Yjz/+aLaOM9Mdi+2P/5FKpSw6Opp16NCBTZkyhR05csRqfbFznp9+ds+ePWbr2nvMt23bxrKyslhUVBRTqVSsefPmbMyYMWzv3r3COramXeZ9/PHHrFOnTkytVrOIiAiWnp7OZsyYwXJzc4V1mjZtyvr162d1X7HXia3XlKX09HTWpEkTm+NijLGePXuy+Ph4ptVqGWPOnTeuvKYZY6ykpISp1WqrKcZ5zr6PMcZNGd2mTRsWEhJi9vyOHj3aairg0tJS9swzz7Dk5GQml8tZixYt2IIFC8ymEGbM9rks9n5myd77kakvv/ySNWvWjCkUCpaRkcF++eUXqzG7+t52+PBh4f1ApVKxVq1asZdeeslsnVdffZU1atSISaVSs6mlxY6tLq9dy9fb2bNn2aOPPsqaN2/OVCoVi4mJYXfddRfbsmWL3ceJEF9G12Ofm61Tl+sxxozvJ7Z++OntnXlv+uijj9idd97JYmNjmVKpZM2bN2fPPvssKy4uZowxVl1dzZ599lnWvn17FhERwcLCwlj79u3Z+++/b3eMpuMUu2bZsmULy8zMZGq1mkVGRrL+/fuzo0ePmq1j7xrDmcdFLpezhg0bsjvvvJPNmzePFRYWWt2Hv77h3+MZs31t0qNHD9a2bVvRfdfleoUx7u/urFmz2E033cQUCgWLi4tj3bt3Z2+99RbTaDSMMdfOm5qaGvb000+zhg0bMolEYvU64RUUFLCQkBD2yCOPiN7OGHcdHhoayh588EFh2R9//MHuvfde4Zxo164de+edd5zav61zfPPmzQwAk0gk7OLFi1a3u3It762/4bbYuo42pdPp2HPPPcfi4uJYaGgoy8rKYqdPn7Yas61t8WO0vLb9/vvvWffu3YXXWpcuXdhXX30l3F5WVsYefvhhFh0dzQAI11e2jq0ur13L19vWrVvZAw88wJKTk5lCoWDJyclsxIgR7OTJk3YeTcIYYxLGfKh7GCHEqzp06ICYmBhs3brV20MhhBBCCCGEEBLgqKcUIQQAsHfvXhw8eBCjRo3y9lAIIYQQQgghhAQBypQiJMgdPnwY+/btw8KFC3H16lWcPXtWmJ2GEEIIIYQQQgjxFMqUIiTIrV27FmPHjoVWq8VXX31FASlCCCGEEEIIIfWCMqUIIYQQQgghhBBCSL2jTClCCCGEEEIIIYQQUu8oKEUIIYQQQgghhBBC6l2ItwdQ3/R6PXJzcxEREQGJROLt4RBCCCHExzHGUFpaiuTkZEilwft9Hl1DEUIIIcRZzl4/BV1QKjc3FykpKd4eBiGEEEL8zMWLF9G4cWNvD8Nr6BqKEEIIIa5ydP0UdEGpiIgIANwDExkZ6eXREEIIIcTXlZSUICUlRbiGCFZ0DUUIIYQQZzl7/RR0QSk+3TwyMpIuqAghhBDitGAvWaNrKEIIIYS4ytH1U/A2RiCEEEIIIYQQQgghXkNBKUIIIYQQQgghhBBS7ygoRQghhBBCCCGEEELqXdD1lCKEEEIIIYQQQnyVTqeDVqv19jAIsUsul0Mmk9V5OxSUIoQQQgghhBBCvIwxhvz8fBQVFXl7KIQ4JTo6GomJiXWaDIaCUoQQQgghhBBCiJfxAan4+HiEhoYG/ayvxHcxxlBRUYHCwkIAQFJSUq23RUEpQgghhBBCCCHEi3Q6nRCQio2N9fZwCHFIrVYDAAoLCxEfH1/rUj5qdE4IIYQQQgghhHgR30MqNDTUyyMhxHn8+VqXHmgUlCKEEEIIIYQQQnwAlewRf+KO85WCUoQQQgghhBBCCCGk3lFQihBCCCGEEEIIIfWmZ8+emDp1ap23c+3aNcTHx+P8+fN13hZv2bJliI6OdmrdOXPmICMjw237tue2227DunXr6mVf9YmCUoQQQgghhBBCCPE78+bNwwMPPIDU1FRvD8XjXnzxRcycORN6vd7bQ3ErCkoRQgghhBBCCCGkzjQaTb3tq6KiAv/73/8wbty4etunN/Xp0welpaX4+eefvT0Ut6KgFCGEEEIIIYQQQlzWs2dPTJo0CVOnTkVcXByysrIAAIcPH0afPn0QHh6OhIQEPPLII7h69arN7UgkEnz77bdmy6Kjo7Fs2TKb99mwYQOUSiVuu+02AIBer0fjxo3xwQcfmK134MABSKVSXLhwAQCwaNEipKenIywsDCkpKZg4cSLKyspqcfTW9Ho95s6di8aNG0OpVCIjIwMbN24UbtdoNJg0aRKSkpKgUqnQtGlTzJ8/HwDAGMOcOXPQpEkTKJVKJCcnY/LkycJ9ZTIZ+vbti6+//totY/UVFJQihBBCCCGEEEJ8CGMMFZoar/wwxlwa6/Lly6FQKLBz5058+OGHKCoqwt13340OHTpg79692LhxIwoKCjB06FC3PkY7duxAp06dhN+lUilGjBiBlStXmq2XnZ2NzMxMNG3aVFhv6dKlOHLkCJYvX45ff/0VM2bMcMuY3n77bSxcuBBvvfUW/vnnH2RlZWHAgAE4deoUAGDp0qX4/vvvsXr1apw4cQLZ2dlC6eG6deuwePFifPTRRzh16hS+/fZbpKenm22/S5cu2LFjh1vG6itCvD0AQgghhBBCCCGEGFVqdWjz8i9e2ffRuVkIVTgfKmjRogXefPNN4ffXXnsNHTp0wOuvvy4s++yzz5CSkoKTJ0+iZcuWbhnnhQsXkJycbLZs5MiRWLhwIXJyctCkSRPo9Xp8/fXXePHFF4V1TBusp6am4rXXXsMTTzyB999/v85jeuutt/Dcc89h+PDhAIA33ngD27Ztw5IlS/Dee+8hJycHLVq0wO233w6JRCIEygAgJycHiYmJ6NWrF+RyOZo0aYIuXbqYbT85ORkXL16EXq+HVBoYOUZePYrff/8d/fv3R3Jysmi6npjt27ejY8eOUCqVuOmmm+ym8xFCCCGEEEIIIcRzTLOVAODQoUPYtm0bwsPDhZ+bb74ZAHDmzBm37beyshIqlcpsWUZGBlq3bi1kS/32228oLCzEkCFDhHW2bNmCe+65B40aNUJERAQeeeQRXLt2DRUVFXUaT0lJCXJzc5GZmWm2PDMzE8eOHQMAjBkzBgcPHkSrVq0wefJkbNq0SVhvyJAhqKysRLNmzTB+/HisX78eNTU1ZttSq9XQ6/Worq6u01h9iVczpcrLy9G+fXs8+uijGDRokMP1z507h379+uGJJ55AdnY2tm7disceewxJSUlC7SohhBBCCCGEEOLP1HIZjs71zmdctVzm0vphYWFmv5eVlaF///544403rNZNSkoS3YZEIrEqG9RqtXb3GxcXhxs3blgtHzlyJFauXImZM2di5cqV6N27N2JjYwEA58+fx/33348nn3wS8+bNQ0xMDP744w+MGzcOGo0GoaGhdvdZVx07dsS5c+fw888/Y8uWLRg6dCh69eqFtWvXIiUlBSdOnMCWLVuwefNmTJw4EQsWLMBvv/0GuVwOALh+/TrCwsKgVqs9Os765NWgVJ8+fdCnTx+n1//www+RlpaGhQsXAgBat26NP/74A4sXL6agFCGEEEIIIYSQgCCRSFwqofMlHTt2xLp165CamoqQEOeOoWHDhsjLyxN+P3XqlMPMpQ4dOuDLL7+0Wv7www/jxRdfxL59+7B27Vp8+OGHwm379u2DXq/HwoULhfK31atXOzVGRyIjI5GcnIydO3eiR48ewvKdO3ealeFFRkZi2LBhGDZsGAYPHozevXvj+vXriImJgVqtRv/+/dG/f3889dRTuPnmm/Hvv/+iY8eOALgG8h06dHDLeH2FX53lu3btQq9evcyWZWVlmdWEWqqurjZLbSspKfHU8PxOUVURXvrzJVyvvO7toRBP0VYAJblAZCNAbieaXl4IaMqBBk3hkareqiKg/Cq3fanc9nrFF4HqUvfv39vkaqBBKgCJ+O1MD9w4B9RYpOFKpEBkEqCMsr3tyhtAWT7gYkPKOpPJgQZpgNTWnxEGFF0ENJYzmUiA8IZAaJxr+6u8DpQVOHecqmggMtn27ToNUJQDRCQCinDX9+nM81l0AVBFAeoYx+M12+cNoOoGEN0UkNj6lpIBxZe45yA80fa2nD1OMWUF3P2jGsNzx1kERDexc5yuuaPxHXii/RNu2Rapf7+dvIIlW07iluQovDrwFm8PhxBCSB089dRT+OSTTzBixAjMmDEDMTExOH36NL7++mt8+umnkMms//bffffdePfdd9GtWzfodDo899xzQnaQLVlZWZg1axZu3LiBBg0aCMtTU1PRvXt3jBs3DjqdDgMGDBBuu+mmm6DVavHOO++gf//+QnN2d3n22Wcxe/ZsNG/eHBkZGfj8889x8OBBZGdnA+Bm/ktKSkKHDh0glUqxZs0aJCYmCjMN6nQ6dO3aFaGhofjyyy+hVqvN+k7t2LED9913n9vG6wv8KiiVn5+PhIQEs2UJCQkoKSlBZWWlaArb/Pnz8corr9TXEP3K75d/x/aL2709DFIfik85t97Vw54dx/VjjtcJjH595nSlwNV/Ha8nduylFwBHcToJbMYNPIbVANePOl5P7JgqcrkfVzl7nJqrgJ0phwUlZ2u3T2efz/JSoPyS4/XEXDvieB0tgKpCx+s5c5y2XLVOibfi6eN0UlpUmtu2RepfaZUWB3KKoJAF4h8BQggJLny20HPPPYf77rsP1dXVaNq0KXr37m2zOffChQsxduxY3HHHHUhOTsbbb7+Nffv22d1Peno6OnbsiNWrV+Pxxx83u23kyJGYOHEiRo0aZRYnaN++PRYtWoQ33ngDs2bNwp133on58+dj1KhRdT9wAJMnT0ZxcTGmT5+OwsJCtGnTBt9//z1atGgBAIiIiMCbb76JU6dOQSaToXPnztiwYQOkUimio6Px3//+F9OmTYNOp0N6ejp++OEHofTw8uXL+PPPP0Wzw/yZhLk636OHSCQSrF+/HgMHDrS5TsuWLTF27FjMmjVLWLZhwwb069cPFRUVokEpsUyplJQUFBcXIzIy0q3H4G+yj2Xjv3//Fx3jO2LsLWO9PRziCXs+A05tAtLuBLpNFF+HMWDVSECvB9IHcz/upK0E1j7K7afDSKB1f/H1ii8DP00H5Cqg+9PuHYM3Hf8JKDgKtB8GtH1QfJ39K7j1GncCmt/NLassAv7+BAhRAoM/A6Qi2SSacmDtOO7/mZO5detDzl/AuR32z6vTW7nxx6QC6YbGkkwP/LEE0OuAAUuB8Hjn9lddCqwbz/3/9imATGF73X3LgLIrQM+ZQHKG+Dq/vgbkH+Yynh76HyB2cWS2z6lcVhIAHP0euHIC6PB/QOv7xbe/93PgpGG2nAfeA8JiHRygQVUJ8M0E7v/NewJdbWT9nPyF2wcA3DULSGovvt6WV4DCY4AiFBj0qfhxiik4Amx9lft/p9FAKxtl9ns+BU5t4f7/4AeAuoH4epYqi4D1hmO76R6gy3jn7udAYlgibo652S3bslRSUoKoqKigv3bw5OOw5WgBHvtiL9qnROO7pzId34EQQgJMVVUVzp07h7S0NKvm3cS2n376Cc8++ywOHz4cMLPR2fLcc8/hxo0b+Pjjj709FIG989bZ6wa/ypRKTExEQUGB2bKCggJERkbabPSlVCqhVNbTBzU/U2Yoq0mLSkPPlJ7eHQzxjAPfAJWVQI0EsPUcVxUD5eXc/6/n2V6vts7tAPh6cGmE7e3rd3FjVSUCt0137xi8iamA8/uA6wW2j33T69yxt/0/IGMEt0yvB/5aDpQWAYo48cDD6a3c/RqkArc/76EDEBGzCTi6Ccg/bfuY9q/hxpbWx/z5/HcDkLsfqJE6f66d2sxtK6Y5kDnL/rp5p4BDX3HntNj29Trg8jFAU2k83xLaWK938hfu9tgWQOZM43KtBMh5BSgqtD3+DbO5+wKATub8cZ7YaLxfwXnb99uz0rheRZX4eroaIPc4FxSurARCGwMNnZx++fw+4/aLrtgex48vmhxniPPHeexH4/0KL7j/PYf4JbWCC7xXaXReHgkhhBB/0q9fP5w6dQqXL19GSkqKt4fjUfHx8Zg2bZq3h+F2fhVK7NatG7Zu3Wq2bPPmzejWrZuXRuTfyrVcICJc7mKvEeI/9IYpRCvs9A0zve3SHi4Y4k6X/hbflyW+t1moi71pfF1jQ1PDS3+L90Oq0QC5B7n/pxgbIEIq5TKnAODi31Z347a513wf9aXxrdy/18/Yfk4v7uH+TbEYG//7pT3O748/fsttiY6ts/3tXzlu3ufK1nqXHI1/r/j9NBVAgUkZrCvHafpauXKcCxjbG5vlfUwVHuF6yjlaT8zFPeL/N1Vdxu1DWM+F7ZuOpfBIYPaRIy5TGWZ6qtRSUIoQQohrpk6d6rGAVNu2bREeHi76w/eJqi/Tp0+3amcUCLwalCorK8PBgwdx8OBBAMC5c+dw8OBB5OTkAABmzZplVtv5xBNP4OzZs5gxYwaOHz+O999/H6tXr8YzzzzjjeH7vTIt98EsTBHmYE3it3SGaVTtNbOvNOkZU1UMXHOy/5SzTD/UVtrpT8Pf5mwJkL9IaseVm1VcA66L9PbJ/xfQVXONomOamd/mKMDCf7jn16svoTFA7E2GMYiMrfIGcPUE9/9Gt5rfxo+1NkGMxrfaX890+5f3iQdYLfdrK1hz0cY+kztwjblLLnMlp5byDhqDwWL7s8dsXSYe+Cq/an4e2QokW+7X2XEwZv6YFOcApfnW6+Xu58oxTcfhLNP3BKYHLu93/r4kYKkpKEUIIcQHbdiwQYhZWP6YNlAntefVoNTevXvRoUMHYUrDadOmoUOHDnj55ZcBAHl5eUKACgDS0tLw008/YfPmzWjfvj0WLlyITz/9FFlZWV4Zv7/jy/coUyqA6fmglL1gkEXAypUPl44wZr49e+PgM25cncXL14UojaV3YkEG08CSxKKjdmM7WUV6vUk2Tz0HpQDj2MSCHZcNTSkbpHEz7ZndzzDW/H+40jJH9Drg0j7zfdoT3waQhwHVJVy2kSX+MUtIN4xf7LHVGY/Bcp+KMCChrWFbIsfOPx789vMOWc+qKEavMwZn+PuKni+G8cY0A0LUhkDyadvrJdxie1tibpzjAqgyBVe6aLotU8JxGrafe8AYBLdHp+XWNRubG99ziN9SyblL0ioKShFCCPEhTZs2xU033ST6ExER4e3hBQSvBqV69uwJxpjVz7JlywAAy5Ytw/bt263uc+DAAVRXV+PMmTMYM2ZMvY87UPCZUhSUCmA6k6CUrbK8CotAkSuZHY7cOAdUmMyCFozle4B5CZ8le4Elvnzv+lkuQ8bUtdNcQCJEbfxwX59S7GRx2SrdA4DoJkB4ApdNlHfI8X6unAA0pVygKV6k95MlWQjQqKPtsfHLbnuS+/fqCa7xttk+DSV+inAgvrX1NuyV8PHbbzcECI3lsuDynZipr/AooC0HlJFcE3XA/vnSpDuXteVovdsmGrdfVeJ4HPxzl9QeSDU0mxZ7T+CPvf0IQBUN1FQ5d5wFh4GaSkAVBWQ8bD5WEtSEnlIUlCKEEEKCil/1lCLuxfeUCpNT+V7A4suImB6ottGfhg8GKQzBSXd+QOQ/4PLbtldGGKiZUoCxBMxeAEesBE/dAIgzNKe2DIDwgYjkDsaZ4eqTWZmcxYfIS3aOSSJxrYSP31ajjlzAyZWxWQZrKq4DV09y/2/Zm8vkAoDLFo8tP65GHcVnPbQ1ftPMwMZdXDtO032aBr0sg8lCf63Ots8r0xK/m/sCUU0AMK7kzhEhc89k/JbnnmmJX0pX++e3Jf58b3Qrd1/+fr4xETDxIr58T6tj0Orc3NuQEEIIIT6LglJBjDKlgoBpOY2tLCV+ebOe3L+Fx5zLqHAG/yGV33bFddsfPgM5U4oPMuQfBjTlxuWl+VzPHkiARp3E72sry8o0OOEN8W24YKOmjDtneHq9SQN2G2Nz1CvLVG36ZvGPt2VpHl+SF9McCIu1vZ5pYEkMP5a8g+aleUU5QFkBIA0BkjNcPE6TpvWJ6YbSvCLz0jxdjbHEr3EXx+OPa8UFNvlzxFbTcrH7pnQ2Hr9lad71s8YSv6R29stMbW6/C5DYDpApbfdbI0GFb3QOULYUIYQQEkwoKBXEhNn3FBSUClg6jfH/tvo58cGghq240iow44f3uuIDCi0Nfd901eYzgpmqCNBG5wAQ1RiISAaYzthPBzB+QI9vAyht1KSn2Mi2cRT48TSpTLxM7upJLivPXlmhabDGUYaMvVJAW/jm6paleZYz6tkKGtmaeY8X08xQmqcxL1nj75fYDpCrXZtpUMg86sJlvomV5l05xpX4KSK41ys/fsvSPMtMNXvlo6Y05VzglL9v7E2G0rxKi+M0nHtJ7bmeabbOUXvH2bgzEKIw6bdGJXzBThkiFdrqVWkpU4oQQggJFhSUCmKUKRUETGcBc5QppY5xLePBEdMPuM3uAqRy++MI5EwpQLzEyZlsJ6FMbr+xTK6qhAtEAM41//YUsaCOM+V2yR24bKLSPKD4ku3tVxbZnsXPnvCG4qV5ljPqmZan8WVypiV+tvZpqwTRMhiU3BGQSIHii0BJnu3xVlw3ZkTxGXP2zpfGnbigYESieGme5XnlbBAw9wAXOI1I5gKpUqnJOEweR9MSP2HMEqDoAlBWaHv7ZVeAG+fNj1PI9nJjLzvilyQSCVQh1FeKEEIICTYUlApSjDFh9j3qKRXATEtubPVzMg0GuZLZ4UjuQcMH3CTuAy4fbLI1jkDuKQWIl1o5KhMDgIY3c5kx2nJjIOryPgCMy2yLSPDIcJ0iNgOfM+V2ilDnZl7jA0pis/g5Yvl46/XWM+ol3ALIQ7nMLj4QZVniZ4tY36qLJtlOAKAMB+L5mfrsHCcf8IltYXydOHu+WJbmWZb4AYZyQBWXLXntjJ1xiDTdF8uysgx6qaK489R0G/a23/BmQB1t2L4LJY4k4PHNzispKEUIIQFhzJgxGDhwoLeHAQBITU3FkiVL7K4jkUjw7bff2l3n2rVriI+Px/nz5902tmXLliE6OtqpdefMmYOMjAy37due2267DevWrfP4figoFaSqdFXQMe6ij8r3ApjehZ5S6hjz7Iy6Nh6+ZJKVIpEYg01i42AsCDKlTD7cM8YFDPlSPnsBHKnMOAsfHwzwdukej9//tVPG59XZcjtnghH8cbpSumdr+1dPANUl5rP4yUK4bCbT9RyV7vEsZ+DTVgL5/5jvGzCZpdBOJtAli2CW6TZMS/PEGshbBo0sS/wAQ5lchvk2xIg13bcszdOUAwVHHK8nRixgyf+/4Ih5vzUSlPhm55UaCkoRQkggePvtt7Fs2TJvDwMAsGfPHkyYMKHO25k3bx4eeOABpKam1n1QPu7FF1/EzJkzobc1i7ubUFAqSPH9pCSQQB2i9vJoiMfoTMr3bGZKGXo5hcYACaYZFafF13fWRYusDiFTSqS3lbbC2P8qEHtKAVzvHKkcKL/ClTnl/wvUVHE9e2Jvsn9fy1nQLMunvCUsluuvBHAZRlXFwJXj3O+OAmbOlG1ddCLryhbL0jzT2e1MywqFQOzfFvt0UC5oWZqXd4grlw2LN/RmsxiHvSbjYvu0LM0zLfEzXc+yNM+yxM/WcVoynVHP9LyyLM2zLPETG4fN4xQJekU1AiIbcdu8vF/8fiRoKOXcZSmV7xFCSGCIiopyOgPI0xo2bIjQ0NA6baOiogL/+9//MG7cODeNyrf16dMHpaWl+Pnnnz26HwpKBSnT0j2phE6DgOVMphQfJFLHOJ9R4Qhj1hknfLBJLDjGj00q52Z0C0RyFTdTGcB9OBeyXm7levfYY5llJZYx4y2mJXxCWWFTIDzewf0MQZL8f8xnsOM5M4ufPQltuWbrfGmerbJC0zI5sRI/W8xK8/42L93juzWbbifvIFCjgRW9zrrcThibSUCLf85jbzLPJrQszbN1btiaqY9XdIELmErlxubjgHVpnq2gHT/2y/vNg+E8XY2x75VlFppY/ywSlIRMKQpKEUKI31i7di3S09OhVqsRGxuLXr16obycS4CwLN8rLS3FyJEjERYWhqSkJCxevBg9e/bE1KlThXVSU1Px2muvYdSoUQgPD0fTpk3x/fff48qVK3jggQcQHh6Odu3aYe/evWbjWLduHdq2bQulUonU1FQsXLjQ7HbL8r1Tp07hzjvvhEqlQps2bbB582aHx7phwwYolUrcdtttAAC9Xo/GjRvjgw8+MFvvwIEDkEqluHDhAgBg0aJFSE9PR1hYGFJSUjBx4kSUlZU53J8z9Ho95s6di8aNG0OpVCIjIwMbN24UbtdoNJg0aRKSkpKgUqnQtGlTzJ8/HwDX0mfOnDlo0qQJlEolkpOTMXnyZOG+MpkMffv2xddff+2WsdpC0YggRTPvBQlHs+/ptFxJE2DSy8aFmbRsKboAlBeaf8Dlt18hMg7T0j3TD/SBxrSRvDP9pIT7GT60XzvN3a/yBheISEz3zDhdkWKSIeNKuV2DNCA0jjtH8w5Z337tlONZ/OyRyc1nB7Q1Nj54c+U4t55liZ89ZsduIxgU25wLyNZUAQX/wsqV44CmlAvGxre2GJsT54tlINnWevzvhUeAapGLID5YldSOC6DaPE4bj2NcS0AZxc3UV3DYevuFR7iMSGUkENfK9nGSoMYHpShTihBCwH0RqSn3zo+TbTzy8vIwYsQIPProozh27Bi2b9+OQYMGgdm4/7Rp07Bz5058//332Lx5M3bs2IH9+60zpRcvXozMzEwcOHAA/fr1wyOPPIJRo0bh//7v/7B//340b94co0aNEvazb98+DB06FMOHD8e///6LOXPm4KWXXrJZOqjX6zFo0CAoFArs3r0bH374IZ577jmHx7tjxw506tRJ+F0qlWLEiBFYuXKl2XrZ2dnIzMxE06ZNhfWWLl2KI0eOYPny5fj1118xY8YMh/tzxttvv42FCxfirbfewj///IOsrCwMGDAAp06dAgAsXboU33//PVavXo0TJ04gOztbKD1ct24dFi9ejI8++ginTp3Ct99+i/R0888XXbp0wY4dO9wyVltsTI1EAh3NvBckHJXvCYEqCZcRAbjnAyL/ATcxHZAbykP5nlL2MqUCtck5r/GtwG5wmTX8MdubeY8XGsNlyFw7Dfxl+CYmKYMLSHibMDvgPq6czXSZPRIJF9g4sYELgFoGOWyV27k6tgs7gdObjWWFljPqhcdzmV1FF4DdH7q2z8adgb2fced70QXjMlP8TH2nNnHrNepkfrvpcZqW25lu69IeLtgDiJ8vjW8FLv4FnNwoXuIHAJFJQGRjoOQSl7GUdqf57fZKQht3BvZ/wY2fnw3Rcj2plCsZPPMrN97kDBvH2ck6M9C0lJOxwA5ME7uo0TkhhJjQVgCvJ3tn38/nAgrHk2Hl5eWhpqYGgwYNEgIwlkENXmlpKZYvX46VK1finnvuAQB8/vnnSE62Psa+ffvi8ccfBwC8/PLL+OCDD9C5c2cMGTIEAPDcc8+hW7duKCgoQGJiIhYtWoR77rkHL730EgCgZcuWOHr0KBYsWIAxY8ZYbX/Lli04fvw4fvnlF2H/r7/+Ovr06WP3eC9cuGA13pEjR2LhwoXIyclBkyZNoNfr8fXXX+PFF18U1hHLBHviiSfw/vvv292fM9566y0899xzGD58OADgjTfewLZt27BkyRK89957yMnJQYsWLXD77bdDIpEIzxMA5OTkIDExEb169YJcLkeTJk3QpYv5NV5ycjIuXrwIvV4PqaPqjlqiTKkgRTPvBQlH5Xv8MlWU8QOxaYPl6tLa7VesWTRfvic2jkBvcs7jH4+8Q4YghsQ6SGELHwQ4+p3hdwc9j+pLfFsus6i6BDi7nVvmbLmdvbItZ2bxc7h9w32P/cj9a2sWvxTLx9bZ8ZsEcEvzAGkIkNzB/nqWhBJFkWCQUJp3HTi/0/bY+PEf+4H717LET1jPThakaTmprfHn7BIv8bNcz95ximXRJbbjtllxFbhx3vp2EjSUIXyjc882VCWEEOIe7du3xz333IP09HQMGTIEn3zyCW7cEKmKAHD27FlotVqzoEdUVBRatWpltW67du2E/yckcDNNmwa7+GWFhYUAgGPHjiEzM9NsG5mZmTh16hR0OusvOo4dO4aUlBSzAFO3bt0cHm9lZSVUKvOM8oyMDLRu3VrIlvrtt99QWFgoBNAALgh2zz33oFGjRoiIiMAjjzyCa9euoaKiwuE+7SkpKUFubq7osR87dgwAV0J58OBBtGrVCpMnT8amTZuE9YYMGYLKyko0a9YM48ePx/r161FTY96GQa1WQ6/Xo7papN2Gm1CmVJCiTKkgoTMJSomV74kFgyKTgKgUroHz5f1Asx6u71csoBDqTKZUgDY550WlAOGJQFk+93vDm40Zao6kdAYOreQaQgO1m5HOE2QhXJbP+R3c2FwpK6xtEMNZ/H0dPWaNuwD/rnH9seVL8/jXVsItgEKkgaa9GfjEZt7j8aV5F//ixqYIFy8rbGxxnLZKQht3AY6sNz62PG0l13jf1jj40rzqYu53sRI/wEHQy04mllzFBbku7+XOhZg08fGTgMdnSlH5HiGEAJCHchlL3tq3E2QyGTZv3ow///wTmzZtwjvvvIMXXngBu3fvRlpa7f+ey+Vy4f8SQwa12DJPzwpnKS4uTjToNnLkSKxcuRIzZ87EypUr0bt3b8TGxgIAzp8/j/vvvx9PPvkk5s2bh5iYGPzxxx8YN24cNBpNnZuvO9KxY0ecO3cOP//8M7Zs2YKhQ4eiV69eWLt2LVJSUnDixAls2bIFmzdvxsSJE7FgwQL89ttvwuN9/fp1hIWFQa323ORolCkVpPigFGVKBTi9SaTbXqaUZdlcYzsfoh0x/YBrGpTi9yGaKWUyA2Agk0jMM1GcKd3jWWbIeHvmPVOmx5Tcgevn5IxGhhnsSi4DxZeNy6uKgcJjhm3XIVOKL80TxmljW5bZQZYlfrbwpXmOtp/cEdwMdjlAaYFxecV1rgm7vX2ajk2sxA8wlubxbJ1Xpq9r014PuQe494rwRC5waokvzRO2Y+Pc47P+bpwDyq4Yl5dfBa6fNdzXRmagM7MxkoCnNsy+R+V7hBAC7jpDEeadHxdK6SUSCTIzM/HKK6/gwIEDUCgUWL9+vdV6zZo1g1wux549xi8ji4uLcfLkyTo/VK1bt8bOnTvNlu3cuRMtW7aETGZ97dS6dWtcvHgReXl5wrK//vrL4X46dOiAo0ePWi1/+OGHcfjwYezbtw9r167FyJEjhdv27dsHvV6PhQsX4rbbbkPLli2Rm+ueYGNkZCSSk5NFj71NmzZm6w0bNgyffPIJVq1ahXXr1uH6de4zmVqtRv/+/bF06VJs374du3btwr//GvugHj58GB06iFQCuBEFpYIUNToPEqaZUtpy61nObJXNCR9eLTIqnJF70PABNwGIbmJc7lSmVIAHpQDzTBRXAi7xbYwzE0Y25gIRvqJxLY9JEcbNkgeYZ0td3g+nZ/FzODYngkaJ6VxDdcB2iZ/N7Zscu60MK1WkMcPJ6jgBxDQHwmLF7+vs+ZLixHEmtQNkCqDiGhc44pmW7tm6CDV7jm0E0NQNjE3ML5u8d/DvI3EtbWdD0gx8BNTonBBC/M3u3bvx+uuvY+/evcjJycE333yDK1euoHXr1lbrRkREYPTo0Xj22Wexbds2HDlyBOPGjYNUKhUyn2pr+vTp2Lp1K1599VWcPHkSy5cvx7vvvov//Oc/ouv36tULLVu2xOjRo3Ho0CHs2LEDL7zwgsP9ZGVl4ciRI1bZUqmpqejevTvGjRsHnU6HAQMGCLfddNNN0Gq1eOedd3D27FmsWLECH374YZ2O19Szzz6LN954A6tWrcKJEycwc+ZMHDx4EFOmTAHAzfz31Vdf4fjx4zh58iTWrFmDxMREREdHY9myZfjf//6Hw4cP4+zZs/jyyy+hVqvN+k7t2LED9913n9vGK4bK94JUfWRKVWl1GPbxX7gtLQaz+lq/MbnqjY3H8eWuC3BuLgj/EROmwIpxXdA0Vvy5YIxhwop92HXmmsvbPgAN5Cbv8Xe9+g2uwBj4GYvdmC4Bvj9Zhedn/yIsT4ceX0mAGyd24o7ZGwHY/kMRIpPg+b6tMfRWQ4aF6Sxkhj8wX/51Aes2HsN6AEXXCnG7yb4A4HUcwQAJsPCPq/j8D/PbnBWmlGHp8A7o2szGh3sAc384itV7L1ot79GqId4d0cHmH8TThaUY8/keFFVoRW93RQcAKwy7GfCdFme/c/54P0UqbpMcxsbiFPxndu0eJ0fkMglm92+LgR0a2Vzn853nsGTLKej03KsxBmX43XBMU3YqsHWn82N7EUkYLvkXmtXjoMET3BhQA6UE+PFGCmY6OM5WiRFYOb6r0IvGUkVCR4QeXgutVAm5rVn8ZHLciG6LBlf34ofrjTHLZJ/xkUp8Nf42JESKlKsB0DXqDH7PvddV4dI68fG+jCQMlRyB5uvR0EBudpzfXWuEF2wcZ0NUYJvhsX3q9xD89rv4ev+HSMyUAFqpGnJbMweGKHEjqi0aXD+Ayre7QmcYudLwPrHwWDQ+tzGO7pDiY8M47l1bhby14uvNRTIGSU6geuUj0BouMfjjXH8lGS/Z2H4iqrBFAiDvIMpmJ4qP34EjcVno+vTyWt2X+AYV3+hcQ0EpQgjxB5GRkfj999+xZMkSlJSUoGnTpli4cKHNhuGLFi3CE088gfvvvx+RkZGYMWMGLl68aNWnyVUdO3bE6tWr8fLLL+PVV19FUlIS5s6dK9rkHOBmw1u/fj3GjRuHLl26IDU1FUuXLkXv3r3t7ic9PV3YF9+InTdy5EhMnDgRo0aNMit1a9++PRYtWoQ33ngDs2bNwp133on58+dj1KhRdTpm3uTJk1FcXIzp06ejsLAQbdq0wffff48WLVoA4IKBb775Jk6dOgWZTIbOnTtjw4YNkEqliI6Oxn//+19MmzYNOp0O6enp+OGHH4TSw8uXL+PPP//El19+6Zax2kJBqSBVruEypSLkER7bx9G8Ehy6WIRL1yvcEpRateciSqtrHK/oZ8qqa/DbySsY1U08KFVYWo3NRwtEb7OPQa7iLux1TAKZhEGhKUIZixTWCA0pBkKAK7owlJk0tduHxqhWytFAUorY6su4wOx/SFy//7IxKMWXIyUaGxSu238JF6vUgAqIRDkqqjXQmyRqhstLABlQUKNGma52z3FZdQ1+PpxvMyjFGMPXe3JQIfJh56d/8vD6g+mIUouXnW09VohLNyprNS5Lf6MJzioSUQEV/tXEg8H54/1G1g23yQ/jG+1tKNN77rWw/sBlu0GptfsuobjSGKArQxh2yG9BS+klbK9uiTIXjulHaScMlW+GQlIDhcX9ftJ2cnic+y7cwIn8UrRrHC16+x5FF3RkauyQdENfOzPq/SK7A8OxF+ssHtuyKzXYefoqBnVsLHq/8+o2kOsb4hqicFwTA9g49p+knTBYvlX0ODfYOc4yROIvRWs0kRTg9+oWNh/bDZIMPK34GtulmegnVuJn8Ku0Kx7CAaglGrPl1SwEP2vaoYyJb38nmuOSMg75LAanNNE2j/NHaScMUmyDUqKFEuZBXHvP52lE4ZCiGdpLzyJcUrvXmlTnuQacpH6oDMHlqhoKShFCiD9o3bo1Nm7caPP2ZcuWmf0eERGB7Oxs4ffy8nK88sormDBhgrDs/PnzVtthzDwtITU11WrZQw89hIceesjmWCy327JlS+zYscPufsS8/PLLePbZZzF+/Hiz2eiefPJJPPnkk6L3eeaZZ/DMM8+YLXvkkUeE/48ZM8ZmAM3SnDlzMGfOHOF3qVSK2bNnY/bs2aLrjx8/HuPHjxe9beDAgRg4cKDNfS1duhRjxoxB48bi18HuQkGpIFWq5WZV82SmFP9NZ5kbAklVWh2ul3Mfon58+nZEqALj1H3319NYs+8ScouqbK6TW8R9QEuIVGL1445nhRDoNAA/y2hYHFBxBcuH34Tqxt2FVWK2/ggcBR66PR29Ovc0uztb0w7I34d194eg/Gbz23j/XCrG018dQF6xyYfIEkNvoCjjm1deURWKwJ1rUgnDb5MyoDcp1Utc/RZQAPzngdvwVHPxfdnz7YFcLN5y0nwcFkoqa4SA1OZn7oQihPsjMuDdnSiu1CKvuNJmUCqvmHt+RnZtggl3NnN5fFZ0dyFCIsF2qYvnMeuBC7pZeCFEBccJxq7bd+EGpq0+ZPdxBIyPx+djO6NZnOE9RH8Hqpge3zvbT0rQE5crH4ZUYz7To14ehpmhDTHTzj0nZu/HkdwS5BZVoZ2Nv5Vna+LwWPVHkNTI0FvPIJWKZ8OtZfdidlV7PD+gA15pxZXvvbHxODb8my8cr5jcCgnGahYiNTYcvz16m4Pj/D+R4wzH86FxeN7OPaG/A1qmx492Htu84ipkfBwLmS4EfRmzmfW3QjIAb1bdgln3paFzqrGUTqdqgP8p7Tfd1+l6oaFEgt/snrc9cbFiFKSGbFzhEOTheCk0Di/Z3cE2XC67bG8Nu5qFOTlpAPFZaiFTimbfI4SQQHTgwAEcP34cXbp0QXFxMebOnQsAeOCBB7w8Muf169cPp06dwuXLl5GSItKLM4DEx8dj2rRpHt9PYHyyJy6rj55S5YZgVHWNHpoavRAEqI18w4dCtVyGtsmRda479hUtErjHP99OEIA/9kbRapslfqJMEiFkEYlAxRUkyisA023oSwAA0bEJiLbcdlo3IH8f4m4cQlzsaNFdSAxlfXnFVWD8B+ESQ+O+KC7TpkanR2FpFfQIgV4RAammFCmqavNxaIsAAPEJyebLndQ6icv4y7cTPMgr4R7jBqFytEgwZggmR6sNQakq3JwYKX5fw/PTKjHCtefAIzz3mtXquA+C9oIwpgHijikNEBXqahBKTO0e0yYxoTiSW+Lw9aNFCKADrpVr0DBCKbpeXkk1qqHALY2ihOe4eUPusbYXpMsrrkINQpAc68y54blzJyFSBS1CoK0Biiq0aBCmEF0vv7gKBYhBWotb0Cgl2jODqctrJD7abcMg/od6ShFCSOB76623cOLECSgUCnTq1Ak7duxAXFyct4flkqlTp3ps223btsWFCxdEb/voo4/Mmqh72vTp0+tlPxSUClJlGs/3lDKdPaesugYxIeIfkpzBf0hOilIFTEAKABKjuHpje0EA47G7OA2naZPz8HigAMZZ7nj872LNh51oPJwQxX3Ar67RGz8I80GpSC4odaWsGnoGhEglkIQ2ADSl1s3O69joPMmFxzHR4nFMilLhWF6J3YAWf1uijb5CgYJ/bEqralBWXYNwpfWfiIISY4A4Uu3dPyGJUdzzkVfi+HkHuOdRLCil1zPhuJKijM8xv31nzo0kL58bKrkMsWEKXCvXIK+4SjQoxQeIAfPjJMRX8EEpmn2PEEICU4cOHbBv3z5vD8OnbdiwAVqteB/bhISEeh5N/aCgVJASMqXknsu6MO3dU1ZVgxgb39w7g89USIoOrA9SyfyHarvBFMOxu/oh0rR3S7jhDazChWAQP9tWwRFAU87NlGZBGSJDXLgCV8s0yC2uRANZFVDNZV8hMhkAhNLEhEgVJOoYoCjHfBx6HVBVzP3fchZAJ/HnxZWyaptZeXmGcSRbPI7845pXZDsbJtfw/CRHuxgY9DPhyhBEqEJQWlWD/OJK3BRv3XOOfz6Tor0fIE7mg5F2yl9Ns5xyiyuR3ti6xOtqWTVq9AxSCRBvErTit2+vvNaX3puSolWGoFQl2iRbZ/0VlhoDxHHh4hljhHiTUs69d1OmFCGEkGBlOvNdsKh9PRXxa/Ux+165SS+p0uq6zVomZLlEBlZQwDQTw1ZjPWOGj4sfevlMKYnMGOyxzFDifxcLBkU14rKdmM44db0Is2wSPktKFS0EsfJNstxEx1FZBPBzKtqaLt6BmFAFFDIpGIOQCWKJL/GyfByTHAQGNTV6XC2rFr1vIHL0eOSX1DJI6gHOZDJZZkrZWyc+QoUQmfHPorB9JzKxfOLxiLSfMcgvT4hU2eytRYg3UaYUIYQQEnwoKBWk+EypCIXnZt+rtMiUqot8H/rg507xESpIJIBGpxf69FjKr3X5nmF7MrkxGFRhUr7HmOOyOSdK+Mw+CPNNziONM7flmQaD+P2YZkrxASplJDfWWpBKJUIpoaPAg+U5xJes2Qo8FJZWgTFAIZMiJrT22X7+wlFJqS8FiIUAWol4lptpWR7gxDHZCFheL9fYzNwQSjtdfX16QJKDIF2gvo+SwGFsdE5BKUIIISRYUFAqCDHG6iVTqsKip1Rd+FKJjDspQqRCGY2jD8wuHztfvieVG4NBphlKmjJAb8imslU2x5fw2QlKJUfzmTWVJkGpZKvxJ0erxTOlhMBY7bKkeHzQLtdhUMo8eMCX8+XaKN8zDVgEQ3YJ3xvJVkkcv9wXAhtJhnLK/OIq6PXWmYZXy6uh1RmX22pYzi9PtniNRanlQuaGrUAPf95YloV6A/8ekevgOJMCvAyV+C9qdE4IIYQEHwpKBaFqXTVqDAELj/aUMglE1T0o5TsfhN3NXl8pnY0GzE7hy/dkIcaAj1mGkiFrSqYE5KHi20gxBKUu/s1lVolINB2/xcx7gEWTcCE4ZpKxxf+/lv2keMYsEQcfyK0ypYzjFyuhrHX5pJ/iAxv5NrKPah0k9YD4CCUkEkCrY7gmkmloGUiyWZJoI/tLIpHYLWcsr65BiSEL1BfOD0eZUoH8PkoCg0oISum9PBJCCCGE1BcKSgUhPksKAEJtBSPcwLTReYmbyvd8oWTI3RLtBFOumTRgbuhqY2I+C0qmsJ+hFBoD2GpYndiOy7SquArcOC+6itkHYTvle2Y9pcTK9+qYKZVoJ3jAGLNTosWdUxUaHUpFgqf5tW0076f8qaeUXCYVXhdigRj+GEIMGW61CdYY+0pZvz75kk+uQXztSk/diX9/dFS+F+izSBL/paKeUoQQQkjQoaBUEOL7SYXJwyCVeO4UMCvfq0NQqkqrE7IgLMtrAoG9sjN+WUKkeQNmp/CZUqble6LBIDsZSnIVkNSe+7+NEr4k0x5ExbbL95Ki1eJlhI76WjnJ3kxsJVU1QpDUsnxPrZAhOlRu877CbHM+0DOoPiQ6mNGOX+4rAWK+FE2sZI2fUZGfic5WmZ+98uAkOzPw+VIpI2B8f8wtrhTN+su1UaZIiK8QekpRUIoQQgiA1NRULFmyxOX7Xbt2DfHx8Th//rzbxrJs2TJER0c7te6cOXOQkZHhtn3bc9ttt2HdunX1si9PoaBUEKqPflKAZfle7Wff48vXVHIpotTez0ZwN3slN7ZmjHMK31NKFmKSKXXDWIbnbC8n0xI+EcbMmkowvnzPEJSq0elRWFptXE8oIzQt37MzA6ALhEwpkYbl/GMbHSoXPvSY3TfSeAy27usrgQdPS46y/Vj4YoCY74ElmillOBfaNY4yTihQYV3mZy9Tyt7rM68ur08PSDA8FlVaPYorrd9zfakpO6mb+fPno3PnzoiIiEB8fDwGDhyIEydOOLzfmjVrcPPNN0OlUiE9PR0bNmyoh9E6j+8ppanRQycSQCaEEOJ7evbsialTp3p7GGbmzZuHBx54AKmpqd4eise9+OKLmDlzJvR6/y19p6BUECrXGGbek3tu5j3AvHyvLplSpg2qJbbKzPxYop0gQJ16wIhlSjEdUFXM/V/o5eQgKOVgBj7TD8LG8r3GAICrZRro9AwhUgnX0J3flwcypez1lBKCBzbKluwGHkqCq6cUf5wlVTUotyhnLCzhAoy+FCC2V7bJP59NYkKFCQUsn2PTGfrEgjXObN9XApYquQwxYdwMkZbjtQoQE7/222+/4amnnsJff/2FzZs3Q6vV4r777kN5ebnN+/z5558YMWIExo0bhwMHDmDgwIEYOHAgDh8+XI8jt08lN16WUrNzQgghtVFRUYH//e9/GDdunLeHUi/69OmD0tJS/Pzzz94eSq1RUCoICZlSCs9mSpmm34v16nGWrQbVgcKs/M2CrRnjnKIzZITI5FwZHt8/jA8IORsM4mfgKzgMaCqsblbJZYgNUyAMlZBUl3ALI5MAGMuFEiJVkEkl9ssI69zonHuMCkurodWZf1NgNgOg2H2jbZdQ5gmzqwVHdkmESo5wZQgA63MyV3gt+k6A2Gz2Rwt5JqWXtmZZ5Gfok0q4xumubD+3Lq9PD0myEeS+UlZtHiAmfm3jxo0YM2YM2rZti/bt22PZsmXIycnBvn37bN7n7bffRu/evfHss8+idevWePXVV9GxY0e8++679Thy+1QhxkxWCkoRQojvGzNmDH777Te8/fbbkEgkkEgkOH/+PHQ6HcaNG4e0tDSo1Wq0atUKb7/9ttV9Bw4ciLfeegtJSUmIjY3FU089Ba3WPNu7oqICjz76KCIiItCkSRN8/PHHdse0YcMGKJVK3HbbbQAAvV6Pxo0b44MPPjBb78CBA5BKpbhw4QIAYNGiRUhPT0dYWBhSUlIwceJElJWVWW2/NvR6PebOnYvGjRtDqVQiIyMDGzduFG7XaDSYNGkSkpKSoFKp0LRpU8yfPx8A1xt3zpw5aNKkCZRKJZKTkzF58mThvjKZDH379sXXX3/tlrF6AwWlghAflPLkzHsAzLIsSt2QKRWomSqmjaUt+8DUKVOKL9+TcgEGq5nvnA0GRTUGwhO57eUeEF0lMUqFRIlhe8ooQMll4eVbPnf8vmoqAa3hQ7ObMqViwxSQyyRgDEJGCM/ROWQsATP/IK/V6XGlrNrufQORrcwxX8sMAkx6YIlmuRkD2saG5eLH1DBCCblI3zZ7zcN9sQm+rUb1/O9CgJgElOJiLgM2Jsb2++iuXbvQq1cvs2VZWVnYtWuXR8fmCqlUAmUI9zqkvlKEkGDHGEOFtsIrP2K9KcW8/fbb6NatG8aPH4+8vDzk5eUhJSVFCAStWbMGR48excsvv4znn38eq1evNrv/tm3bcObMGWzbtg3Lly/HsmXLsGzZMrN1Fi5ciFtvvRUHDhzAxIkT8eSTT9otWd+xYwc6deok/C6VSjFixAisXLnSbL3s7GxkZmaiadOmwnpLly7FkSNHsHz5cvz666+YMWOGU4+DI2+//TYWLlyIt956C//88w+ysrIwYMAAnDp1CgCwdOlSfP/991i9ejVOnDiB7OxsofRw3bp1WLx4MT766COcOnUK3377LdLT082236VLF+zYscMtY/WGEG8PgNS/Mk399JSqdFP5ni9+EHYnvvxNU6PHjQqtUH4D1LGnFF++JzOUWYU2AEouGfs5ORsMkkiAlM7AsR+4Er7UTKtVkqJUqMo3bC/KdOY9i2CQMpILkulruP1HNXK+jNABqVSChEgVLt2oRH5xJRqZZEUJwQMb5Xu2SrQKS6vBGCCXSRBr8rwEusQoFU4Vllll2/higNhWAE2vZygoNgYUbWUkGo/JRhadYfvXyjWo0uqE2cHM7+s7j0eig4CiL42VuIder8fUqVORmZmJW265xeZ6+fn5SEhIMFuWkJCA/Px8m/eprq5GdbUxyF9SUlL3ATugVshQXaOnTClCSNCrrKlE15VdvbLv3Q/vdmqW9qioKCgUCoSGhiIxMVFYLpPJ8Morrwi/p6WlYdeuXVi9ejWGDh0qLG/QoAHeffddyGQy3HzzzejXrx+2bt2K8ePHC+v07dsXEydOBAA899xzWLx4MbZt24ZWrVqJjunChQtITk42WzZy5EgsXLgQOTk5aNKkCfR6Pb7++mu8+OKLwjqmfbFSU1Px2muv4YknnsD777/v8HFw5K233sJzzz2H4cOHAwDeeOMNbNu2DUuWLMF7772HnJwctGjRArfffjskEokQKAOAnJwcJCYmolevXpDL5WjSpAm6dOlitv3k5GRcvHgRer0eUqn/5R3534hJnfGz73k6U8ps9r06lO8F+uxnihCpUE5jWVpUp2PX80EpQzDFcuY7IRjkRIYSX8JnZwa+JMk17hfTmfeE0jfDB2GJxNjsXMjYumE+vjpItjFTmtkMgGL3i7YRsCgyBgWlQZRdYjvbxnczgyxn1rtWroFGp4dEwgV+hWOyeI1ZnaMWokPlQp+bghLx88pWWag32JotkH9v8aXnjrjHU089hcOHD3skbX/+/PmIiooSflJSUty+D0t8s/NKjf82bCWEEAK899576NSpExo2bIjw8HB8/PHHyMnJMVunbdu2kMmMX/glJSWhsLDQbJ127doJ/5dIJEhMTLRax1RlZSVUKvPrnYyMDLRu3VrIlvrtt99QWFiIIUOGCOts2bIF99xzDxo1aoSIiAg88sgjuHbtGioqrNuXuKKkpAS5ubnIzDT/Yj8zMxPHjh0DwJUyHjx4EK1atcLkyZOxadMmYb0hQ4agsrISzZo1w/jx47F+/XrU1Jh/tlar1dDr9WZfJPkTypQKQvU3+557glL5JYH/YSo5WoWrZdXIL67CLY2iAJg3YK5bo3O+fI+f+Y4PSrlQNte4M/fvxb+52fss+gklRqlQDcP2TINSYg2k1TFA+RXr3lZ17CnFjwOwzhJxVAbp8H6RvhN0qA+Osop8KUAcH6Eym1nPsqF5w3CuLM9WNpyjRvYSiQRJUWqcu1qOvOIqNI3l3jcrNDXCDHe+lH0kBOlKzINvgZ5xGqwmTZqEH3/8Eb///jsaN25sd93ExEQUFBSYLSsoKDD7ZtvSrFmzMG3aNOH3kpISjwem+GzEqhrKlCKEBDd1iBq7H97ttX3Xxddff43//Oc/WLhwIbp164aIiAgsWLAAu3ebH49cbj5xjkQisZpFzpl1TMXFxeHGjRtWy0eOHImVK1di5syZWLlyJXr37o3Y2FgAwPnz53H//ffjySefxLx58xATE4M//vgD48aNg0ajQWio46yxuujYsSPOnTuHn3/+GVu2bMHQoUPRq1cvrF27FikpKThx4gS2bNmCzZs3Y+LEiViwYAF+++034bG5fv06wsLCoFb7zjW6KyhTKgjxmVIRCs/Nvlej00Nj0my6Lj2lgqHshJ8VLs8kE+NqeTVq9LYbMDvE95QSyvcsMqVcCQYlZ3DBrfJCoCjH6uakKBUShUwp4wcj0Q/CoSbNzrWVXH8pwBg0qwNbGT6OziH+fmXVNSit0jp9v0BlayZDXwxsmGYamgYVLbO6+ECarZ5S9o4pUeg5Zrwv//8whQwRSt/5fsdx8M0/L1aIOcYYJk2ahPXr1+PXX39FWlqaw/t069YNW7duNVu2efNmdOvWzeZ9lEolIiMjzX48TSVkSlFQihAS3CQSCULloV75cWVCG4VCAZ3O/D17586d6N69OyZOnIgOHTrgpptuwpkzZ9z9EInq0KEDjh49arX84YcfxuHDh7Fv3z6sXbsWI0eOFG7bt28f9Ho9Fi5ciNtuuw0tW7ZEbm6uW8YTGRmJ5ORk7Ny502z5zp070aZNG7P1hg0bhk8++QSrVq3CunXrcP0693lNrVajf//+WLp0KbZv345du3bh33//Fe57+PBhdOjQwS3j9QbfuZIm9aY+MqUqLHpBlFVrbaxpX3WNDlfLuFnkAnn2M7HSIn7msPgIFUJEGjA7JGRKGYJSljPfuZIpJVcDie2A3P1cCV+DpmY3J0WpUS0RyZQSKxkyLSPkxyIN4fpN1ZHYzGMlVVohU89W4CFUEYIotRzFlVrkFVchQsU9ZsJsc9G+E4SpDzYDG3XpceZByVEqXCmtRm5RpZBpaJnVZTmhAH+xledEiSz//OeanFemJaG+MhMhYHyfzCuyPE77ZYrEvzz11FNYuXIlvvvuO0RERAh9oaKiooRvSUeNGoVGjRoJs/dMmTIFPXr0wMKFC9GvXz98/fXX2Lt3r8NZjOqbWk6NzgkhxJ+kpqZi9+7dOH/+PMLDwxETE4MWLVrgiy++wC+//IK0tDSsWLECe/bscepLlLrKysrCrFmzcOPGDTRoYPzSOzU1Fd27d8e4ceOg0+kwYMAA4babbroJWq0W77zzDvr374+dO3fiww8/dNuYnn32WcyePRvNmzdHRkYGPv/8cxw8eBDZ2dkAuJn/kpKS0KFDB0ilUqxZswaJiYmIjo7GsmXLoNPp0LVrV4SGhuLLL7+EWq026zu1Y8cO3HfffW4bb32jTKkgxDc692RPKdPSPQCo0uqh1bneH4JvVKwMkSI6VO5gbf/F9zoyz/SoY5aOjgvmiWZK6WqAKm62JqczlExL+Cwkmcy+xwxBKZ2eocAwC57ZB37TMkIhMNbAqiSwNsRmYuMf0yi1HKEK23F4sSwrIYvGRoP0QCXWY8uXA8RiM+tZvn5MJxS4Xq4xrudEebBYM/U6zYzpQfzxVmp1KKk0ZqgGa9ZfoPrggw9QXFyMnj17IikpSfhZtWqVsE5OTg7y8vKE37t3746VK1fi448/Rvv27bF27Vp8++23dpuje4NaYSjfo6AUIYT4hf/85z+QyWRo06YNGjZsiJycHDz++OMYNGgQhg0bhq5du+LatWtCs3JPS09PR8eOHa1m+gO4Er5Dhw7hwQcfNCt1a9++PRYtWoQ33ngDt9xyC7Kzs4Uvddxh8uTJmDZtGqZPn4709HRs3LgR33//PVq0aAEAiIiIwJtvvolbb70VnTt3xvnz57FhwwZIpVJER0fjk08+QWZmJtq1a4ctW7bghx9+EEoPL1++jD///BNjx45123jrG2VKBSEhU0rhwUwpDfdhSC2XCd92llXVoIGLM5iZluD4UjaCu4kHROrYS4sv3xN6SplkSlUVGddzNiiV0gX4+yPgknVQKjFKhWpD+V6ZMhERAK6WVUOnZ5BJJWhoWn4YatLo3NkZAJ1Ul+BBYpQKx/NLzUrWHM3MFqj4wEVxpRYVmhqEKkJQWOK7AWKxHliWrx++zO9qWTXyiqsQG660mqHPFvFgpyFrzMcCliq5DA1C5bhRoUVeSSWiQuW2A8TEbzkzVff27dutlg0ZMsSsqasvUoVQUIoQQvxJy5YtsWvXLqvln3/+OT7//HOzZaaBnmXLllndZ8mSJWa/nz9/3mqdgwcPOhzTyy+/jGeffRbjx483m43uySefxJNPPil6n2eeeQbPPPOM2bJHHnlE+P+YMWMwZswYh/sGgDlz5mDOnDnC71KpFLNnz8bs2bNF1x8/frzZjIOmBg4ciIEDB9rc19KlSzFmzBiHvSV9GWVKBaH6mH2vwtALIlIdIsxcVZtm577YWNkThJ5SYuVBtT12vnxPLFOKDwYpowCZk7FpPlMq/1+uF5QJlb4SURJuZopcPRd04mf7SohQQmY6c51pcKzSfU3OAWOZVWFpFWoMmXmiJYRi9xWZtYx/PpKDrHwvQhmCMEO2An8ems7e5msBYrHy11yRGRf555E/JssZ+mxJFikLFdu+r0gyKeEDgCulNgLEhPgglYJ6ShFCCKmbfv36YcKECbh8+bK3h+Jx8fHxePXVV709jDqhoFQQqpfyPcPFZKgiBOFKLihSm2bnvloi426mmR78N+B1PnY9H5QyZKcJwaAbJsEgF5qLRzcBwuK5DKzcg+a3lXCNAEuYGrmVXJDLZrmQWHDMTZlScWFKhEgl0DOg0JAZ4my2k2WWlVanF7YRbCVPEonEakbCfAez1HmTWA8ssQbmxobllWbr8DP0Odq+WKNzX3xvssy85INpVgFiQnyQmm90rnW95J8QQgjhTZ061WMzxrZt2xbh4eGiP3yfqPoyffp0JCQk1Os+3Y3K94JQ/WRKGcv3AK6UqzaZUvk+2ljZ3RKiuOyF6ho9iiq0aBCmqHsPGJ1F+Z4QDKpl2ZxEwpXwHf+Ra3be1GTGphLuW4g8FmvyQdhGppdoplTdZ94DAKlUgoRIFS4XVSKvuArJ0WqngwdCYKPEmF3CGCCXSRAXFnzZJcnRapy5Um71fPpaPynAemY9xpjx9WOSAWUrWONsFt3VMg2qa3RQhsjq3vPNg4xBNPPgmy+OlRBLxqAUZUoRQgjxTRs2bIBWKz6Rl78HiLyBglJBiO8pFa7wXFCKT7sPU8oQIuO+ma/NDHy+XCLjTsoQGeLCFbhapkFucSUahCmEmb5qXTqmtyjf43tHaUqBsgLu/66WzTXubAhKWfSVMgSl8lmM8EHf5gd+0+BYZZFhbO7JlAK4x4sLSlUCaGCcQc/BB3LjrGXm40+IVEEahNklQkkp/3gU+W6A2HJmPVtleUkWDdydLZFtECqHMkSK6ho9Coqr0SQ21Fja6YNBOr5RPf/+GSzvoyQw8CX/1RSUIoQQ4qNMZ74jdUfle0GmWlcNrSFYESb3XKPzckNQSq0IQbiSi33WpnwvmGY/My0R0usZCoRyqdr2lDLMMCY1BKVUUQAMwZXrZ7l/XQ0GCTPw7QFMG+0ayvfyWIzVB36rIIZarHzPPZlS3P7MZzLMdzLwYFmiFSylo7YkWWSO+fLjYTmzHv8cxoUroQgx/plLsugN5Wy2k0QiMbtvpUaHogqtU/f1BmOZIv8aMARmg+B9lPg/ypQihBBCggsFpYIM308KAEJDQj22n0pD+V6oXFanoJQvl8i4m2lfqWvlGmh1DBIJEF/bxsRCo3NDQqRUBqijuf9fO8P962qmVHIHrhywLB8ovmRcblK+5zAYZFZGeK1247DDskTL2dIl/n6l1TUordKa3C84s0v4rBrLnlK+OOkAP7MewD3vtgJoNoM1Try/CEHLkirhsQhVyBCp8r2E49oG3wjxBdTonBAS7JyZYZUQX+GO85WCUkGG7ycVGhIKmVTmsf0Ijc6VMoQbPrS52lOqukaHq2Vco+nkICg7Mf0gyX+YjI+w34DZLj3fU0puXMZnKV07bfjdxQwlRSiQcAv3f9MSvmJDUAoxQrmcEBiwLD/k98n0wI3z5uNyA9PHsbRKi1LDeeco8BCmDBECDPnFVcIsfMlB+kGeD2Dws+7xj4evBjZMZ9azVTqaHG0+oYArZW3JJrMz5vnwTISAeZkiY8zYDywI3keJ/6NMKUJIsJLLuWv2iooKL4+EEOfx5yt//taG733FSzxK6CflwSbngLF8L1Qhg8zwoa3MxUypwhIuIKUIkaJBaO1Pcn9hOoOYszPG2aWzmH0P4DKSrp+pffkewJXw5R3kSvhueYhbZijfy2cxyC+ugs6k/NAqGBSiBORhgLbcOA4PZUrxGTGRqhCEKR2/3SVFqVFSVcrdt8R3eyjVhySTzCBNjV4IEPti+R7AZUH9g2LkF1fa7BUVH2mcUOBGhdalGfRMm4fz2Yu+mDUGGDPCKjQ6lFTVUKNz4ldUhqBUFQWlCCFBRiaTITo6GoWFhQCA0NBQn/zyixCAy5CqqKhAYWEhoqOjIZPVPuGFglJBRph5z4NNzgGT8j1FCORCo3PXglKmJTjB8IacbNILyS29tPQW5XuAMQjF31abYFBKF2DPJ9wMfDyT8r0KjQ7nrpahRs8glQANw0XKD0NjgOJy4zjcmCll2lPK2UbWxvuqcKKg1OK+wflBnn/Miiq0uHCNe99QhEgRE6awdzevEQtGWgZhzCYUKKoUnaHPme3HR/p2kEetkCE6VI6iCi0u36i0HSAmxAephaCU3ssjIYSQ+peYmAgAQmCKEF8XHR0tnLe1RUGpIMP3lPJ0plSFSaYU/62nqz2lnJ2uPVCYZkoJM8bVduY9ANCJlO9ZBqFq02C88a3cv3mHAG0VwHRAVREAoFKVAFQC+y9wvydEqhAiVn6obgAUX7Q9rjrgy+0KSqpw6YZrjyNfApZbXIm8It/toVQfIlUhCFXIUKHR4UBOEQDfDhCblqzlFtl+70iKUuNqmQZHc0tEZ+izuX2Tnm8NI/iZ93z3vSkpSo2iCi0OXy5GjZ5BJpUgPsJ3x0sIT0Xle4SQICaRSJCUlIT4+Hhota7PXE5IfZLL5XXKkOJRUCrI8OV7npx5DzAPSoUq+Ebnrr2xuprl4u/MekoVuSGzQciUEukpxatNMKhBGhAaB1Rc5QJT/DYUEYgIiwUqS7A/5wYAO5kkVsEx9wWlYsOVCJFKUKNn+OdSEQDnH8fESO5cu3SjEoWlwZ1dIpFIkBilwtkr5cbn04dnbzN9/dhryp4YpcK/l4uFY7Kcoc8W06AxX77ny03wk6JUOJZnfC3GRyghk/pmQJEQU2pqdE4IIZDJZG75sE+IP6BG50FG6Cnl4fK9CkP5nloRgohaNjoPtj4ofLZGlVaPE/mlAOraU0rD/WuWKWWRGVWbYJBEwpXwAVwJn6F0D5HJQmDANLNGlOl+FeFAiPtKwmRSifBY8uPgg02O8OP991Ix9AwIkUoQK1Z+GCT4klL+cfTlRtl8wMze7Humyxyeozbud7WsGjnXK1y6rzckWhxnsLyPEv+npp5ShBBCSFChoFSQ4XtK1VemVJhChnBl7YJSfAmOL5fIuJNKLkOsoV/PyUIuKFWnY+fL98R6SvFqWzbHl/Bd+luYeQ9RjYQP6fz4bWa5me7XjVlSPKtxOFm+x6/H3y8hUhXU2SWJFo+jLwc2+IBZzvUKaGq4XjRiZXn8OWk8R507ppgwhZBRdfoKF9yvU3mthyVbPHfJPpzVRYgplZx7nVFQihBCCAkOFJQKMt7oKSUEpVzsKcWX4PhyiYy78R/6GTP/vVb48j3TTCnTHlJSOZelVBuN+UypvcLMe6aZUvz4ncqUUkfXbgx2WD6OrmbDuHq/QOVPjwc/sx4/VltledbH5Nz7i0Qisb6vkxl43sC/b7rlvYSQeqSmnlKEEEJIUKGgVJCp755SXPkeFxQprcPse8HC9FidbcBsk47vKWVSGmeWodSA20ltNOoISKRc6R4/C19kI6sAos0PwqbBMTc2OedZnjNO95RydvxBwvL4fbmnFD+zHs/Wc251TC48x6bHr5bLEKn23baMtX0NEOJt1OicEEIICS4UlAoyfPlehCLCo/upNPSUClPIjD2lXMiU0tTocbWsGkBwfZgyzdpoGK6EXGzmOmfpHZTv1SUYpAgDEtpy/z/zK/dvZCOrckPvle9ZBpecy2gJVxp7oAG+3UOpPliWfPn642H6vNt637A8JlfeX0yPPynad2ciBMSCUr793BHC4xudV2n1YHyqHyGEEEICFgWlggxfvld/mVLG8r1KrQ5and6p+xeUVIExQCGTIibMfU2wfZ1p1kadg3E6kfI9dwaD+BI+vkwwspFV1olT5XsezpSKUIUI56Cr9/XlzKD6UJesIm9w5vXDl/kZ13M+WOPW16eH+dtzRwiPz5QCgOoa564ZCCGEEOK/vB6Ueu+995CamgqVSoWuXbvi77//trmuVqvF3Llz0bx5c6hUKrRv3x4bN26sx9H6P2H2vXrrKRWCMJOAQLmTJXzGflK+nY3gbmYBkToHpQyz78lMe0q5MRjEz8DHi0w2+4AvlXDT0IvycKZUXYIHiU5k2wQL0+NXyKSICfXtALH560c82GQ6oYDlfVzavg/3kwK4994otfG1H+znMvEfKpNecJUaKuEjhBBCAp1Xg1KrVq3CtGnTMHv2bOzfvx/t27dHVlYWCgsLRdd/8cUX8dFHH+Gdd97B0aNH8cQTT+DBBx/EgQMH6nnk/qs+Zt9jjKHCpHxPESKF0nCRWepkCR8/816wfZAyLz+q44desUbnilAgxPCYmvZ1qo3Gnc1/j2oEtUKG6FBuf/ERKoTYKj/0cE8pszIrFx9H0xLEJB8vV/O0KLVcaDqcGKWC1MdnIjR9rpPtzIxnOmueK33bnN2+r+DfP+0GiAnxMSEyKRSGvx3UV4oQQggJfF4NSi1atAjjx4/H2LFj0aZNG3z44YcIDQ3FZ599Jrr+ihUr8Pzzz6Nv375o1qwZnnzySfTt2xcLFy6s55H7LyFTqrazrjmhukYPvaENBN8bQugr5WymVBA2OQfMj7fu5XsiPaUAY2ZSXYNBMc2A0Fju/4pwQBkJwFjyZjfTy8OZUnHhSsgMARTXM6X8p0TL00xnnPOH8i9nSy/5LCdbM/Q5tX0/ejzsBogJ8UEqOQWlCCGEkGDhtatUjUaDffv2oVevXsbBSKXo1asXdu3aJXqf6upqqFTmHwTUajX++OMPj47V15VqSnGu+JxT69ZHplSFSbp9qIILiPAz8IkFpXR6hr3nr+O3k1eEnwM5RQCcb1AdKBLd+aFXLzL7HmAMCNU1GCSRGLOlIpOFmfz4D8J2AzrKKG72PtPxuJFMKkGCITPE1ceRH3eIVIK4cMouSXTm+fQR5gFF2+8dTp2jDrfvD4+H2vCv74+VEFN8X6kqCkoRQgghAc9r81lfvXoVOp0OCQkJZssTEhJw/Phx0ftkZWVh0aJFuPPOO9G8eXNs3boV33zzDXQ62xct1dXVqK6uFn4vKSlxzwH4kCnbpmBfwT6sf2A9mkU1s7su3+g8Qu652ff40j1liFTIVuEbTZdWaa3WX7HrPOb8cFR0W/5QIuNOKrkMMWEKXC/X1H2mM7FG54AxCMRnOdVF41uBkxuByEbCIr7kzW7ZnFTKBcUqrnokKMWPI7e4ymq2NYf3M6yfEKkSzt9gxj8e/jB7G/9cSyRAQpTtgCJfvudqYCk2TAFFiBSaGr2fPB7c8QXb+yjxf8YZ+CgoRQghhAQ6rwWlauPtt9/G+PHjcfPNN0MikaB58+YYO3aszXI/AJg/fz5eeeWVehxl/Tt27Rj0TI/debvtBqU0Og00eq75dZjCc5lSlUKTc+MMOsaglHWm1OFcLlCYEKlEbJjxg2RsuAK92yZ6bJy+aso9LbD3wg1kpETXbUN6G+V7t00E5KFAy9512z4AZIwEcnYDXSYIi0Z0boLLNyox5NbG9u/bYwZwaQ+Q2L7u4xAx/o5miFZfRK82CY5XNtElLQZ90xNxR4uGHhmXvxl5WxNcLavG4E6NHK/sZSkxaozs2gSx4UooQ2Q21+vfLhl/n7uO0d1TXdq+RCLBM71a4nRhGVoleC6w7y4DMpKxL+cGHrkt1dtDIcQlfC+7Sg3NvkcIIYQEOq8FpeLi4iCTyVBQUGC2vKCgAImJ4oGIhg0b4ttvv0VVVRWuXbuG5ORkzJw5E82a2Q7EzJo1C9OmTRN+LykpQUpKinsOwgeUacqEPlGHrhzCiJtH2F7XsB4AhIV4LihVbjLzHi/cTk8pvn/UjKyb8VAnB4GMIDC6e6rLH5ZF2cqUatWH+3GHyGTg/9aaLUpvHIXlj3axcQcTXR/nfjyk9y2J6H2L60FNlVyG90d28sCI/FPHJg2cez59gEQiwbwH0x2ulxITimVja3dMT/ZsXqv7eUPT2LBaHych3sSX71FPKUIIISTwea2nlEKhQKdOnbB161ZhmV6vx9atW9GtWze791WpVGjUqBFqamqwbt06PPDAAzbXVSqViIyMNPsJJIUVxpkKDxUesrtuuYbrJ6UOUUMmtZ1FUFd8+Z5pplSEIVOqTCRTKq84OGfa8zgdlxUHmdz+eoQQQogPoUbnhBBCSPDwavnetGnTMHr0aNx6663o0qULlixZgvLycowdOxYAMGrUKDRq1Ajz588HAOzevRuXL19GRkYGLl++jDlz5kCv12PGjBnePAyvyi/PF/5/qewSrlVeQ6xavFeQMPOe3HMz7wE2yvdsZEoxxpDHz7RX1x5KxBxfvif1qypdQgghQU5Njc4JIYSQoOHVT6vDhg3DlStX8PLLLyM/Px8ZGRnYuHGj0Pw8JycHUqkxmauqqgovvvgizp49i/DwcPTt2xcrVqxAdHS0l47A+woqzMsf/7nyD+5qcpfounxQypMz7wHG8j21aaaUSrynVElVjTBbn70p3Ekt8OV7lClFCCHEj1Cjc0IIISR4eD2FYtKkSZg0aZLobdu3bzf7vUePHjh6VHyWtmCVX5Fv9vuhK4dsBqXKtVz5XoTCsw16Kw3le2GmPaWUXGDEMijF95OKDpWbBbGIG+j5oJTCu+MghBBCXCD0lNJQUIoQQggJdF7rKUXco6Ccy5RKCksCAPxz9R+b69ZXplSFSKaUsXxPa7ZurtBPikr33EqvB5hh1iLLRueEEEKID1NTo3NCCCEkaFBQys/xmVL3Nr0XAHD46mHU6K2biQPcTH2A53tKVYj0lBIanVeLZ0pRk3M305sE/2ReT4gkhBBCnKYSekrpvTwSQgghhHgaBaX8HJ8p1T25O8Ll4aisqcSpG6dE162/TCl+9j3T8j3x2ff4JueJFJRyL51JUIoypQghhPgRanROCCGEBA8KSvk5oXwvPAnpcekAuGbnYvieUuEKL2RK8Y3OLTKl8oq48r1kCkq5l05j/D81OieEEOJH+PJ/6ilFCCGEBD4KSvmxcm05SrWlAICE0AS0a9gOANfsXAxfvufxTKlq66CU0FPKstF5CZ8pRT2l3Mq0hFNK5XuEEEL8h4p6ShFCCCFBg4JSfqyggsuSipBHIEwehvYN2wOwHZQSZt+Te3b2vQotH5QyBkMibMy+l0c9pTyDL9+TygGJxLtjIYQQQlygknOXp1S+RwghhAQ+Ckr5sfxyrsl5QlgCAAiZUjmlObhRdcNqfaGnlMKzmVKVQk8p60ypSq0ONTpj41JqdO4hfKNzKt0jhBDiZ2j2PUIIISR4UFDKj/H9pPigVJQyCqmRqQDE+0rxQSlPz75XbijfU5sEpcKUMqvbS6q0wmx81OjczXSGjDRqck4IIcTPUKNzQgghJHhQUMqP8eV7iaGJwjJ7JXz11lNKpHxPGSKDIoQ73UqruSwePksqSi03W5e4gZApRY8rIYQQ/6JSUKYUIYQQEiwoKOXHhPK90ARhWft4LiglliklzL7n4UwpvnwvzCRTCgAi+Wbnhuwo6iflQfzse5QpRQghxM+oQmj2PUIIISRYUFDKjwmZUmHGTKl2cVxfqX+v/gud3vxiTugp5eFMKbHyPQAIV5rPwJdXVAmAglIewZfvUU8pQgghfoa/fqjS6h2sSQghhBB/R0EpP8YHpUwzpW6Kvglh8jBU1FTgdNFps/WF2fcUnp19r1KkfA8wNjvnZ+DjM6USo9QeHU9Q4sv3pFS+RwghxL9QTylCCCEkeFBQyo9Zzr4HADKpDLfE3QLAvK+UVqdFta4aQD30lBKZfQ8wZkqVGsr3+J5SyZQp5X46vqeUwrvjIIQQQlxEs+8RQgghwYOCUn6qQluBUk0pAPPyPcBYwmcalOJL9wDPBqV0eiak21sHpbhSMr58L7eYK9+jmfc8QGh0TuV7hBBC/ItKzl2eVmp1YIx5eTSEEEII8SSq7fFTfOleuDzcKsiUEZ8BANidtxtfHPkCAFCsKQYAqEPUCPFgSZfpt5qW5XsRQqNz89n3kqh8z/34nlJUvkcIIcTP8LPvMQZodHooQ2QO7kEIIYQQf0WfWP2U2Mx7vHZx7SCBBAUVBViwd4HZbdHKaI+Oiy/dk0iM33TyhKBUlXn5XlI0ZUq5HWVKEUII8VN8+R4AVGkoKEUIIYQEMgpK+Smxmfd40apovNL9FezO3222XAIJeqf29ui4Kgwz74XKZZBIJGa3mfaUKq3SCr2lEiMpKOV2Og33r5SCUoQQQvyLXCZFiFSCGj1DpVaHKNDfMkIIISRQUVDKT4k1OTf1YIsH8WCLB+tzSACACg0XlFIrrE8t09n3+CypSFUIwpR0GrodX74no8eWEEKI/1HLZSitrqFm54QQQkiAo0bnforPlBIr3/OmSi0XDAlTWqfaRyiN5Xt5/Mx70dRPyiP48j3KlCKEEOKHlIYSvioKShFCCCEBjYJSfqqg3DeDUuWG8j3TfhC8cKHReQ3yaOY9z9LxPaUU3h0HIYQQUgtqhXEGPkIIIYQELgpK+an8Cq58T6ynlDfx5XuhCrFMKS5rp7TamCmVREEpzxAanVP5HiGEEP/Df7lVpaGgFCGEEBLIKCjlp3w1U8pYvme7p1RZldY4814Ule95BN9Tisr3CCGE+CE+KEWZUoQQQkhgo6CUH6rQVqBEUwLAdqNzb7Fbvqc0lu/lGoJSVL7nIUKmFAWlCCGE+B9jTym9l0dCCCGEEE+ioJQf4puch8nDEKGI8PJozFXaK98zm32P6ylF5XseotNw/1KmFCGEED9EmVKEEEJIcKCglB/y1Zn3AJOeUmLle4ZlFRodLt/gg1JUvucRfPke9ZQihBDihygoRQghhAQHCkr5IV/tJwUAFRouGBJqZ/Y9ACg3BK+ofM9D9DT7HiGEEP+lVlCjc0IIISQYUFDKD+WX++bMe4D92feUITIoQoynXIQqRMieIm6mMwSlqHyPEEKIH1LJuesFypQihBBCAhsFpfyQUL7nY03OAfvlewAQYbI8mUr3PEdP5XuEEEL8l0podE5BKUIIISSQUVDKD/l2TylD+Z5IphRgXsJHpXseRJlShBBC/Bj1lCKEEEKCAwWl/BBfvuebQSnu4lEt0lMKgFm5Hs2850H87HsyCkoRQgjxP2rKlCKEEEKCAgWl/BCfKeWLPaUqDUGpMBvle+ZBKSrf8xi+0bmUyvcIIYT4H758r5IanRNCCCEBjYJSfqayphLF1cUAfLOnVLmhfE9to3wvQkWZUvVCx/eUokwpQggh/kfFz76n1Xt5JIQQQgjxJApK+ZmCci5LSh2iRoQ8wsujscZ/oxlqo3wvQmUMklBPKQ/iM6VkCu+OgxBCCKkF6ilFCCGEBAcKSvkZ09I9iUTi5dFYq3ChfC85moJSHqOj8j1CCCH+i4JShBBCSHCgoJSf8eWZ9wDH5Xvms+9RTymP0VP5HiGEEP+lknOXqNTonBBCCAlsFJTyM7488x5gUr5nKyhlyJSKUIaYZU0RNxMypSgoRQghxP/Q7HuEEEJIcKCglJ/he0o5mnnv+0O5WLP3Yn0MSaCp0aNGzwAAoQrxgBPf6DyJSvc8S6fh/qVMKUIIIX6Ib3RO5XuEEEJIYKNUFT+TU5oDAEgKS7K5TpVWh2mrDkLPGO5rm4godf0EJioMpXuA7UypxEguGJUWF1YvYwpafPke9ZQihBDih4SeUhqafY8QQggJZPSJ1Y/omR6Hrx4GALSNa2tzvbziKiFj6Xq5ph6DUty3mXKZBHKZeBLeXTfHY+GQ9riteWy9jClo6Wj2PUIIIf6LyvcIIYSQ4EBBKT9ytugsyrRlUIeocVP0TTbXyyuuFP5/o0KDNNRPVlKF0E/K9mkll0nxUKfG9TKeoKbng1JUvkcIIcT/qCgoRQghhAQF6inlRw5dOQQAuCXuFoTYKcvKK6oS/l9UofH4uHh8+Z6t0j1Sj3RUvkcIIcR/8ZlSNXoGrY5K+AghhJBARUEpP8IHpdo3bG93vfwSY1DqRrnWo2MyxWdKqSko5X2UKUUIIcSPqRTGS1Rqdk4IIYQELgpK+ZF/rvwDAGgX187uepble/Wl0hCUCrNTvkfqCT/7npSCUoQQQvyPQiaFVML9v0pDQSlCCCEkUFFQyk+UaEpwpvgMAKBdQwdBKbPyvfrLlCo3lO9RppQP4Mv3KFOKEEKIH5JIJEJfKcqUIoQQQgIXBaX8xL9X/gUApESkIFZtf+a6vGKT8r167SnFNzqnoJTX8eV71FOKEEKInzLOwEc9pQghhJBARUEpPyGU7jnIkgLMe0rVZ6YUle/5EB3fU0rh3XEQQgghtUSZUoQQQkjgo6CUn3C2yXmVVofr5cbsqKLK+suUovI9H0KNzgkhhPg5/nqiknpKEUIIIQGLglJ+QM/0TmdK5ZuU7gH1O/teJZXv+Q6+pxSV7xFCCPFTKjl3mVpFmVKEEEJIwKKglB84X3wepdpSqGQqtGzQ0u66eRZBqSKv9JSiQIjXUaYUIYQQP2fsKUVBKUIIISRQUVDKD/Cle23j2kIutR9kyCuuBACkxKgBADfqsadUhaF8jzKlvIwxQGcIRjo4XwghhBBfRT2lCCGEkMBHQSk/wAelnGlyzmdKtU6MBMBdyNXXN4w0+56P0Js835QpRQghxE+pKShFCCGEBDwKSvkBZ5ucA8aeUi0SwiGTSgDU3wx8VL7nI/Qmzzf1lCKEEOKnhEwpanROCCGEBCwKSvm4Uk0pzhSdAeBcUIov30uOViNazWXJ3KinvlJUvucjdCZBKZnCe+MghBBC6oDPlKqu0Xt5JIQQQgjxFApK+bh/r/4LBoZG4Y0Qp45zuD5fvpcUpUJ0aH0HpbhvMtUUlPIufY3x/1S+RwghxE/x1xOUKUUIIYQELgpK+bh/rvwDwLl+UoCxfC8xUo0GoVyWTHE9le/xF41hVL7nXUKmlASQUoCQEEKIf+LL9yooKEUIIYQELK8Hpd577z2kpqZCpVKha9eu+Pvvv+2uv2TJErRq1QpqtRopKSl45plnUFVVVU+jrX+u9JOq0upwrZzLikqOViHaEJSqrxn4yg3le5Qp5WV8TynKkiKEEOLHwhR8UKrGwZqEEEII8VdeDUqtWrUK06ZNw+zZs7F//360b98eWVlZKCwsFF1/5cqVmDlzJmbPno1jx47hf//7H1atWoXnn3++nkdeP/RML2RKOROUKijhgnMquRRRajka1HP5XiXNvucbdIbnW0pBKUIIIf4rXMVlXpdWU1CKEEIICVReDUotWrQI48ePx9ixY9GmTRt8+OGHCA0NxWeffSa6/p9//onMzEw8/PDDSE1NxX333YcRI0Y4zK7yVxdKLqBEUwKlTIlWDVo5XN/YT0oNiUSCBmFcplRRPfeUovI9L9MZLt5l9DwQQgjxXxEq7suV0ioKShFCCCGBymtBKY1Gg3379qFXr17GwUil6NWrF3bt2iV6n+7du2Pfvn1CEOrs2bPYsGED+vbta3M/1dXVKCkpMfvxF3zpXtvYtpA7UYrFz7yXFKUCAJNG554v39PrGTU69xVC+R7NvEcIIcR/hSu5L1fKquqnDQEhhBBC6p/XUimuXr0KnU6HhIQEs+UJCQk4fvy46H0efvhhXL16FbfffjsYY6ipqcETTzxht3xv/vz5eOWVV9w69vqSU5IDAGjRoIVT6/OZUol8UEpdf5lSBaXcvqUSIEJFGTpexTc6p/I9QgghfiySL9+jTClCCCEkYHm90bkrtm/fjtdffx3vv/8+9u/fj2+++QY//fQTXn31VZv3mTVrFoqLi4Wfixcv1uOI60Zj6A0UGhLq1Pr5QvkeF5RqUI+ZUgdyigAANydGCrPlEC/RU/keIYQQ/8f3lCqjnlKEEEJIwPLap9a4uDjIZDIUFBSYLS8oKEBiYqLofV566SU88sgjeOyxxwAA6enpKC8vx4QJE/DCCy9AKrWOsSmVSiiVSvcfQD2oYdxFWIjUuacpt8jYUwqAyex7ns+U2n/hBgCgY9Noj++LOECZUoQQQgIA9ZQihBBCAp/XMqUUCgU6deqErVu3Csv0ej22bt2Kbt26id6noqLCKvAkk3FZOYwxzw3WS7SG4ILcyeBCfol5T6kGYdz9iusjU+piEQCgQ0oDj++LOMDPvudEHzJCCCHEVwk9paproNcH3nUeIYQQQryYKQUA06ZNw+jRo3HrrbeiS5cuWLJkCcrLyzF27FgAwKhRo9CoUSPMnz8fANC/f38sWrQIHTp0QNeuXXH69Gm89NJL6N+/vxCcCiRaQ8NqZ5qcA8byvUShfM/QU6pSC8YYJBKJB0YJaGr0+PdyMQCgY1MKSnmdnjKlCCGE+D/THpVlmhpEqujvGiGEEBJovBqUGjZsGK5cuYKXX34Z+fn5yMjIwMaNG4Xm5zk5OWaZUS+++CIkEglefPFFXL58GQ0bNkT//v0xb948bx2CR9UYegOFSBw/TdU1Olwt4zJkkoXyPe7iTadnKKmqQZTaMxdzR3KLoanRo0GoHKmxzvW/Ih6ko55ShBBC/J8yRAq5TAKtjqGsioJShBBCSCDy+qfWSZMmYdKkSaK3bd++3ez3kJAQzJ49G7Nnz66HkXmfK5lSBcXVALgLOD4YpQyRIVQhQ4VGh6IKjceCUnyT8w5NGngsG4u4gM+Ukim8Ow5CCCGkDiQSCSJUclwv11BfKUIIISRA+dXse8FGCEo5UYaVV2zsJ2UaGGogNDv3XF+p/TmGJudNoj22D+ICanROCCEkQBj7Snm+PyYhhBBC6h8FpXyYUL7nxOx7+SXmM+/x+OwoT87Ax2dKdWxC/aR8gp7K9wghhAQGvq9UCWVKEUIIIQGJglI+zJVMqdwiPiilMlvOz8BX5KGgVEFJFS4XVUIqAdqlRHtkH8RFlClFCCEkQAiZUhSUIoQQQgISBaV8mCtBqXxD+V6iRVAqmi/fK/dM2vsBQ+ley4QI4cKReJnOEIB0ctZGQggh/uf3339H//79kZycDIlEgm+//dbu+tu3b4dEIrH6yc/Pr58B11KEobk59ZQihBBCAhMFpXyYK+V7ecWGTKlo8/K9Boam50WVngpKFQEAOjal0j2fwZfvOXHeEEII8U/l5eVo37493nvvPZfud+LECeTl5Qk/8fHxHhqhe/Dle9RTihBCCAlM9KnVh2l1rjQ6NwSlIi3K9wyZUp4q3+ObnHeg0j3foaPZ9wghJND16dMHffr0cfl+8fHxiI6Odv+APIQPSlGmFCGEEBKYKFPKh7k2+x4XlLJZvueB2fc0NXr8c6kYAGVK+RQ9H5Si8j1CCCHmMjIykJSUhHvvvRc7d+709nAc4lsDUFCKEEIICUyUKeXDnC3f09TocbWsGgCQbKt8zwOZUsfzS1Bdo0eUWo602DC3b5/UktDonF7ehBBCOElJSfjwww9x6623orq6Gp9++il69uyJ3bt3o2PHjqL3qa6uRnV1tfB7SUlJfQ1XQD2lCCGEkMBGn1p9mJAp5SDjpaCEy5JShEiFIBSvgZAp5f6g1P4LhtK9JtGQSiVu3z6pJb6nFGVKEUIIMWjVqhVatWol/N69e3ecOXMGixcvxooVK0TvM3/+fLzyyiv1NURR4dRTihBCCAloVL7nw5wt3xP6SUWpIJGYB4eiDUEqT8y+d+BiEQCgYxMq3fMp/Ox7TpR9EkIICV5dunTB6dOnbd4+a9YsFBcXCz8XL16sx9FxIqmnFCGEEBLQKFPKhzlbvpdXXAmAC0pZivZgo3OhyXmTaLdvm9SBjnpKEUIIcezgwYNISkqyebtSqYRSqazHEVnje0qVVVNQihBCCAlEFJTyYa5nSqmtbuPL+co1Omhq9FCEuCc57kppNS5er4REAmTQzHu+hS/fo55ShBASsMrKysyynM6dO4eDBw8iJiYGTZo0waxZs3D58mV88cUXAIAlS5YgLS0Nbdu2RVVVFT799FP8+uuv2LRpk7cOwSl8T6kyypQihBBCAhJ9avVhzgal8m3MvAcAkSo5pBJAz4CiSg3iI6zXqY0DhiyplvERwgUj8RFCppTCu+MghBDiMXv37sVdd90l/D5t2jQAwOjRo7Fs2TLk5eUhJydHuF2j0WD69Om4fPkyQkND0a5dO2zZssVsG76Iz5QqoaAUIYQQEpAoKOXDXC3fSxYJSkmlEkSp5bhRoUVRhdZtQan9OUUAqHTPJ+mpfI8QQgJdz549wRizefuyZcvMfp8xYwZmzJjh4VG5XwQ1OieEEEICGgWlfJir5XuJIuV7ADcD340KLW6UO+4r9eeZq3hm1UFUaHR216s03E5Nzn2Qjsr3CCGEBAY+KFWl1UOr00Muozl6CCGEkEBCn1p9lE6vg57pAbg2+54YYQa+CsffMq7ecxEFJdVOjVEtl+H2FnFOrUvqEWVKEUIICRB8+R7A9ZVqEEal6YQQQkggoaCUj6phxt4J9sr3NDV6XC3jgki2glINXJiBjy/Le2tIe3R0UJoXF6FEJPWT8j06w/PsIJhJCCGE+LoQmRRquQyVWh1KKShFCCGEBBwKSvkorc6Y1SS3k/FSUFIFxgCFTIoYGxdq0YaglKNMqatl1ci5XgEAuLdNAqLUFNTwS0Kjc3p5E0II8X8RqhAuKEV9pQghhJCAQ4X5PorvJwUAIRLbwYX8EuPMexKJRHQdvnzPUabUAUOW1E3x4RSQ8meGBvk0+x4hhJBAEG7oK1VKM/ARQgghAYeCUj6Kn3lPKpFCJpXZXM9RPykAaCAEpex/w3gg5wYAOCzbIz6Oz5Si8j1CCCEBIMLQKqCMglKEEEJIwKGglI9yeua9okoA9oNSxvI9+5lS+4WgFM2o59eo0TkhhJAAEmFodk7le4QQQkjgoaCUj3I6KFXMl++pba5jbHRu+2KuRqfHoYvFAIAOFJTybzrDN8l2GuQTQggh/iLCUL5HmVKEEEJI4KGglI/iy/fszbwHAPmGoFRytOPyPXuZUicKSlGp1SFCGYIW8eGuDpf4EsqUIoQQEkDCDZlSJRSUIoQQQgIOBaV8lPOZUlz5XmKkM+V7tjOl9huanGc0iYZUKt4wnfgJnSH4SD2lCCGEBAChp1Q1BaUIIYSQQENBKR+l1blWvpdkr3wvzDj7HmNMdB2+yXmHlGhXh0p8DV++J6PyPUIIIf7POPse9ZQihBBCAg0FpXxUDXNcvqfV6XGlrBoAkGS3fI/LlKrRM5vfMh4wZEp1aEr9pPyeUL6n8O44CCGEEDeIpJ5ShBBCSMCioJSPciZTqqCkCowBCpkUMaG2AxAquQwqOfdUizU7v16uwbmr5QAoUyogGM4dKt8jhBASCPieUqUUlCKEEEICDgWlfJTQU8pOs2q+yXlClNJhH6hote0Z+A5e5Er3mjUME/pPET+mp/I9QgghgYPvKVVKPaUIIYSQgENBKR8lzL4nsR1YcKafFC/azgx8+y8UAQA6NqHSvYBAmVKEEEICiLGnFAWlCCGEkEBDQSkf5UymFD/zXlKU7X5SvAbCDHwiQSlDk3MKSgUIfvY9O+cOIYQQ4i8i+J5S1dTo/P/Zu+/wJgu1j+PfJE33oLsFSkvZswzZIqgoKiLoUdwgoh7XOSpO9Iive288bgXXQUXFAQ5EkSFDZtkbOuigFLooHUneP54mtFJoC23T8ftc13OVPnmS3AGU9M49REREmholpRooV1LqBNUuzkqpqOokpVwb+Cq+obPZHaxLPgRA7zYtTiJSaXCc7XuqlBIRkSYgQDOlREREmiwlpRooV/veCbbvOWdKtaxW+17llVLbMvIoKLbh7+VBx8iAkw1XGhJn+55mSomISBPgnCmVf6QUh8Ph5mhERESkNikp1UBVp1JqX00qpXwrr5Rak3QIgISYICxVDEuXRsLuTEppaL2IiDR+zplSpXYHR0rsbo5GREREapOSUg1Uia3qpFR6LcyU0jypJsbhUPueiIg0KX6eFkxln5vlaa6UiIhIk6KkVANV6jhx+16JzU5mXhFQvUqpo+17Fd/MOZNSmifVRNjLzdtQ+56IiDQBJpMJf82VEhERaZL0U2sDVVWlVGZeEQ4HWC0mwvy8qny8Fj7G4+zPKyIj12j7yy8qZdf+AgB6x6hSqkmwlUs6qlJKRESaiEBvK3lHSslXUkpERKRJUVKqgXLNlLJUnlhwtu5FBnpjrsYsKOf2vc1puQx4an6F2+LD/Aj20/yhJsFWrj3zOH93REREGhtVSomIiDRNat9roFzb90yV5w3TyoacV2eeFEDX6CA6RwXgYTZVOLw8zFzZv03tBC3uV759T5VSIiLSRASUDTvP10wpERGRJkWVUg1UVZVSaYecSSmfaj2ej6eFn+48o3aCk4bL2b5nsoBZOWcREWkanBv4clUpJSIi0qTop9YGypWUOk61S00rpaSZKPt7o9Y9ERFpSgK8jX/XNFNKRESkaVFSqoFyte8dZ/teeq4xU6o6m/ekGXFWSql1T0REmhDNlBIREWmalJRqoKqqlNpXw/Y9aSacM6Us6swVEZGmI1AzpURERJokJaUaqKqSUulq35PKOLfvqVJKRESaEFVKiYiINE1KSjVQJ2rfK7XZycxTUkoqYdNMKRERaXqc2/fyipSUEhERaUqUlGqgSmzHr5TKzCvC7gAPs4kwf6/6Dk0aMmf73nFmkYmIiDRG/mWDzlUpJSIi0rQoKdVAnah9z7l5LzLQG7PZVK9xSQPnqpTydG8cIiIitchZKZV/RDOlREREmhIlpRqoE7XvaZ6UHJdd7XsiItL0BHg5B52rUkpERKQpUVKqgXJVSlWSXEjLKQQguoU278nf2NS+JyIiTU+A2vdERESaJCWlGqjqtO+pUkqOoUopERFpgvxd7XtKSomIiDQlSko1UNVp34sKVFJK/sZWbHytJJkpIiLSWLlmShWXYrc73ByNiIiI1BYlpRqoE1VK7Str32vZQkkp+RubKqVERKTp8S+bKeVwQEGxqqVERESaCiWlGqgTJaVclVJBmiklf1NWYaeklIiINCVeHmasFmPjsOZKiYiINB1KSjVQx2vfK7XZycwrAjRTSirhrJRS+56IiDQhJpPJNexcG/hERESaDiWlGqjjVUrtzy/CZnfgYTYR5u/ljtCkIdOgcxERaaKcLXx5R0rcHImIiIjUlgaRlHrjjTeIi4vD29ubAQMGsGLFiuNeO3z4cEwm0zHHqFGj6jHiuldiqzwp5dy8FxnojcVsqve4pIGzlX16XMmAfBERkcbMOexc7XsiIiJNh9uTUp9//jmTJ0/mkUceYfXq1SQkJDBy5EgyMzMrvf7rr78mLS3NdWzYsAGLxcJll11Wz5HXreO17x2dJ6XWPamEKqVERKSJOloppaSUiIhIU+H2pNRLL73EjTfeyMSJE+natStvvfUWvr6+fPDBB5VeHxISQlRUlOuYN28evr6+TS4pdbz2vX2HjM17micllbIVG181U0pERJoYzZQSERFpetza41NcXMyqVauYMmWK65zZbGbEiBEsXbq0Wo/x/vvvc8UVV+Dn51dXYdabrel57C8bYl5YaiQXEpPzSc/Kcl2zJvkQoKSUHIezfc+i9j0REWlajrbvaaaUiIhIU+HWn1yzsrKw2WxERkZWOB8ZGcmWLVuqvP+KFSvYsGED77///nGvKSoqoqioyPV9bm7uyQdch1btzeYfbx5NxPl3KsZkhru/XI+jJPWY66OCfOozPGksXO17nu6NQ0REpJY5k1L5at8TERFpMhp1OcX7779Pjx496N+//3Gvefrpp3n00UfrMaqTsyfrMAB+nhZiQnxJNdkAaB8WhMURUOHaED9PLugRVe8xSiNQNiBf7XsiItLUOGdK5SopJSIi0mS4NSkVFhaGxWIhIyOjwvmMjAyiok6cdCkoKGDmzJk89thjJ7xuypQpTJ482fV9bm4uMTExJx90HSm12wEYGB/KuxP6kvCR8f0X/xxKsHewO0OTxsRVKdWo880iIiLH0EwpERGRpsetg849PT3p27cv8+fPd52z2+3Mnz+fQYMGnfC+X375JUVFRVxzzTUnvM7Ly4vAwMAKR0NUYnMA4GExuTbvwbHb90ROyDlTSpVSIiLSxPhrppSIiEiTU+OkVFxcHI899hhJSUm1EsDkyZN59913mTFjBps3b+aWW26hoKCAiRMnAjB+/PgKg9Cd3n//fcaOHUtoaGitxOFupTajMsrDYnZt3oNjt++JnJBz+55Ff29ERKRpCXTOlFKllIiISJNR46TUnXfeyddff018fDznnHMOM2fOrDBIvKYuv/xyXnjhBaZOnUqvXr1Yu3YtP/30k2v4eVJSEmlpaRXus3XrVhYvXsykSZNO+nkbmlK7USllNZsosSkpJSfJrplSIiLSNDlnSuVpppSIiEiTcVJJqbVr17JixQq6dOnCv/71L6Kjo7n99ttZvXr1SQVx++23s3fvXoqKili+fDkDBgxw3bZgwQKmT59e4fpOnTrhcDg455xzTur5GqLicpVSpQ7jzZYJExazxZ1hSWPjbN/TTCkREWliXDOllJQSERFpMk56plSfPn147bXX2LdvH4888gjvvfce/fr1o1evXnzwwQc4HI7ajLPJKy2bKWW1HK2UUpWU1Jhr0Lmne+MQERGpZdq+JyIi0vScdDlFSUkJ33zzDR9++CHz5s1j4MCBTJo0iZSUFB588EF+/fVXPvvss9qMtUlzzpSylpspZdVcIKkpm9r3RESk8Smxl7Dj4A4AuoR2qfSaANdMKQ06FxERaSpqnJRavXo1H374If/73/8wm82MHz+el19+mc6dO7uuufjii+nXr1+tBtrUlZTNlPIwm13b97R5T2rMrvY9ERFpfP63+X88v/J5zow5k9fOeq3Sa5xJqSMldkpsdqwWty6RFhERkVpQ459c+/XrxznnnMObb77J2LFjsVqPrcho27YtV1xxRa0E2FwcrZQyHa2UUrWL1JQqpUREpBHqEd4DgPVZ63E4HJhMpmOucbbvgTFXKthPreoiIiKNXY2TUrt27SI2NvaE1/j5+fHhhx+edFDNUUnZTCkPJaXkVNiKja9q/RQRkUakc0hnLCYLWYVZZBzOIMov6phrPCxmfKwWCkts5CkpJSIi0iTUuO45MzOT5cuXH3N++fLlrFy5slaCao5K7WXb99S+J6fC2b6nvzsiItKI+Hj40CG4AwAbsjYc9zpnC1+e5kqJiIg0CTVOSt12220kJycfcz41NZXbbrutVoJqjkpKy23fU6WUnCybtu+JiEjj1D2sO2C08B2PvzMppQ18IiIiTUKNk1KbNm2iT58+x5zv3bs3mzZtqpWgmqMSZ6WUxUyJTUkpOUl2Z1JKf3dERKRx6RFmzJU6caWU8e9bvpJSIiIiTUKNk1JeXl5kZGQccz4tLQ0PD7UMnaxSm7NSykypQ+17cpI06FxERBopZ6XUxgMbsdltlV4T4KX2PRERkaakxkmpc889lylTppCTk+M6d+jQIR588EHOOeecWg2uOXHOlLJaTKqUkpPnnCllUUJTREQal3ZB7fDx8KGgpIA9uXsqvSbI13hvdCC/uB4jExERkbpS46TUCy+8QHJyMrGxsZx55pmceeaZtG3blvT0dF588cW6iLFZcG3fM5uPzpRSC5bUlCqlRESkkbKYLXQN7Qocf65U6xY+AKQeKqy3uERERKTu1Dgp1apVKxITE3nuuefo2rUrffv25dVXX2X9+vXExMTURYzNQqnNOVPq6KBzD5OqXaSGbGWfHCuhKSIijVBVc6Vah/gCkJytpJSIiEhTcFJZDz8/P2666abajqVZK7Uf3b5XWtaCpUopqTFn+57mkYmISCNU1Qa+mGCjUirl4OF6i0lERETqzkn/5Lpp0yaSkpIoLq7Y03/RRRedclDNUXFpWaWU2UyhXTOl5CQ52/csnu6NQ0RE5CQ4K6W2ZW+jyFaEl8Wrwu0xrkqpwzgcDkwmU73HKCIiIrWnxkmpXbt2cfHFF7N+/XpMJhMOh1Hh43xTYLNVvi1FTqx8pVSus31P1S5SU3ZnUkoJTRERaXyi/aIJ8Q4h+0g2W7K3kBCeUOH2VmUzpQqKbRw6XEKwnz6EERERacxqPFPqjjvuoG3btmRmZuLr68vGjRtZuHAhp512GgsWLKiDEJsH50wpq8V8tH1PlVJSUza174mINGTJycmkpKS4vl+xYgV33nkn77zzjhujajhMJtMJ50p5Wy1EBBjVU8lq4RMREWn0apyUWrp0KY899hhhYWGYzWbMZjOnn346Tz/9NP/+97/rIsZmwbV9z1Ju+56SUlJTqpQSEWnQrrrqKn7//XcA0tPTOeecc1ixYgUPPfQQjz32mJujaxiqnCulYeciIiJNRo2TUjabjYCAAADCwsLYt28fALGxsWzdurV2o2tGSu1llVLmctv3VO0iNeXcvqeEpohIg7Rhwwb69+8PwBdffEH37t35888/+fTTT5k+fbp7g2sgnJVSG7M2Vnp767Jh56qUEhERafxqnPXo3r0769ato23btgwYMIDnnnsOT09P3nnnHeLj4+sixmahtHyllE2VUnIS7HZwGMlNVUqJiDRMJSUleHkZ7We//vqra0FM586dSUtLc2doDYazUmpP7h5yinII8gqqcHtMsFEppQ18IiIijV+NK6X+85//YC+r6nnsscfYvXs3Q4cOZe7cubz22mu1HmBzUVw2U8rDYtJMKTk5ztY9UFJKRKSB6tatG2+99RaLFi1i3rx5nHfeeQDs27eP0NBQN0fXMAR5BdEmoA0AGw8cWy0VE1JWKaX2PRERkUavxpVSI0eOdP26ffv2bNmyhezsbIKDg7WW9xQ4K6WsZrPa9+Tk2MolpZTQFBFpkJ599lkuvvhinn/+eSZMmEBCgrFd7rvvvnO19YlRLZWUl8SGrA0Mbjm4wm3OSim174mIiDR+Ncp6lJSU4OPjw9q1a+nevbvrfEhISK0H1ty4Zkp5HJ0pZVW1i9SEKqVERBq84cOHk5WVRW5uLsHBwa7zN910E76+vm6MrGHpEdaDubvnVjrsvLWrfa8Qu92B2awPRUVERBqrGrXvWa1W2rRpg81mq6t4mi3X9j2zWe17cnJspUd/rSo7EZEGqbCwkKKiIldCau/evbzyyits3bqViIgIN0fnZiVHXL90beDbvx6Hw1HhsugW3phNUFxqJyu/qF5DFBERkdpV45lSDz30EA8++CDZ2dl1EU+zVVo2U8pq0fY9OUnOSimzB6iVVkSkQRozZgwfffQRAIcOHWLAgAG8+OKLjB07ljfffNPN0blR7j6Y1g9WzQCgc0hnPEweHDhygIzDGRUutVrMRAdpA5+IiEhTUOOk1LRp01i4cCEtW7akU6dO9OnTp8IhJ6fEXm77nl3b9+Qk2IqNr/p7IyLSYK1evZqhQ4cCMGvWLCIjI9m7dy8fffRR814Ys2o65CTB9/+GuffhbfKgQ3AHgEpb+DTsXEREpGmocSnO2LFj6yAMcVVKmbV9T06Ss31P86RERBqsw4cPExAQAMAvv/zCJZdcgtlsZuDAgezdu9fN0bnR8CnGhyq/PwEr3oasrfRo253N2ZtZv38958SeU+FyY65UNsnZqpQSERFpzGqclHrkkUfqIo5mzWZ3UFYoVaFSSu17UiPO9j0lpUREGqz27dsze/ZsLr74Yn7++WfuuusuADIzMwkMDHRzdG5kMsGweyGiM3z9T9i1gO4Fe/jCFxKzEo+5PKbcsHMRERFpvGrcvie1r6SsSgrAo9xMKVVKSY3YnDOl9PdGRKShmjp1Kvfccw9xcXH079+fQYMGAUbVVO/evd0cXQPQZTRM+gWC2pCQnQLApsx1lJYbgg7l2vc0U0pERKRRq3FSymw2Y7FYjntIzZXaj26V8bSYKbWpfU9OgiqlREQavEsvvZSkpCRWrlzJzz//7Dp/9tln8/LLL7sxsgYkqjvc9Dtx0f0IsNkpdJSy4+1BsOFrsBsf5LUuq5RSUkpERKRxq3F/2DfffFPh+5KSEtasWcOMGTN49NFHay2w5qS0fKWUWdv35CQ5Z0rp742ISIMWFRVFVFQUKSlGJVDr1q3p37+/m6NqYPzCMI//jm7fjGHZ4RQSizLpPGsiRL0MZ08lJnwIAGmHjlBqs+NhUfG/iIhIY1Tjn17HjBlzzLlLL72Ubt268fnnnzNp0qRaCaw5KbEdrZSyaNC5nCzn9j1VSomINFh2u50nnniCF198kfz8fAACAgK4++67eeihhzCblVxx8fCkZ/sLWJb4DomxpzFu2ypIT4RPLyUqvAsTrIP4suR00nOPuCqnREREpHGptXc+AwcOZP78+bX1cM1KaVkputViwmQqN1NKyQWpCbtmSomINHQPPfQQ06ZN45lnnmHNmjWsWbOGp556itdff52HH37Y3eE1OD3DegKw3gO4Yx0Muh2svpj2b+ZRywcs87oNy08PQNZ29wYqIiIiJ6VW+nwKCwt57bXXaNWqVW08XLNTUmpUSnmUfTqq9j05Kc72PYv+3oiINFQzZszgvffe46KLLnKd69mzJ61ateLWW2/lySefdGN0DU+P8B4A7MrZRa7VSuDIJ+GMe2Hd/0j79XWiS1MJ3DoDtn4EXcfAsPsgspuboxYREZHqqvFPr8HBwZhMJtf3DoeDvLw8fH19+eSTT2o1uOaipKxSysNi/L6qfU9OimvQuad74xARkePKzs6mc+fOx5zv3Lkz2dnZboioYQvxDqG1f2tS8lPYkLWBwS0Hg08LGHgLr6cOJnnVXP4vcjHtDi6GTbONo8toGHY/RPVwc/QiIiJSlRonpV5++eUKSSmz2Ux4eDgDBgwgODi4VoNrLkrLZkpZLRUrpZSUkhqxqX1PRKShS0hIYNq0abz22msVzk+bNo2ePXu6KaqGrUd4D1LyU1i/f72RlCrTOtSPz+w9eSPqfF66wgp/PAebvoXN3xtHp1Ew7F5o2duN0YuIiMiJ1Dgpdd1119VBGM1bie3oTClQ+56cJLva90REGrrnnnuOUaNG8euvvzJo0CAAli5dSnJyMnPnznVzdA1TQngCP+7+kcSsxArnncPNkw8ehsjBMG4GZG6Ghc/Dhq9h6xzjaH+O0dYXow2HIiIiDU2Nf3r98MMP8ff357LLLqtw/ssvv+Tw4cNMmDCh1oJrLkrtFWdKnbB9b/9W+Ou9o5vWRJwO7DS+qlJKRKTBGjZsGNu2beONN95gy5YtAFxyySXcdNNNPPHEEwwdOtTNETY8PcKMNrz1+9fjcDhcFfsxwT4AJGcXHr04ogtc+oHRvrfoRVj/JeyYZxxth8HwByB28DHPISIiIu5R46TU008/zdtvv33M+YiICG666SYlpU5C6XEqpSpNSv3xLGz4qt5ik0bIN9TdEYiIyAm0bNnymIHm69at4/333+edd95xU1QNV+eQzljNVg4WHSQlL4WYwBgAYkKMSqmMvCMUldrw8rAcvVN4J7jkHSM5tfhlWPc/2P2HccQPh+EPQpsBbng1IiIiUl6Nk1JJSUm0bdv2mPOxsbEkJSXVSlDNTUnZTCkPS8VKqUrb947kGl87XwjRCfUSnzQiFiv0uKzq60RERBoJT4snXUK6kJiVSGJWoispFerniY/VQmGJjX2HjtA2zO/YO4e2gzHTjPa9xS/D6o9h1wLjaD/CSE617luvr0dERESOqnFSKiIigsTEROLi4iqcX7duHaGhqtA4Gc6ZUh7mskop2wkqpZwb1rqOhZ5KPoiIiEjT1yO8h5GU2p/IqPhRAJhMJloH+7A9M5/k7MOVJ6WcWrSBC1+G0++ChS/A2k9hx6/GoZlTIiIibmOu6R2uvPJK/v3vf/P7779js9mw2Wz89ttv3HHHHVxxxRV1EWOTV2p3tu+ZcTgclDrKZkpZKklKOTesVXabiIiISBPUM8zYTLg+a32F884WvuSDh6v3QC3awEWvwe0rodc1YLIY86bePwc+GgN7ltRq3CIiInJiNa6Uevzxx9mzZw9nn302Hh7G3e12O+PHj+epp56q9QCbA2f7ntVicrXuwXHa95wDzpWUEhERaTQuueSSE95+6NCh+gmkkeoRbgw735y9mSJbEV4WL+DosPOUg4XHvW+lQtrC2DfgjLth0UvGzClnW1/sEDjncbX1iYiI1IMaJ6U8PT35/PPPeeKJJ1i7di0+Pj706NGD2NjYuoivWSgtN1PKOeQcjtO+56qU8qyP0ERERKQWBAUFVXn7+PHj6ymaxqe1f2uCvYI5WHSQLdlbSAg35mq2Di6rlMquZqXU34XEGzOnzrgXlrwCaz6BvUvgvbOh3w1w9sPgfeI/OxERETl5NU5KOXXo0IEOHTrUZizN1tH2PVMNklKqlBIREWksPvzwQ3eH0KiZTCZ6hvfkj5Q/WL9/vSspFRNiVEol17RS6u+CY42ZU0PvgfmPQuLn8Ne7sPl7OP8ZY5anyXSKr0JERET+rsYzpf7xj3/w7LPPHnP+ueee47LLNHj7ZLi275krVkpZTJZjL3a271WWsBIRERFponqEGS18ifsTXeeclVIpJ1sp9XdBreCSd2D8txDSDvLT4cvr4NPLYP+22nkOERERcalxUmrhwoVccMEFx5w///zzWbhwYa0E1dyU2o5WSjlnSlnNVkyVfSJnV/ueiIiIND89w41h54lZR5NSzkHnBwqKOVxcWun9Tkr8cLjlTxj2gPGea8c8+O9A+P4OyEuvvecRERFp5mqclMrPz8fT89iEiNVqJTc3t1aCam5KypJS5SulKm3dA7XviYiISLPUPaw7Jkyk5qdyoPAAAEE+VgK8jWkUydmn2ML3d1ZvOHOKkZzqdAE4bLBqOrzWG+Y/Dkf0vldERORU1Tgp1aNHDz7//PNjzs+cOZOuXbvWSlDNjat9r9xMqUo374G274mIiAgLFy5k9OjRtGzZEpPJxOzZs6u8z4IFC+jTpw9eXl60b9+e6dOn13mctSnAM4C2QW2Bii18naMCAFi260DdPHFYB7jyfzDxJ2jdH0oOw6IXjOTUlrl185wiIiLNRI2TUg8//DCPP/44EyZMYMaMGcyYMYPx48fzxBNP8PDDD9dFjE2ec9C5p8VMia26lVJq3xMREWmuCgoKSEhI4I033qjW9bt372bUqFGceeaZrF27ljvvvJMbbriBn3/+uY4jrV3OuVIbD2x0nTu3axQAP25Iq9snjx0Ek36Byz+B0PZwOAtmXmm09BUX1O1zi4iINFE1TkqNHj2a2bNns2PHDm699VbuvvtuUlNT+e2332jfvn1dxNjkla+UKnWUzZQ6XiWU2vdERESavfPPP58nnniCiy++uFrXv/XWW7Rt25YXX3yRLl26cPvtt3PppZfy8ssv13GktatrqFGVv+nAJte587obSakVu7PJyi+q2wBMJugy2mjpG/wvwGS09L01FFJW1e1zi4iINEE1TkoBjBo1iiVLllBQUMCuXbsYN24c99xzDwkJCbUdX7NQ6kpKHa2U8jBV0b6n7XsiIiJSTUuXLmXEiBEVzo0cOZKlS5ce9z5FRUXk5uZWONzNmZTaeGAjDofx/ikmxJcerYKwO+CXjRn1E4iHF5z7hLGlL7AVZO+E98+Bhc9DWQW8iIiIVO2kklJgzDKYMGECLVu25MUXX+Sss85i2bJltRlbs+Fs37Oaj86UqrRSyuHQ9j0RERGpsfT0dCIjIyuci4yMJDc3l8LCygeEP/300wQFBbmOmJiY+gj1hDqFdMJispB9JJuMw0cTUOf3qKcWvr+LHwa3LIFuFxuD0H97Av53BRQerN84REREGqkaJaXS09N55pln6NChA5dddhmBgYEUFRUxe/ZsnnnmGfr161dXcTZpxc7texYzpfay9r3KKqHs5VYdq31PRERE6tCUKVPIyclxHcnJye4OCR8PH+JbxAMVW/jO7x4NwNKdBzh0uLiegwqGSz+EMf8FD2/Y/jO8MxzSN9RvHCIiIo1QtZNSo0ePplOnTiQmJvLKK6+wb98+Xn/99bqMrdkore72PVu5N1lKSomIiEg1RUVFkZFRsbUtIyODwMBAfHx8Kr2Pl5cXgYGBFY6GoGvI0RY+p7ZhfnSOCqDU7mDepnpq4SvPZILeVxuD0Fu0gYN74L0RkPhF/cciIiLSiFQ7KfXjjz8yadIkHn30UUaNGoXFYqnLuJqVUlu57Xv2E2zfcw45B7XviYiISLUNGjSI+fPnVzg3b948Bg0a5KaITl5lw87haLXUjxvS6z0ml+gEuOkPaHc2lBbC1zfCj/dDaT1Xb4mIiDQS1U5KLV68mLy8PPr27cuAAQOYNm0aWVlZpxzAG2+8QVxcHN7e3gwYMIAVK1ac8PpDhw5x2223ER0djZeXFx07dmTu3LmnHIc7ldjLKqXMVbTvlU9KVVZJJSIiIs1Cfn4+a9euZe3atQDs3r2btWvXkpSUBBitd+PHj3ddf/PNN7Nr1y7uu+8+tmzZwn//+1+++OIL7rrrLneEf0q6hXUDjKSUc9g5HJ0rtXh7FrlHSiq9b73wDYGrv4Sh9xjfL38Lpo+CnFT3xSQiItJAVTspNXDgQN59913S0tL45z//ycyZM2nZsiV2u5158+aRl5dX4yf//PPPmTx5Mo888girV68mISGBkSNHkpmZWen1xcXFnHPOOezZs4dZs2axdetW3n33XVq1alXj525ISl0zparZvme2GmXiIiIi0iytXLmS3r1707t3bwAmT55M7969mTp1KgBpaWmuBBVA27ZtmTNnDvPmzSMhIYEXX3yR9957j5EjR7ol/lPRKbjyYecdIvxpF+5Hsc3Ob5srfy9Zb8wWOPthuOJ/4BUEKSvg7TNg1wL3xiUiItLA1Hj7np+fH9dffz2LFy9m/fr13H333TzzzDNERERw0UUX1eixXnrpJW688UYmTpxI165deeutt/D19eWDDz6o9PoPPviA7OxsZs+ezZAhQ4iLi2PYsGEkJCTU9GU0KM6ZUlaLiRLbCdr3tHlPREREgOHDh+NwOI45pk+fDsD06dNZsGDBMfdZs2YNRUVF7Ny5k+uuu67e464N3h7ermHn5edKmUymci189byF73g6XwD/XABRPeBwFnx8MSx8Hso2L4uIiDR3NU5KldepUyeee+45UlJS+N///lej+xYXF7Nq1SpGjBhxNBizmREjRrB06dJK7/Pdd98xaNAgbrvtNiIjI+nevTtPPfUUNpvtuM9TVFREbm5uhaOhqdC+56hG+56GnIuIiEgz1i30aAtfec4WvgVb91NQVHrM/dwiJB4mzYPe14DDDr89AbMmas6UiIgIp5iUcrJYLIwdO5bvvvuu2vfJysrCZrMRGRlZ4XxkZCTp6ZUPqNy1axezZs3CZrMxd+5cHn74YV588UWeeOKJ4z7P008/TVBQkOuIiYmpdoz1paTU+LSsfKXUCdv3lJQSERGRZux4w867RgfSJsSXolI7C7bud0dolbP6wJg34KLXjYr3TbNh5pVQfNjdkYmIiLhVrSSl6ovdbiciIoJ33nmHvn37cvnll/PQQw/x1ltvHfc+U6ZMIScnx3UkJyfXY8TVU2p3zpQqt32vssSTTe17IiIiIuWTUuWHnZtMJle1VINp4Suvz3i46nOw+sKOX+GTS+BIjrujEhERcRu3JaXCwsKwWCxkZGRUOJ+RkUFUVFSl94mOjqZjx45YLBbXuS5dupCenk5xceUl0F5eXgQGBlY4GpoS10ypam7fU6WUiIiINGPHG3YOuOZK/bYlk8Li4494cJt2Z8G1s40B6ElLYfqFUHDqG61FREQaI7clpTw9Penbty/z5893nbPb7cyfP59BgwZVep8hQ4awY8cO7OWGQ27bto3o6Gg8PRtv9ZCzUspak+17IiIiIs2Ut4c37Vq0AyoOOwdIaB1ETIgPh4tt/LSxAVZLAbQZANf9AL5hkJ4IH54POanujkpERKTeubV9b/Lkybz77rvMmDGDzZs3c8stt1BQUMDEiRMBGD9+PFOmTHFdf8stt5Cdnc0dd9zBtm3bmDNnDk899RS33Xabu15CrXBWSnmYy7XvafueiIiIyHEdb66UyWTiH31aA/DlypR6j6vaonvC9T9BYGvI2mYkpg7udXdUIiIi9cqtSanLL7+cF154galTp9KrVy/Wrl3LTz/95Bp+npSURFra0U+4YmJi+Pnnn/nrr7/o2bMn//73v7njjjt44IEH3PUSakWpzTlTyqT2PREREZFqcCal/l4pBbiSUn/uPEDKwQY8TDysA1z/IwS3hUN74cML4MBOd0clIiJSbyrpEatft99+O7fffnulty1YsOCYc4MGDWLZsmV1HFX9OjpTqprte0pKiYiISDPXLbQbAJsPbMbhcGAymVy3xYT4MrhdKH/uPMBXq1K5Y0QHd4VZtRZtYOJc+GhMWcXUBTDhewjv6O7IRERE6lyj2r7XVJU4K6XMZkpsJ2jf0/Y9EREREQA6Bnc87rBzgMtOM6qlZq1Oxm53HHN7gxLYEq6bAxFdIT8dpl8AGcdWgImIiDQ1Sko1AKVlb5Q8LCZKHWrfExEREanKiYadA5zXLZoALw+SswtZvju7vsOrOf8ImPADRPWEgv0wfRTsW+vuqEREROqUklINgHOmlKflaKWUtu+JiIiInJhrrlTWsUkpH08LFyZEA/DlquR6jeuk+YXChO+gVV8oPGi09O1b4+6oRERE6oySUg2Aa/ueRdv3RERERKrLOVdqU/amSm+/tG8MAD+uTye/qLTe4jolPsFw7Wxo3R+OHDISU6mr3R2ViIhInVBSqgEotTtnSpXbvldZi57a90RERERcnJVSzmHnf9enTQviw/0oLLExJ3FffYd38rwD4ZqvIGYAHMmBj8ZCyip3RyUiIlLrlJRqAEpd2/fM2r4nIiIiUk3lh52nF6Qfc7vJZOKysmqpL1em1Hd4p8aZmGozCIpy4OOxkLLS3VGJiIjUKiWlGoBi5/Y9i+nE7XvaviciIiLi4u3hTfsW7YHKh50DXNKnFWYTrNx7kF378+szvFPnFQBXz4LYIVCUW1YxpcSUiIg0HUpKNQCuSimz+Wj7nrbviYiIiFSpe1h3ABKzEiu9PTLQmzM6hgMwa1Ujq5YC8PKHq7+E2NOhOA8+vkQzpkREpMlQUqoBcM6UsnqYqte+p+17IiIiIgD0DO8JwIasDce9xtnC9/XqVOz2Y2dPNXiefnDV59Bm8NFWvn1r3R2ViIjIKVNSys0cDsfR7Xtmbd8TERERqQlnpdTGrI3Y7LZKrxnRNYIAbw/Sc4+wfHd2fYZXe7z84eovjg4//3gspK93d1QiIiKnREkpN7OV+7TOajGpfU9ERESkBtoFtcPHw4fDpYfZnbO70mu8PCxc0D0agO/WpdZneLXLOWOq1WlQeBA+GgMZm9wdlYiIyElTUsrNSsslpTy0fU9ERESkRixmC11DuwKwPuv4lUNjercEYE5iGkWllVdUNQrOrXwte8PhA/DRRbB/m7ujEhEROSlKSrlZSdnmPQAPs4kSm7bviYiIiNREzzBjrtSJklID2oYSGehF7pFS/ti6v75Cqxs+LeDabyCqJxTsNxJT2bvcHZWIiEiNKSnlZs55UgBWS7nte5VVQ6l9T0REROQYzrlSJxp2bjGbuCjBqJb6du2+eomrTvkEw7WzIbwL5KXBjDGQ0wi3C4qISLOmpJSblZZVSplMxpslbd8TERERqZkeYT0A2HZwG4Wlhce9bkyvVgD8ujmDvCMl9RJbnfILhfHfQkg7yEmCGaMhL93dUYmIiFSbklJuVlI2U8pqMZd9r+17IiIiIjUR5RdFmE8YNoeNLdlbjntdt5aBtAv3o6jUzs8bM+oxwjoUEAkTvoOgNkYL30djoCDL3VGJiIhUi5JSbuaslLKaTcb32r4nIiIiUiMmk8nVwrd+//HnSplMJle11LdrG/EWvr8Lam0kpgKiYf8W+HgsHM52d1QiIiJVUlLKzZwzpTz+Viml7XsiIiIi1eds4TvRXCmAMb2MuVJLdmSRmXekzuOqNyFtYfx34BcO6euNiiklpkREpIFTUsrNSu1llVIWo1LqhO172r4nIiIiUilnUioxK/GE18WG+tG7TQvsDpiTmFYfodWf8I4w4YeyxFSiElMiItLgKSnlZiWlZZVSZjMOh0PteyIiIiInoVtYNwBS81PJPnLiRMyYsi18s5vCFr6/i+isxJSIiDQaSkq5WUlZpZSHxUSpo9R1Xtv3RERERKov0DOQuMA4oOoWvlE9W2Ixm1iXfIg9WQX1EF09U2JKREQaCSWl3Ky0bKaUp8VMie3oamJt3xMRERGpGWcL3/qs4w87BwgP8GJI+zAAvm2K1VJQSWLqIiWmRESkwVFSys2c2/f+XillraxFT+17IiIiIsfVI7x6SSmAsWUDz79clYzN7qjTuNymQmJqPcy4CAoOuDsqERERFyWl3KzEfnSmVPlKKQ+Ttu+JiIiI1ET5DXwOx4kTTRf0iKaFr5WUg4X8viWzPsJzD1diKgIy1hsVU0pMiYhIA6GklJs5K6WsFpNr856H2QOTyXTsxdq+JyIiInJcHYM7YjVbySnKISUv5YTXelstXN4vBoAZS/fUQ3RuFNEZrnMmpjbAjNFQkOXuqERERJSUcreSsplSHhbziTfvgdr3RERERE7A0+JJl5AuACRmJVZ5/TUDYjGZYNH2LHbuz6/r8NwrvBNcNwf8IyFzo5GYyt/v7qhERKSZU1LKzUqcM6XMFSulKqXteyIiIiIn1D2sO1D1Bj6AmBBfzu4cCcDHS/fWaVwNQnjHssRUFGRugumjIC/d3VGJiEgzpqSUm5Xane17ZldS6riVUtq+JyIiInJCzqRUdYadA0wYHAvArFUp5BeVVnF1ExDWwUhMBbSErK3w4fmQc+JWRxERkbqipJSbOdv3rBaT2vdERERETlHP8J4AbD6wucISmeMZ0i6M+HA/8otK+WZ1M0nOhLWHiXOhRRvI3mUkprJ3uzsqERFphpSUcrPScjOlqt2+p6SUiIiISKXaBLShhVcLiu3FbMreVOX1ZrOJ8QONaqkZS/dWubWvyQhpCxN/hJB4OJQEH14AWdvdHZWIiDQzSkq52dH2PZPr07xKK6UcDm3fExEREamCyWSiT0QfAFZlrKrWff7RtzV+nhZ2ZOazdOeBugyvYQlqbSSmwjpB3j4jMbV/q7ujEhGRZkRJKTdzbd8zmyl1lLXvVVYJZbcBZZ/cqVJKRERE5Lj6RvYFqp+UCvC2ckmf1gDMWLqnrsJqmAKijFa+yB5QkAkzLoIDO90dlYiINBNKSrmZa/teuUopD1Ml7XvO1j3Q9j0RERGRE+gbZSSl1mSswWa3Ves+4wcZLXzzNmWQeqiwzmJrkPzCYMJ3ENEV8tPhozFGS5+IiEgdU1LKzUrLklJWc7nte5VWSpUb1Kn2PREREZHj6hTcCV8PX/JK8th+qHpzkjpEBjC4XSh2B3y2fG8dR9gA+YbA+G8htD3kJMOM0ZC7z91RiYhIE6eklJu52veq2r5XfnuM2vdEREREjsvD7EHviN5A9Vv4AK4pG3g+a1UKNnszGXhenn8EjP8OWsTCwT1GxVT+fndHJSIiTZiSUm52dNB5Fdv3nO17Zg8wmeorPBEREZFGqaZzpQDO7hJBsK+VjNwiFm5vpsmYoFYw4XsIbAVZ28oSU5nujkpERJooJaXcrLSsUspqMR1t3ztRpZRa90RERESqVD4p5XBUr+rJy8PC2N6tAPhyZXKdxdbgBccaiSn/SMjcCO+N0PBzERGpE0pKudnR9j1z9dr31LonIiIiUqXuYd3xNHuSfSSbPbl7qn2/y/rGAMbA8+yC4iqubsJC28HEHyE4Dg7thffPgZTqV52JiIhUh5JSbuZq3zObqtm+p6SUiIiISFU8LZ70DO8J1KyFr2vLQHq0CqLE5mD2mtS6Cq9xCG0Hk+ZBdC84fABmXAjbfnZ3VCIi0oQoKeVmJWXb9zwsZkpsJ2jfs6t9T0RERKQm+kT2AWB1xuoa3W/caa0B+GJlcrVb/5os/wi4bg60HwElh+F/V8Kq6e6OSkREmgglpdyswvY9h9r3RERERGrLyQw7B7gooRWeHma2pOexITW3LkJrXLz84cqZ0OtqcNjg+zvgx/srbocWERE5CUpKuVlpWaWUZ7lKqRO27ykpJSIiIlItvcJ7YTFZ2Fewj335+6p9vyBfKyO7RQFGtZRgvAcd8wYMn2J8v/wt+PhiKMhyb1wiItKoKSnlZiX2skops7bviYiIiNQmX6svXUO7AjWvlnK28H27NpUjJbZaj61RMplg+ANw+afg6Q97FsE7wyFtnbsjExGRRkpJKTcrLTdTyrV9r7JqKLXviYiIiNSYs4VvdWbN5koNbhdGqxY+5B4p5eeN6XURWuPV5UK4YT6ExENOMrw/Etb+D5r7/C0REakxJaXcrLRsppTVou17IiIiIrXtZOdKWcwm/tHXqJb6cmVKrcfV6EV0hht/h/bnQGkhzL4ZvrhW7XwiIlIjSkq52dH2PfOJ2/e0fU9ERESkxnpH9MaEid05uzlQeKBG972sLCm1ZGcWydmH6yK8xs2nBVz1OZz5HzB7wObv4Y0BxlcREZFqUFLKzUpKne17pqPte9q+JyIiIlIrgryCaB/cHqh5C19MiC+D24XicMDHy/bWRXiNn9kCw+6FG3+DiG5wOAs+vwa+vgkKD7o7OhERaeCUlHKzUruRlLJazNVr31NSSkRERKRG+kacXAsfwA1D2wLw6bK9HDpcXKtxNSnRCXDT73D6ZDCZIfFzePcs2L/N3ZGJiEgDpqSUm5W4ZkpV0b6n7XsiIiIiJ6Vv1Mknpc7sFEGX6EAKim1M/3NPLUfWxHh4wYhH4PpfIKgNZO+C90bAzt/cHZmIiDRQSkq5mbNSqur2PVVKiYiIiJyM0yJPA2BL9hayCms2iNtkMnHbme0A+HDJHvKLSms9viYnpp/RzhczAIpy4JNLYcW77o5KREQaICWl3My1fc9spsR2ova9skopbd8TERERqZEwnzC6hXYDYEnqkhrf//zu0bQN8yOnsITPlmu2VLX4h8OE7yHhSnDYYO49MOfuo+9pRUREUFLK7UpsRyultH1PREREpG6c3up0ABalLqrxfS1mE7cMM6ql3l20myMltlqNrcny8IKxb8KIRwET/PUefHwx5O93d2QiItJAKCnlZkdnSpVr36usRU/teyIiIiInbWjroQD8mfqn6z1XTYzt3YqWQd7szyviy1UptR1e02Uywel3whWfgqc/7FkE7wyDlJrP9xIRkaZHSSk3K3VWSpmr2r7nrJRSUkpERESkprqHdqeFVwvySvJYt39dje/v6WHmpjPiAXj7j52uanepps6jjDlToR0gNxU+PA9WTXd3VCIi4mYNIin1xhtvEBcXh7e3NwMGDGDFihXHvXb69OmYTKYKh7e3dz1GW7tK7Nq+JyIiIlLXLGYLQ1oNAWBx6uKTeowr+rchzN+TlIOFfL9uX22G1zyEdzISU50vNLoAvr8DvvsXlBxxd2QiIuImbk9Kff7550yePJlHHnmE1atXk5CQwMiRI8nMzDzufQIDA0lLS3Mde/c23oGTzkopq7bviYiIiNSpoa2MFr5FKTWfKwXgbbVw/eltAfjvgp3Yyz5clBrwDoTLP4GzpwImWP0RfDASDjbe9/MiInLy3J6Ueumll7jxxhuZOHEiXbt25a233sLX15cPPvjguPcxmUxERUW5jsjIyHqMuHY5t+95WKrZvqfteyIiIiInZXDLwZgwsfXgVjIKMk7qMa4dGEuAtwc7MvP5PlHVUifFZIKhd8M1X4FPCKSthbfPgG2/uDsyERGpZ25NShUXF7Nq1SpGjBjhOmc2mxkxYgRLly497v3y8/OJjY0lJiaGMWPGsHHjxvoIt06U2J0zpbR9T0RERKQuBXsH0yO8B3DyLXwB3lb+WTZb6vmft1JUqk18J6392fDPhdCqLxw5BJ9dBr89AXb9noqINBduTUplZWVhs9mOqXSKjIwkPT290vt06tSJDz74gG+//ZZPPvkEu93O4MGDSUmpfAtKUVERubm5FY6GpNR2dKaU2vdERERE6parhS/15Fr4AK4/vS0RAV6kHCzk02VJtRVa89QiBib+CP1uNL5f+Dx8fLHa+UREmgm3t+/V1KBBgxg/fjy9evVi2LBhfP3114SHh/P2229Xev3TTz9NUFCQ64iJianniI/P4XBQane271VRKaXteyIiIiKnbGhrIym1LG0ZJc73VzXk6+nBXed0BOD137aTe+TkHkfKeHjBqBfgkvfA6gu7/4D/DoTFrxx9DywiIk2SW5NSYWFhWCwWMjIq9vRnZGQQFRVVrcewWq307t2bHTt2VHr7lClTyMnJcR3JycmnHHdtKbEdHY5pNZtdb4xOOFNK7XsiIiIiJ61LSBdCvUMpKClgTeaak36cy/q2pl24HwcPl/D2HztrMcJmrOdlRjtf7OlQchh+fcSYNZW0zN2RiYhIHXFrUsrT05O+ffsyf/581zm73c78+fMZNGhQtR7DZrOxfv16oqOjK73dy8uLwMDACkdDUVo2TwrA6mGi1FHWvldZNZTa90REREROmdlk5vRWpwOn1sLnYTFz/3mdAXh/8W7Sc47USnzNXlgHuO4HGPsm+IZC5iZjO993/4bCg+6OTkREapnb2/cmT57Mu+++y4wZM9i8eTO33HILBQUFTJw4EYDx48czZcoU1/WPPfYYv/zyC7t27WL16tVcc8017N27lxtuuMFdL+Gkla+U8ihXKWU1naB9T9v3RERERE7J6a3LklIpJ5+UAjinayR9Y4M5UmLnlV+31UZoAsZ2vl5Xwe0rofe1xrnVM+CNAbBxNjgcJ7y7iIg0Hm5PSl1++eW88MILTJ06lV69erF27Vp++ukn1/DzpKQk0tLSXNcfPHiQG2+8kS5dunDBBReQm5vLn3/+SdeuXd31Ek5aqa1cpVT5mVKVVUNp+56IiIhIrRgUPQiLycLOnJ2k5qee9OOYTCYevMColvpiZTLbM/JqK0QB8A2BMdOMQehhHSE/A76cADOvgpyT/3MTEZGGw+1JKYDbb7+dvXv3UlRUxPLlyxkwYIDrtgULFjB9+nTX9y+//LLr2vT0dObMmUPv3r3dEPWpcw45t5hNZd8b7XsnnilVyW0iIiIiUm1BXkEkhCcAsDhl8Sk9Vt/YEM7tGondAc/+tLU2wpO/ix0MNy+GYfcbXQNb5xpVU39OgxK1TYqINGYNIinVXJWUVUp5mE3YHDYcGEmqyrfvOWdKqVJKRERE5FQ5t/D9kfLHKT/Wfed1xmI28evmDFbuyT7lx5NKeHjBmQ/CzYugdX8ozoNfHoLXesNf70FpsbsjFBGRk6CklBs5Z0pZLWZX6x4cLyml9j0RERGR2nJWzFkALEtbRl7xqbXdtY/wZ9xprQF45sctODTzqO5EdIHrf4bRr0Fga8jbB3Puhml9YfXHYCt1d4QiIlIDSkq5kXOmlIfF5GrdgyqSUpW19omIiIhIjcS3iKddUDtK7CUsSF5wyo93x9kd8baaWbn3IPM3Z57y48kJmM3QdwL8ezWc/zz4R8KhJPjudph2Gqz9TMkpEZFGQkkpNzpepVTlM6XUviciIiJSm86JOweAeXvnnfJjRQV5M3FIWwCe+3kLNruqpeqchxcMuAn+vRbOfQJ8Q+Hgbph9C7zRD9b+T8kpEZEGTkkpNyq1G5VSVrOJkrJKKA+TByaT6diLtX1PREREpFadG3suAEtSl5BfnH/Kj3fzsHYE+VjZlpHP16tTTvnxpJo8fWHwv+CORBjxqJGcyt4Fs282klMbvgK1VIqINEhKSrmRs1LKw2Km1GF8imO1VNK6B9q+JyIiIlLL2rdoT1xgHMX2YhamLDzlxwvysXLbme0AeHneNo6U2E75MaUGvPzh9DuPTU7Nuh7eGwFJy9wdoYiI/I2SUm5UfqaUq1LqeDOj1L4nIiIiUqtMJhPnxBotfL/s/aVWHnP8oDiig7zZl3OEj5furZXHlBoqn5w68yGw+kHqSvhgJHx+LRzY6e4IRUSkjJJSbuSaKWU+OlOq0iHnoO17IiIiInVgZNxIABanLuZwyeFTfjxvq4W7RnQE4I0FO8g9UlLFPaTOePnDsPvg32ug73VgMsPm7+CN/vDVjZC2zt0Riog0e0pKuVGJ/djte8evlNL2PREREZHa1jG4I20C2lBkK6qVFj6AS/q0on2EP4cOl/DWAlXluF1AJIx+FW75E9qfA/ZSWP8FvH0GTL8Qtv0MZe/LRUSkfikp5UallWzfO36llNr3RERERGqbyWTi3Dhj4HlttfB5WMzcN7ITAO8t3s3O/ac+RF1qQUQXuGYW3LQAul8KJgvsWQSfjYM3B8Gm7zQQXUSknikp5UbOmVJWi6nqpJS274mIiIjUCedcqdpq4QM4p2skZ3QMp7jUzgNfJWK3K9nRYLTsDZe+D3esM7b2eQXC/i3wxbXw3tmwu3Yq5kREpGpKSrlRSdmbEw+z+cTte3YbOMpKio+3nU9ERERETkqXkC608m9FYWkhi1MX18pjmkwmnrq4O76eFv7ac5BPVyTVyuNKLWoRA+c+AXdtgDPuKxuIvgpmjIaPL4GUlaqcEhGpY0pKuVGF7XsnqpRytu6BklIiIiIitax8C9+8vfNq7XFbB/tyb1kb37M/biEtp7DWHltqkXcQnPUQ3LEW+t1ozHDdOd+omvrvQFj0EuSkuDtKEZEmSUkpN6owU8p2oqRUua0tat8TERERqXXnxhpJqT9S/uBI6ZFae9zxg+Lo3aYF+UWl/OebDThUedNw+UfAqBfg9r8g4UqweBltffMfhZe7G0PR18+q+N5cREROiZJSbrJ031KWZn2LNfhP9pvmsyBlAXCc9r3y//Adb+aUiIiIiJy0bqHdaOnXksLSQpakLqm1x7WYTTz7j55YLSbmb8nk+8S0WntsqSMh8XDxW3DvdrjodYgdAjiMoehfTYJXe8Gf0+BIrrsjFRFp9JSUcoN9+fu4ad5N/Jr5Nt5R37GHT/l6+9cA+Fp9j72Ds33PZAGz/shEREREalv5Fr7ZO2fX6mN3jAzgtjPbA/Dodxs5WFBcxT2kQfAOgj7jYeJcuCMRhk8Bv3DITYFfHoKXu8HPD0HGRs2eEhE5ScpwuEF6QToAniYfSnJ7EmHuz8i4kVzQ9gJuTrj52Dto856IiIhInbukwyUA/JH8Byl5tTtD6Jbh7egQ4c+BgmKmfrdRbXyNTXAsDH8A7twAo1+DsE5QlAtLp8Gbg+GlLjD7NtjwFRzOdne0IiKNhpJSbpBfkg9AkDWaI6lX0dv737ww7AWePeNZEsITjr2Ds31PQ85FRERE6kzboLYMbjkYBw4+3/p5rT62l4eF5y7ticVs4vt1+5i1SoOzGyWrN/SdALcug6u+hI7ngYcP5KXB2k9g1vXwfDv48AJY+l84uNfdEYuINGhKSrlBQUkBAB4mo1XPajGd+A5KSomIiIjUi6s6XwXA19u/prC0drfl9W4TzORzOgIw9duN7MjMr9XHl3pkNkPHc+Gqz+H+PXDtbBh0O0R0BYcd9i6Bn6fAqz3hrdNhwbOQtk5tfiIif6OklBs4K6WsJh8APKqaE+WcKaX2PREREZE6dXqr02nt35rc4lzm7JpT649/87B2DGkfSmGJjds/W82RElutP4fUM6s3tDsTRj4Jty6FO9fDec9A7OlgMkP6eljwFLx9htHm992/YcscKC5wd+QiIm6npJQb5BcbSSkLZUmp6lZKafOeiIiISJ2ymC1c0fkKAD7b8lmtz36ymE28PK4XoX6ebEnP46m5m2v18aUBaNEGBt4CE+fAPdthzBvQaRRY/Yw2v9UzYOZV8Fw8zLwa1s+CIlXNiUjzpKSUGzgrpTzKklJWSxV/DHa174mIiIjUl7Htx+Lj4cP2g9tZlbGq1h8/ItCbF8cZc0Q/WrqXnzak1/pzSAPhFwa9r4ErP4P7dsE1X0H/fxqJq9IjsOUH+GqSMYdq5tWQ+IUGpYtIs6KklBs4Z0qZ8QbAw1xVpZTa90RERETqS5BXEKPiRwFGtVRdGN4pgn+eEQ/A/V8lknLwcJ08jzQgVm9oPwIueA7uSIR/LoKhd0NIu6MJqq9vNBJU758LC18wWv80h0pEmjAlpdzA2b5ndlSzUsqVlPKoy7BEREREpMyVna8E4Lek30gvqJtKprvP7URCTAtyCku44p1l7M7SjKFmw2SC6J5w9lT41yq4eTEMvQciuxuD0pOXw2+PG0PSX+9jJKhyUt0dtYhIrVNSyg3+XilV9fa9UuOrKqVERESkzBtvvEFcXBze3t4MGDCAFStWHPfa6dOnYzKZKhze3t71GG3j0zG4I/2i+mFz2Phi6xd18hyeHmbevLoPcaG+pBws5LK3/mRDak6dPJc0YCYTRPWAsx+GW5bAXRvhwpeh4/ng4QPZu4wE1Svd4ZN/wMZv1OInIk2GklJu4JwpZbY7B51r+56IiIhU3+eff87kyZN55JFHWL16NQkJCYwcOZLMzMzj3icwMJC0tDTXsXfv3nqMuHFyVkvN2jaLIltRnTxHyxY+zLplMN1aBpKVX8yV7yxj2a4DdfJc0kgEtYbTroerZsJ9O2HsmxA7xKig2vErfHkdPNcWnu8AM0bD3Hth5QeQlgh2bXMUkcZFSSk3cLbvmRw1nCllVvueiIiIwEsvvcSNN97IxIkT6dq1K2+99Ra+vr588MEHx72PyWQiKirKdURGRtZjxI3TmTFnEuUXxcGig3y749s6e54wfy9m3jSQAW1DyCsqZfwHK/hlo4afC+DpB72ugolz4V+rjRa/EGMWGQWZsHshrHgHfrgL3h4Kz7SBj8bA70/Bzt+gWC2hItKwKSnlBs5KKYfDC6jO9j2174mIiIihuLiYVatWMWLECNc5s9nMiBEjWLp06XHvl5+fT2xsLDExMYwZM4aNGzee8HmKiorIzc2tcDQ3HmYPrut2HQBvrH3D9cFiXQjwtjLj+v6c2zWS4lI7N3+yit+3HL/yTZqh0HZGi9+/18CUVLjxNxjzXxj8L4g/EzwDoDgfdi2AP56Fjy+GZ2Lhg/OMJNWexVBaNxV/IiInS0kpN3DOlMJWVilV5Uwpte+JiIiIISsrC5vNdkylU2RkJOnplVfXdOrUiQ8++IBvv/2WTz75BLvdzuDBg0lJSTnu8zz99NMEBQW5jpiYmFp9HY3FuE7jiAuMI/tINu+tf69On8vbauG/V/fhkt6tsDvgri/WknqosE6fUxopL39o1Rd6Xw3nPgHjZ8MDe+HmJTDqRegxDgJbg70EkpYaSarpo+DpGHh/JMybClvmQkGWu1+JiDRzSkq5gatSyl7NSilt3xMREZFTMGjQIMaPH0+vXr0YNmwYX3/9NeHh4bz99tvHvc+UKVPIyclxHcnJyfUYccNhNVuZ3HcyAB9v+pjU/LrdgOZhMfP0P3rQs3UQhw6X8K/PVlNis9fpc0oTYbZAVHfodwP84124a4NRVTX6Neh+KfhFgK0IkpfBkldh5pXwfDt4/TT4/g5I/BJy97n7VYhIM6MsRz2z2W0UlpZ94mX3Boq0fU9ERESqLSwsDIvFQkZGRoXzGRkZREVFVesxrFYrvXv3ZseOHce9xsvLCy8vr1OKtakYHjOcAVEDWJ6+nFdWvcLzw56v0+fz8rAw7co+jHp9EauTDvHCz1uZckGXOn1OaYJMJmP+VEg89J0ADoexyS9pmZGYSl4B+7fAge3GsWq6cb/gttD2DIgfBm2HgV+YW1+GiDRtqpSqZwWlR4cN2mxGksnDrO17IiIiUj2enp707duX+fPnu87Z7Xbmz5/PoEGDqvUYNpuN9evXEx0dXVdhNikmk4l7+t2DCRM/7fmJtZlr6/w524T68vylCQC8vXAX8zdnVHEPkSqYTMZcqt5Xw0Wvw23L4b7dcMX/YNDtEJ0AJjMc3A2rZ8Cs641KqjdPh58fgs3fq5JKRGqdKqXqmXNApqfZE7vN+O2vulJK2/dERETkqMmTJzNhwgROO+00+vfvzyuvvEJBQQETJ04EYPz48bRq1Yqnn34agMcee4yBAwfSvn17Dh06xPPPP8/evXu54YYb3PkyGpXOIZ25uMPFfL39a57/63k+vuBjzKa6/Xz3vO5RTBwSx4dL9nD3l+uY8++htGrhU6fPKc2Mbwh0vsA4AI7kwN6lxla/3X9AxgbIWG8cS6cZ1wREG/OsWvWBlr0hupfxOCIiJ0FZjnrmnCfl7+lPid0BVKNSStv3REREpJzLL7+c/fv3M3XqVNLT0+nVqxc//fSTa/h5UlIS5nLvLw4ePMiNN95Ieno6wcHB9O3blz///JOuXbu66yU0Sv/q/S9+3P0jiVmJ/LT7Jy6Iv6DOn3PK+V1Yvfcg61JyuP2z1cy4vj+B3tY6f15ppryDoNN5xgGQv99ITu1eCKmrIXMj5KXBlh+Mw6lFGyM51bI3xA6Gln3AQz+7iEjVTA6Hw+HuIOpTbm4uQUFB5OTkEBgYWO/PvyZzDeN/HE9MQAwkT2FTWi7TJ/ZjeKeI499p/mOw6EUYcAuc/0z9BSsiIiJuf+/QUOj3wfBO4ju8vuZ1ovyi+G7sd/h41H3lUnL2YS54bRF5R0ppGeTN85clMKS95vyIGxQXQFoipK4yjrS1xpyqv/PwgZh+EDsE2gyEqJ6qphJpZqr7vkGVUvXM2b7nb/XnkN3YpKLteyIiIiKNw/iu45m1bRZpBWm8te4t7up7V50/Z0yILx9d3587P1/L3gOHufq95Vw3OI77z+uMj6elzp9fxMXTD2IHGYdT4SFIT4R9ayFlhdH+dzirrAVw4dHrgmIgqoeRoIpOgNangf8JPpgXkWZBWY56VlBiDDr3s/qRZTOK1KpOSql9T0RERKQh8PbwZkr/Kfz7938zY+MMzos7jy6hdb8Zr3ebYOb+eyhPzd3Mp8uTmP7nHhZu28+L4xLo3Sa4zp9f5Lh8Whjb+tqeYXzvcMD+rbB3iXGkroKDeyAn2Ti2zj163xZtoNVpRoKqdT8jYWX1dserEBE3UVKqnlWcKWVUSnlUd9C5klIiIiIibndmmzM5N/Zcftn7C/+39P/49IJP8aiHhTR+Xh48eXEPzukayf1fJbIrq4B/vPkn/xzWjjtHdMDLQ1VT0gCYTBDR2Tj6TTLOHcmBjI1G6196ojGfav8WOJRkHBu/Nq4zW41qqtb9jERV7BAIauW+1yIidU5JqXpWvn2v1FkpVdWgc23fExEREWlQpgyYwtK0pWw6sIlPN3/KhG4T6u25h3eK4Oc7z2Dqtxv5bt0+3lywk/mbM3jhsgR6tm5Rb3GIVJt3kDEAPXbw0XNHcmHfakhZaRypK6Fgv3Fu32pY8bZxXVhHiB9uHHGnG48lIk2Gshz1zFkp5Wf1o6QsKVVlpZS274mIiIg0KGE+Ydxz2j088ucjTFszjbPanGUssqknLXw9ee3K3lzQI5r/zF7Ptox8Lv7vn9w8LJ5/n62qKWkEvAOPJpvAaPs7tPdokip5GaStg6xtxrHiHTCZIaRd2Wyq7ka7X1QPCIhy5ysRkVOgpFQ9c86U8rf6U2JzDjpX+56IiIhIY3Nx+4uZs2sOK9JX8PjSx3n7nLcxmap4X1fLzuseRf+2ITzy3Ua+X7ePN37fyc8bM/i/0d04vYM29EkjYjJBcJxx9LjUOFd4EPYshl0LYOfvkL0TDmw3DmfLH0BgK2jVB1r1NWZUtewFXgH1/xpEpMaUlKpn5WdKlZYlpTyq276n7XsiIiIiDYbJZGLqoKlc8u0lLE1byg+7fmB0u9H1HkeInyevX9mbUT2i+M/sDezIzOea95dzfvcoHhrVhdbBvvUek0it8AmGLqONAyAvA9LXQ8Z6SN9g/PrAdshNNY7N3xvXmSzGhr/YwUbLX5tBxkB2EWlwlOWoZ+W375XYy2ZKeWj7noiIiEhjFBsYyy29buHV1a/y7F/P0i+qH1F+7mklOq97NIPahfHyvG18vGwvP25I5/etmdwyrD3/HBaPt1UtfdLIBUQaR4cRR88V5RttfqmrjLlUKasgN+XobKql0wCTMZsqosvRI7wLhMTrg38RN9N/gfWs4qDzsvY9s9r3RERERBqrCd0m8MueX9icvZn7Ft7H+yPfx2q2uiWWIB8r/3dRN67oH8Mj325k+e5sXv51G7PXpvL0JT0YGB/qlrhE6oyXP8QNMQ6nnBTY+6fR+rd3CRzYAVlbjWPT7KPXWTwhtD2Ed4LwzsbX1v0gqHW9vwyR5kpJqXrmGnTu4YfdUQiAh0Xb90REREQaK6vZyovDXmTcD+NYk7mGaWumcVffu9waU+eoQGbeNJAfEtN4/IdN7M4q4Ip3lnFl/xgeOL8LQT7uSZqJ1Iug1tBznHGA0faXsR4yN0PmFsjcBPu3QkmB8evMTRXvHxxntP3FDTW+KkklUmeU5ahnzqSUl4cv4ExKafueiIiISGMWExjDY0MeY/KCyXyw4QP6RvbljNZnuDUmk8nE6ISWnNExnGd/2sJny5P434pk5m/O5LEx3Tive7Rb4xOpN862v/bl2v7sdshJNpJT+7cYXzPWG3OqDu4xjjWfGNeGtIN2Z0G7M41ElXegO16FSJOkpFQ9Kyg2Zkp5m/2AAwBYqz3oXEkpERERkYbqnNhzuKrzVXy25TMeXPwgX174JdH+7k/8BPlYeeriHlyU0JIpX69nd1YBN3+ymu6tArmqfywX9WqJv5d+LJBmxmyG4Fjj6Hju0fNHciFpGexdbLT/7VtrbP3L3gl/vWsMUW/Zy5hJFdYeQjtAWAcIbgse+nlNpKb0r089c1VKWY5uQamyUkrb90REREQahbtPu5t1+9ex8cBG7l14Lx+e96Hb5kv93cD4UH68Yyiv/7addxfuZkNqLg9+s54n52xiTO9WXD2gDd1aBrk7TBH38g40klTORNWRHNi9CHb9Djt/N5JTqauMozyrL8SfCZ3Og47ngX9E/ccu0ggpy1GPbHYbh0sPA+BpKpeUqnLQudr3RERERBoDT4snLwx7gXHfj2Pd/nW8vOpl7ut3n7vDcvG2Wrh3ZGcmnR7PV6tS+N+KJHZlFfDZ8iQ+W57E+EGxPHhBF23qE3HyDoIuFxoHwKEkSPkLsnbAge2QtQ0O7ITifNg6xzgwQau+0P5saDPQGJ7uFeDWlyHSUCkpVY+cCSkAL4sPAFaLCZNJ2/dEREREmorWAa15fMjj3LngTj7e9DHRftFc2/Vad4dVQYifJzeeEc8NQ9uybFc2ny7fyw+JaXy0dC8rdmcz7aretI/QD9Eix2jRxjjKczggbR1s+wm2/ghpayF1pXEAmMwQ2Q1iBkLsIIgdAgFR9R66SEOkpFQ9yi82WvesZitmjASTR1XzpABsJcbXBlL6LSIiIiIndnbs2dzR5w5eXf0qz/31HMHewVwYf6G7wzqGyWRiULtQBrUL5dK+mdz9xTq2pOcx+vUlPDqmG5f1bV31B6gizZ3JZMyZatkLhj8AuWmw/WfYswSSlxnVVellQ9T/ete4T0g7iB1sJKha9YHQ9mBWhaI0P0pK1SPnPCl/qz8lNjtQjXlSAPaypJRFSSkRERGRxmJS90kcKDzAJ5s/4eHFDxPkGcTQ1kPdHdZxDe8UwY93DGXyF+tYvCOL+2YlsnDbfqZe2JWIQG93hyfSeARGQ9/rjAOMJFXy8rIB6kuM5JRzePqaj41rPHwgsitEdoeoHtCyj1FdZdV/e9K0KSlVjwpKjM17flY/Su0OAKyW6lRKqX1PREREpLExmUzc2+9eDhYdZM6uOUxeMJl3z32XXhG93B3acUUEevPR9f15a+FOXvxlGz8kpvHblkz+eUY7bjyjLb6e+vFBpMYCo6HbWOMAKDxkJKn2LDYSVRkboOTwsQPUzR4Q0QVa9oZWp0H7ERDUyg0vQKTu6F+VeuSqlPL0p7i0rFKqqiHncLR9T5VSIiIiIo2K2WTm8SGPk1OUw+LUxdw2/zamnzedDsEd3B3acZnNJm4d3p4h7cL4v+83sibpEC//uo3PVuzlnnM78Y8+rTFX5z2siFTOpwV0HGkcAHYbZO+G9MSyNr9E2LcGDh842va3+iPj2qie0Ol8Y8NfdC+ozjgYkQZMSal65ExK1bxSSkkpERERkcbKarby4rAXuWneTazbv46JP09k2lnTGnTFFEBCTAu+vmUwPySm8exPW0g5WMi9sxJ5d9EuJg5py9herfDx1AwckVNmtkBYe+PofolxzuGAnBQjOZW21qiqSl5RlrhKhD+eBb9wYyZV3OnGfKrwLkpSSaPTIP7GvvHGG8TFxeHt7c2AAQNYsWJFte43c+ZMTCYTY8eOrdsAa0lBsdG+52/1p7QmM6XUviciIiLSqPlafXnj7DfoEdaDnKIcbvjlBn5P+t3dYVXJZDIxOqElv04expTzOxPg7cG2jHymfL2eAU/9ypNzNpF04HDVDyQiNWMyQYsY6HoRnD0VJv0C9+6AsW9Cl9Hg6Q8F+2HTbJh7D7w5GJ5vB5/8A358AP56D3YvNOZZORzufjUix+X2SqnPP/+cyZMn89ZbbzFgwABeeeUVRo4cydatW4mIiDju/fbs2cM999zD0KENd1jk35Vv3yuxVbNSym4Hh834tbbviYiIiDRaQV5BvHfue9zzxz0sSl3EnQvu5KEBDzGu0zh3h1Ylb6uFfw5rxxX92vDlqmQ+WrqXpOzDvLtoN+8t3s2ILpH884x4TosLcXeoIk2XXxj0uso4SouM+VN7lhjD05OXQ2E27PjVOMoLaAltz4D4YcbXoNbuiV+kEm5PSr300kvceOONTJw4EYC33nqLOXPm8MEHH/DAAw9Ueh+bzcbVV1/No48+yqJFizh06FA9Rnzyym/fK7VXc6aUc/MeqH1PREREpJHztfry2lmv8djSx/hmxzc8vuxxMg9ncluv2zCZGv6cpiBfKzcMjWfikLb8sS2T6X/uZeG2/czblMG8TRn0adOCm85ox7ldIzV3SqQueXgZLXuxg4F7jZEvaeuMoelZ2+HADsjaBgf3QN4+SJxpHAAh8RA3tOwYAoEt3flKpJlza1KquLiYVatWMWXKFNc5s9nMiBEjWLp06XHv99hjjxEREcGkSZNYtGjRCZ+jqKiIoqIi1/e5ubmnHvhJyi8uN1OqupVSztY9UPueiIiISBPgYfbg0cGPEukXyVvr3uLtxLfJKsziPwP/g4fZ7Z8ZV4vFbOKszpGc1TmSHZn5vLdoF1+vTmV10iFu/mQVbcP8GHdaDCO7RRIf7u/ucEWaPosVWp9mHOWVFBpVVLsXwq4/YN9qyN5lHKtnGNeExJfNpTrd+KoNf1KP3PqvXlZWFjabjcjIyArnIyMj2bJlS6X3Wbx4Me+//z5r166t1nM8/fTTPProo6caaq0oKDk6U6qkujOlbKqUEhEREWlqTCYTt/W6jXCfcJ5c/iRfbf+Kg0cO8tyw5/CyeLk7vBppH+HPM//oyeRzOjL9zz18smwvu7MKePanLTz70xY6RPgzslsUI7tF0b1VYKOoCBNpMqw+ED/cOM4GjuTA3qWwZ5HR9pe2rlySqmzDX3Dc0SRV634Q2s6YcSVSBxrHRzFl8vLyuPbaa3n33XcJCwur1n2mTJnC5MmTXd/n5uYSExNTVyGeUPnteyUlZZVSVW1HcCalTGZjK4OIiIiINBnjOo0jxDuE+xfez2/Jv3HzvJt57azXCPAMcHdoNRYR6M1953Xm1jPb8+3aVH7akM7SnQfYnpnP9swdTPt9BwPahvDgBV1IiGnh7nBFmifvIOh0nnGAkaRKWmYkqfYsMTb9HdxjHGs+Ma7xCYZWZVVYrftBzADwUgWk1A63JqXCwsKwWCxkZGRUOJ+RkUFUVNQx1+/cuZM9e/YwevRo1zm7czaThwdbt26lXbt2Fe7j5eWFl1fD+LTJVSnl6U9pUXUrpbR5T0RERKQpGxE7grfOeYt//fYvVmasZOJPE3nrnLcI86neh7ANjb+XB1cPiOXqAbHkFJbw+5ZMft6YzvwtmSzfnc2YN5ZwYc9o7h3ZidhQP3eHK9K8eQdBx5HGAXAk10hS7V1sVFSlrYPCg7BjnnEAmD2gZR+jmirudIjpD16NL5EuDYNbk1Kenp707duX+fPnM3bsWMBIMs2fP5/bb7/9mOs7d+7M+vXrK5z7z3/+Q15eHq+++qrbKqCqq/yg8+yazpTS5j0RERGRJqtfVD8+HPkhN/96M1sPbuXaudfy6lmv0jG4o7tDOyVBPlbG9m7F2N6tSD1UyIu/bOWbNan8kJjGzxvTuap/G64dFEf7CFVdiDQI3oHQ8VzjACgtNoanp6yElL8gaSnkJEPKCuNY/BJgMuZSRSdAdE/ja8s+4NPCna9EGgm3t+9NnjyZCRMmcNppp9G/f39eeeUVCgoKXNv4xo8fT6tWrXj66afx9vame/fuFe7fokULgGPON0TOQef+Vn8yy2ZKWauqlLKXGl81T0pERESkSesS2oWPz/+Ym+bdREp+Clf8cAV39LmDa7tei9lUxQeZjUCrFj68NK4XN5wezzM/bWHhtv3MWLqXGUv30qNVEBf3bsXohJaEBzSMLgcRATw8oVUf4xhwk3Hu4F7Ys/jokZME2TuNY+PXxjUmM0T1LKumGgptBipJJZVye1Lq8ssvZ//+/UydOpX09HR69erFTz/95Bp+npSUhLmquUuNhGumlKcfJXajUsqjyplSat8TERERaS7aBLbh0ws+5f/+/D8WpCzghZUvsDBlIU+e/iRRfseOt2iMurYM5KPr+7NkRxYfLtnNgq37WZ+aw/rUHJ6cu5kzOoQxfnAcwzqEYzZruLJIgxMcaxy9rza+L8gy2vzS1kF6IuxbCwd3G/Op0tbC0mllSaoeRoIqdgjEDjJmVUmzZ3I4HA53B1GfcnNzCQoKIicnh8DAwHp97oGfDaSgpIAfLv6B39bbefT7TVzYM5ppV/U5/p1SV8G7Z0FQDNy1of6CFREREcC97x0aEv0+1C+Hw8Gs7bN4/q/nKSwtJMAawIMDH2RU21FNbnvdgfwifkhM45s1qaxNPuQ6Hx/ux3WD4/hHn9b4ebn9s3QRqYncNGO7355FRjXVgR1/u8AEkd0hsiuEdYTwThDWCULaqkuoiaju+wb9372e2B1216BzP6sfpbZcoDozpdS+JyIiItLcmEwmLut4Gf2j+vPgogdJzEpkyqIpzNo2i3v73Uu30G7uDrHWhPp7MWFwHBMGx7Frfz6fLEviy5XJ7NpfwNRvN/L8T1v5R9/WjE5oSZ82LZpcUk6kSQqMhh6XGgeUS1KVtfwd2A4Z642jPA9vo9UvfjjEn2m0ADaRzimpnJJS9eRwyWHXr/2t/hTbDgHgUVVJstr3RERERJqt2MBYZpw/g/fWv8d7699jVcYqrvzhSka3G82/e/+bSL9Id4dYq+LD/Zk6uiuTz+3IV6tSmP7nHnZnFTD9zz1M/3MPrVr4cGHPaC7s2ZLurQKVoBJpLP6epMrLMAal798KWduMY/82KCmAXQuMg/8DnxCIGwKt+xtb/qITwOrjvtchtU5JqXrinCflYfbAy+JFadn2PQ9t3xMRERGRE/Awe3Bzws2MaTeGV9e8ypxdc/hu53fM2zuPSd0ncX2P67E2sfeK/l4eTBgcx7UDY1m4fT+z16Qyb1MGqYcKeXvhLt5euAt/Lw/ahPgSG+pLm1Bf4kL9OLNTBFFB3u4OX0SqEhAJXUYbh5PDYSSpdv8BO383KqoKs2Hz98YBxs/FUT2MBFXMAOMIauWe1yC1QkmpelJ+857JZKLUbmzf89T2PRERERGphmj/aJ4Z+gxXd76a51c+z5rMNUxbO42FKQt5eujTtAls4+4Qa53ZbGJ4pwiGd4rgSImN37dk8kNiGvO3ZJBfVMqmtFw2peW6rve0mLnstNbcPKwdMSG+boxcRGrMZIKIzsYx4J9gKzFmLCcthZSVkLwCCjJh32rjWP6Wcb+gmLIk1UCj9S+yG5gt7n0tUm1KStUT1+Y9qx8AJTWtlFL7noiIiIgAPcJ7MOO8GczZPYenlj9FYlYil35/KQ/0f4CL21/cZFvavK0Wzu8Rzfk9oikqtZGcfZi9B4wjKfswa5MPsTb5EJ8uT+Lzv5K5uHcrbj2zPW3D/NwduoicDIvVSDK1GWh873DAoSRI+ctIUCUvg/QNkJNsHBu+Mq7zDICYfkaSKjrBGKYeFGMkvaTBUVKqnjiHnPtb/QEotRmVUh5VVUq5klKqlBIRERERg8lk4sL4Czkt8jSmLJrCyoyVPPLnIyxKWcQjgx6hhXcLd4dYp7w8LLSPCKB9RECF88t3HeD133aweEcWX65K4avVKfRvG8J53aIY2T2K6CDNohFptEwmCI41DudsqqJ8o2oqaZlxpPwFRbmw8zfjcPIKhIguRhVV7BCIG2q0EIrbKSlVT/5eKVVqNyqlrFVtEtD2PRGpJpvNRklJibvDEGl0rFYrFovK/KVxivKL4r1z32P6xulMWzONX5N+ZWXGSm5OuJlxncY1uVlTVRkQH8qA+FBWJx1k2m87+G1LJst2ZbNsVzb/9/0mesW04LzuUYzp1VIJKpGmwMsf2p5hHAB2G2RuMhJUySsgY6MxRL0oF5KXG8fKD4xrwztD22EQdzq07KVqKjdRUqqeuCqlPI1KqeIaV0qpfU9EKudwOEhPT+fQoUPuDkWk0WrRogVRUVFNtu1JmjaL2cKkHpMY2HIgDy16iJ05O3lmxTPM3DKTu/rexZkxZza7v9t92gTzwXX9SM4+zM8b0/lpQzqrkg66Wvye/WkLQ9qFcUmfVozsFoWfl34sEmkSzBZjEHpUD+h/o3HOVgIHdhgJqn1rYPdCSF8P+7cYx4q3jet8giGqJ0T3hOhe0Po0aBGrRFUd0/9960n5QedwtH3PWu3te/qjEpHKORNSERER+Pr6NrsfPEROhcPh4PDhw2RmZgIQHR3t5ohETl630G7MumgWX2//mjfWvsGe3D3c8fsd9Ivqx91976ZbWDd3h1jvYkJ8uWFoPDcMjScz9wi/bMrg+3X7WL47m8U7sli8Iwtfzw2c3SWS/nHB9G4TTOeogKrnvopI42GxGq17EV2Otv0dzoY9i4wEVdJy2L8ZCg8am/92/3H0vn7h0Kqvg9m9wgAAYM9JREFUkaBqdRq07A0+LdzyMpoqZTrqibN972hSqqx9r9rb91QpJSLHstlsroRUaGiou8MRaZR8fIwWnszMTCIiItTKJ42ah9mDcZ3GcUHbC3h/w/t8tPEj/kr/iyvmXMGINiP4V+9/Ed8i3t1hukVEoDfXDIzlmoGxJGcf5ps1qXy9OoU9Bw7z/bp9fL9uHwA+Vgs9WwfRNzaYfm1D6BsbTKB382qDFGnyfEOg6xjjACgtgszNkLYO0hMhdbVRTVWwH7b9ZBxOYR2NBFWrPkbCKrIbeHi553U0AUpK1RPXTCnPsu17ZTOlPKqcKaX2PRE5PucMKV9frb0WORXO/4ZKSkqUlJImwd/Tnzv63MG4juOYtnYa3+/8nl+TfuW35N+4MP5Cbu11K638W7k7TLeJCfHl32d34F9ntWd10kEWbstidVl7X96RUpbvzmb57mxYsBOzCbpEB9IvLoQzOoYxpH0YXh76/4RIk+LhZcyVatnr6LmSI0aCKmWlMUA9dRUc2mvMqMraBus+M64zW43EVMveZUcvCO8CHvoZvjqUlKonx9u+V2WllCsppT8qETk+teyJnBr9NyRNVbR/NE+e/iQTu01k2tppzE+az3c7v2Pu7rmcF3cel3e6nITwhGb734DJZKJvbAh9Y0MAsNsd7Nyfz6q9B1m59yArdmeTlH2Yjfty2bgvl+l/7iHAy4OzukRwfvcohnWMwMdTCSqRJsnqDTH9jcMpf7+x7S9lJaSuhH1roTAb0tYax6oPjessnhDR1UhQRfeC6ATje6t3vb+Mhk6ZjnrinCnl3L5XUta+V2W/uk3teyIiIiJyatoHt+eVM19h/f71vLbmNZalLeOHXT/ww64f6BTcics7X86otqPwtTbvyluz2USHyAA6RAZwRf82AGTkHmHF7myW7TrAvE0ZZOYV8e3afXy7dh8+VgtndY7gwp7RnNk5Am+rElQiTZp/OHQcaRwADgccSjIGqO9bYySs0tbBkZyjiSons4ex8S+qp5GkatnLGMhe1k3VXCkpVQeOlB7By+JV4ROnYyql7GXb98zaviciUl3Dhw+nV69evPLKK6f0OAcOHKBLly6sWLGCuLi4Wolt+vTp3HnnndqCWIUHHniAgoICXn/9dXeHItIs9Qjvwbvnvsv6/ev5fOvn/LTnJ7Ye3MpjSx/jpZUvcV2367i267XNPjlVXmSgN6MTWjI6oSWPj+nOmuSD/Lg+nR83pJN6qJA569OYsz4NP08L53SNZFTPlgyMDyFAc6hEmj6TCYJjjaPbWOOcwwEH95QlpdYZ1VTpiXD4AGRsMA5n65/JbMyoiu5ltP616mMkqqw+bnk57qCkVC2yO+xMmH4aG0zF/JBVSKuyuVEA+VHhgNHfD1Ci7XsiIm7z5JNPMmbMmFpLSEn13XPPPcTHx3PXXXcRH988hy2LNAQ9wnvQI7wH9/a7l9k7ZvPlti/Zm7uXaWun8fnWz7ml1y1c3P5iPPQetAKz+Wi730OjurA+NYc5iWn8kJhG6qFCZq/dx+y1+zCZoFNkAL3bBNM31jjiQrUhV6RZMJkgpK1xdLvYOOdwQG4qpCUaCap9a42kVV4a7N9iHIkzy+5vMVr9WvU2klVRPYzvvfzd9ILqlv6VqUVmk5kSRymlZhOJjsO0OnzYdVt+kRU8PV2VUkfb97R9T0SkuLgYT8/6+f/c4cOHef/99/n555/r5fkai/r6MwgLC2PkyJG8+eabPP/883X+fCJyYkFeQUzoNoFru17LT7t/4rU1r5Gan8pjSx/j400fc0fvOzizzZmYTVV8kNoMmUwmerZuQc/WLXjg/M6sST7ED+vS+HVzBknZh9mSnseW9Dz+tyIJgJZB3gxpH8bpHcIY3C6M8ABt6xJpNkwmCGptHJ0vOHo+L91IUJVv/yvYDxnrjYOPnA9gJLkiu5e1//U0vgZEGY/diOlfl1qW0M74C7au37Vw6zLjCGxNftmWvWMHnWv7nog0P8OHD+f222/nzjvvdCUpADZs2MD555+Pv78/kZGRXHvttWRlZR33cUwmE7Nnz65wrkWLFkyfPv2495k7dy5eXl4MHDgQALvdTuvWrXnzzTcrXLdmzRrMZjN79+4F4KWXXqJHjx74+fkRExPDrbfeSn5+/km8eti5cydjxowhMjISf39/+vXrx6+//lrhmqKiIu6//35iYmLw8vKiffv2vP/++67bN27cyIUXXkhgYCABAQEMHTqUnTt3Asbv75133lnh8caOHct1113n+j4uLo7HH3+c8ePHExgYyE033QTA/fffT8eOHfH19SU+Pp6HH37YteXR6fvvv6dfv354e3sTFhbGxRcbnwI+9thjdO/e/ZjX26tXLx5++GHX96NHj2bmzJk1/40TkTpjNpm5IP4Cvhv7Hff3u58WXi3YnbObOxfcycXfXsw327+h2Pm+VI5hMpno0yaYqaO7svC+M1nx0Nm8dU1f/nlGPKfFBuNpMbMv5whfrkrhjplr6ffkrwx97jcumraYa95bzq2fruKBrxL5YPFusgv0+yzSbAREQafz4MwpcPUXcM92uHMDjPsIhtwJ7c4G/0jAAdm7YPN38PsT8Nk4eKkzvNABPr4EfvkPrPnU2BBYXODuV1UjqpSqZQkxZ/DZnjmsy0+GiC7GyRZtKDAZn5A4B52XlrX2afueiNQ2h8NBYYmt3p/Xx2qpUVvCjBkzuOWWW1iyZAkAhw4d4qyzzuKGG27g5ZdfprCwkPvvv59x48bx22+/1VqcixYtom/fvq7vzWYzV155JZ999hm33HKL6/ynn37KkCFDiI2NdV332muv0bZtW3bt2sWtt97Kfffdx3//+98ax5Cfn88FF1zAk08+iZeXFx999BGjR49m69attGljDNYdP348S5cu5bXXXiMhIYHdu3e7EnSpqamcccYZDB8+nN9++43AwECWLFlCaWlpjeJ44YUXmDp1Ko888ojrXEBAANOnT6dly5asX7+eG2+8kYCAAO677z4A5syZw8UXX8xDDz3ERx99RHFxMXPnzgXg+uuv59FHH+Wvv/6iX79+gJHcS0xM5Ouvv3Y9R//+/UlJSWHPnj1qoRRpYDwtnlzT9RrGtB/D9I3T+WzzZ+zK2cXUP6fy+prXuarLVYzrNI5Az0B3h9qgRQR4c173KM7rHgVAYbGNv/Zks2RHFot3ZLFxXy7J2YUkZxcec99nftzCed2juLJ/GwbGh6jlT6Q5MZmgRYxxdB1z9Hz+fmMWVfr6siMRsrYZVVU75xtHeS1ijw5Td86r8g2pz1dSbcp01LKe4T0B2JK9hSOlR/D28MYeEE1BQTJQfqZUWfueWdv3RKR2FZbY6Dq1/lvTNj02El/P6v+z0qFDB5577jnX90888QS9e/fmqaeecp374IMPiImJYdu2bXTs2LFW4ty7dy8tW7ascO7qq6/mxRdfJCkpiTZt2mC325k5cyb/+c9/XNeUrzyKi4vjiSee4Oabbz6ppFRCQgIJCQmu7x9//HG++eYbvvvuO26//Xa2bdvGF198wbx58xgxYgRAhflLb7zxBkFBQcycOROr1RikezK/P2eddRZ33313hXPlX3NcXBz33HMPM2fOdCWlnnzySa644goeffTRCq8HoHXr1owcOZIPP/zQlZT68MMPGTZsWIX4nb//e/fuVVJKpIEK8AzgX73/xcRuE/lq+1d8tOkjMg9n8urqV3kn8R1GxY/iik5X0Cmkk7tDbRR8PC2c0TGcMzoac2azC4rZuT+fvCMl5BaWknukhOyCYn7dnMGG1Fy+W7eP79btIz7Mj1E9o+kXF0Kf2GD8vfTjm0iz5B8O/mdCuzOPnis+DJmbjARV5hbj1/u3GImqQ3uNY/N3R68PioHIbsYR0dVoBQxt7/YCGP1frZa18m9FqHcoB44cYHP2ZnpH9KYwMBLHYeMTDlelVFn7XpUzpdS+JyJNVPlqJYB169bx+++/4+9/7BDHnTt31lpSqrCwEG9v7wrnevXqRZcuXfjss8944IEH+OOPP8jMzOSyyy5zXfPrr7/y9NNPs2XLFnJzcyktLeXIkSMcPnwYX9+abanKz8/n//7v/5gzZw5paWmUlpZSWFhIUpJRVbt27VosFgvDhg2r9P5r165l6NChroTUyTrttNOOOff555/z2muvsXPnTvLz8yktLSUw8GhFxNq1a7nxxhuP+5g33ngj119/PS+99BJms5nPPvuMl19+ucI1Pj7GRpnD5WYvikjD5O/pz4RuE7iq81X8uOdHPtzwITsO7WDWtlnM2jaLPhF9uKLzFYxoMwKrRdvmqivEz5MQv2OrFu4c0ZH1KTl8tiKJ79amsiurgNd/2wGA2QTdWgZxWlwwvWKMWVaxIb6Yq9rmLSJNk6cvtD7NOMorOACZG48OU9+3xmj9y0k2jm0/Hb3W4gk3/GpUVbmJklK1zGQykRCewG/Jv7Eucx29I3qT7xcGgAXwthg/CGn7nojUFR+rhU2PjXTL89aEn59fhe/z8/MZPXo0zz777DHXRkdHV/oYJpMJh8NR4dzf5x/9XVhYGAcPHjzm/NVXX+1KSn322Wecd955hIaGArBnzx4uvPBCbrnlFp588klCQkJYvHgxkyZNori4uMZJqXvuuYd58+bxwgsv0L59e3x8fLj00kspLjb+n+9M2hxPVbebzeZq/b78/c9g6dKlXH311Tz66KOMHDnSVY314osvVvu5R48ejZeXF9988w2enp6UlJRw6aWXVrgmOzsbgPDw8BM+log0HFaLlYvaXcTo+NGszFjJzC0zmZ80n9WZq1mduZpQ71Au6XAJ/+j4D1r5t3J3uI1aj9ZBPN26Bw+N6sKP69NYuvMAK/Zkk3KwkPWpOaxPzXFdG+DlQbdWgfRoFUTnqEA6RQXQPsIf7xr+mywiTYhfKLQ9wzicCg8Z7X8Zm4yEVcZG49clBRDc1m2hgpJSdaJneE9+S/6NxKxEAPJ9gwDwd5hcPeFH2/e0fU9EapfJZKpRG11D0adPH7766ivi4uLw8Khe/OHh4aSlpbm+3759e5XVN7179+aTTz455vxVV13Ff/7zH1atWsWsWbN46623XLetWrUKu93Oiy++iLms7fqLL76oVoyVWbJkCdddd51rQHh+fj579uxx3d6jRw/sdjt//PGHq32vvJ49ezJjxgxKSkoqrZb6+++LzWZjw4YNnHnmmcdcW96ff/5JbGwsDz30kOucc9B7+eeeP38+EydOrPQxPDw8mDBhAh9++CGenp5cccUVxySyNmzYgNVqpVu3bieMR0QaHpPJRL+ofvSL6kfm4Uy+2vYVX277kv2F+3l3/bu8t/49hrYeyriO4xjcajBWs6qnTpa/lweXnRbDZafFAJCWU8iK3dms2nuQxJQcNqXlkldUyrJd2Szble26n9kEcaF+dGkZyJB2YZzRMYzWwTX78EREmhifFhB3unE42e2QmwLe7p0R2Ph+amkEEsKN0rd1metwOBzkl/0h+9vtrmtK7dq+JyJS3m233ca7777LlVdeyX333UdISAg7duxg5syZvPfee1gsx37qe9ZZZzFt2jQGDRqEzWbj/vvvr7KlbeTIkUyZMoWDBw8SHBzsOh8XF8fgwYOZNGkSNpuNiy66yHVb+/btKSkp4fXXX2f06NEsWbKkQtKqpjp06MDXX3/N6NGjMZlMPPzww9jL/RsRFxfHhAkTuP76612Dzvfu3UtmZibjxo3j9ttv5/XXX+eKK65gypQpBAUFsWzZMvr370+nTp0466yzmDx5MnPmzKFdu3a89NJLHDp0qFpxJSUlMXPmTPr168ecOXP45ptvKlzzyCOPcPbZZ9OuXTuuuOIKSktLmTt3Lvfff7/rmhtuuIEuXYxlH85B9uUtWrSIoUOHVll1JSINW4RvBLf0uoUbet7AH8l/8MXWL1iatpSFKQtZmLIQXw9f+kb2ZUD0AAZED6BjcEfMJi3/PlnRQT6M6dWKMb2MSrQSm53tGflsSM1hw74ctqbnsTUjj0OHS9iVVcCurALmJBofULQL9+OMjuGc3j6MhJgWhPl7ufOliEhDYDZDizbujkJJqbrQLawbFpOFzMJM0gvSKfA02iP8SkvAVgIWK6U25/a96ial9CmTiDRtLVu2ZMmSJdx///2ce+65FBUVERsby3nnneeqTvq7F198kYkTJzJ06FBatmzJq6++yqpVq074PD169KBPnz588cUX/POf/6xw29VXX82tt97K+PHjKyRMEhISeOmll3j22WeZMmUKZ5xxBk8//TTjx48/qdf60ksvcf311zN48GDCwsK4//77yc3NrXDNm2++yYMPPsitt97KgQMHaNOmDQ8++CAAoaGh/Pbbb9x7770MGzYMi8VCr169GDJkCGBswVu3bh3jx4/Hw8ODu+66q8oqKYCLLrqIu+66i9tvv52ioiJGjRrFww8/zP/93/+5rhk+fDhffvkljz/+OM888wyBgYGcccYZFR6nQ4cODB48mOzsbAYMGHDM88ycObPCY4pI42Y1WxkRO4IRsSPYm7uXWdtm8d3O78g+ks2i1EUsSl0EQAuvFlwYfyGXd7qcuKA49wbdBFgtZrq2DKRry0DGYVRTORwO9ucVsTUjjzVJh1i4bT9rkg+xc38BO/cX8OGSPQC0DPKmZ+sW9GgdRMsW3pTaHNgdDkrtDux2B23D/OnfNgRPDyURRaRumRx/HzrRxOXm5hIUFEROTk6Fwa21bdz349icvZnnhz2P2WHi7oX30OfIEWZc8TsEtabn//1M7pFS5t89jHbhxw71dfngfEj6E8Z9VHElpIgIcOTIEXbv3k3btm2PGd4txzdnzhzuvfdeNmzYcNyEl5w8h8NBhw4duPXWW5k8eXKF23788UfuvvtuEhMTq92mWR9O9N9Sfb13aOj0+yA1YXfY2XZwG8vTlrM8bTmrMlZxuPRoe/WQVkO4qvNVDGk5BItZ84/qUk5hCX/uyOKPbftZufcgO/fnU52fAP08LZzeIYyzOkdwZqcIIgL1PkNEqq+67xsazrvBJiYhPIHN2ZtZl7mOjsHGxig/uwNyUiGoNaX2skqpqn4YUvueiEitGzVqFNu3byc1NZWYmBh3h9Ok7N+/n5kzZ5Kenl7p3KmCggI+/PDDBpWQEpHaZzaZ6RzSmc4hnZnQbQIl9hKWpy1n5paZLExZyJLUJSxJXUIr/1aMaT+Gi9pdpAHpdSTIx8r5PaI5v4exNCTvSAkb9+WyPiWHdSmHyCkswWI2YTGZsJhNOIA1SYfIyi/i540Z/LwxA4COkf4MaBtK/7YhDIgPISLAm5zCEjam5pCYmsP6lBxyj5Rw2WkxjOoRjUVbAUWkGvSOsI4kRCQwc+tMErMSaenfEiibKZWbCuBq3/OwVPE/a9f2PbXviYjUpjvvvLPOHrtbt27HDAh3evvtt7n66qvr7LndLSIigrCwMN55550KM7uc/r6JT0SaB6vZyumtTuf0VqeTnJvM51s/5+sdX5Oan8p/1/6X/679L6dFnsZF7S7i3Lhz8bP6Vf2gclICvK0MjA9lYHzoca+x2x1s2JfDb1sy+X1LJutSctiWkc+2jHw+Xmb8+xbm70VWftEx9120PYtX5m3j1jPbM6ZXy6rHlYhIs6akVB1JCDOGnW8+sJn+Uf0BZ1JqHw6Hg2KbMdC2yqSUa/ueklIiIo3F3LlzKSkpqfS2yMjIeo6mfjWzqQAichJiAmO4p9893Nb7Nn7d+yvf7vyWFWkrWJmxkpUZK3ly+ZMMbjmYc2LPYVjMMAI91S5a38xmEz1bt6Bn6xbcOaIj2QXFrNh9gOW7s1m+K5vN6bmuhFRMiA89WxnzqQqLbUz/cw+7sgq458t1vDp/G1f1j6VNiC8RgV5EBHgREeCNj6daNkXEoKRUHWkd0JoQ7xCyj2TzV/pfAPjbHZC7D5v96Bt2te+JiDQ9sbGx7g5BRKTB8/HwYXS70YxuN5q0/DTm7J7Dtzu+ZU/uHn5P/p3fk3/Hw+zBgOgB9Arvha+HLz5WH3w8jKNnWE/CfcPd/TKahRA/T87rHs153Y0WwJzCEnbuz6dtqB/BfhV/TrnxjHg+WbaX9xbtIjm7kGd/2nLM40UEeJEQ04LebVrQK8ZIfvl76UdTkeZI/+XXEZPJRM/wnixIXsD6rPUA+DnskJvimicFYK1qo4W274mIiIhIExftH80NPW5gUvdJbDu4jXl75/Hr3l/ZmbPTNX/q77wsXlzT5Rom9ZhEgGeAG6JuvoJ8rPRpc2yLNoC/lwc3D2vHhEFxfP5XEiv2ZLM/r4jMvCIyc4soLLGRmVfEvE0ZzNtkzKsymaB1sA9tQnxpE+JLTIgvMcG+hPh5EuDtQYC3lQBvDwK9rdoIKNLEKClVhxLCE1iQvAC7w2jVc7bvlZS17gF4VDUA0Kb2PRERERFpHkwmE51COtEppBO3976dXTm7+C3pN1LyUigsLXQdWYVZ7Di0g/c3vM9X27/i5oSbGddxHFa9Z24wfDwtXDekLdcNaes653A4yCsqZWt6HmuTDrE22ThSDxWSnG0cSzhw3Mc0m6BDRAC9YlrQq6zKqmNkgIaqizRiSkrVoYTwhArf+5W17zmHnANVD/5T+56IiIiINFPxQfHE94g/5rzD4eCPlD94adVL7M7ZzTMrnuGzzZ8xKn4UXUK60CW0C5G+kZhMSlY0JCaTiUBvK/3iQugXF+I6vz+viD0HCkg6cJik7MMkZx8m5WAhhwqLyTtSSm5hCQXFNuwO2JqRx9aMPD5fmQyAxWzC28OMZ7kjxM+LXq2D6NWmBb1jgokN9XX9XSgqtXEgv5jsgmKig7wJ9fdyy++FiBiUlKpD3UK7YTaZK1ZKFaRRUmokmkwmqs7q28oG5Wr7noiIiIgIYCQ3hscM5/RWp/P19q/579r/kpSXxJvr3nRd08KrBV1DuzI8Zjjnxp5LqM/xt82Je4UHeBEe4FUhUfV3pTY7WfnFrEsxqqvWJR8iMSWH/KJSCoptFBTbXNcmZxeyLvkQM5YamwKDfa208PUkK7+IvCOlFR63Q4Q/A+JDGNA2lAFtQ4gI9D6l1+JwOPh1cyb/XbCDQG8rT1/Sg5YtfE7pMUWaMiWl6pCv1ZeOwR3Zkm0M9/PHBA479lyjd7rKIecA9rKklEqRRUREREQq8DB7MK7TOEbFj+L7nd+zPms9W7K3sPPQTg4VHeLPfX/y574/eXbFswyMHsgF8RdwVsxZ+Hv6uzt0qSEPi5moIG+igqIY2S0KAJvdwf68IopKbRSX2ikqO1IOHna1Bm5MzeXg4RIOHj66FddqMRHkYySptmfmsz0zn0+WJQHg52khMtCbiEAvIgO9CfHz5EiJnYKiUuMoLsViNjGgbShndAynR6sgLGYTDoeDhduzeOmXraxLyXE91/mvLuK5S3u6YhaRipSUqmMJ4QlHk1LeoXD4MI7cVAA8LNUoJ1b7nog0M9dddx2HDh1i9uzZ7g6FuLg47rzzTu68887jXmMymfjmm28YO3bsca85cOAAXbp0YcWKFcTFxdVKbNOnT+fOO+/k0KFDtfJ4TdUDDzxAQUEBr7/+urtDEZE65Gf144rOV3AFVwBQZCtix8EdrMxYyU+7f2LDgQ0s2beEJfuMgekB1gACvQIJ9Awk0CuQmIAYLml/CT3Ce7jzZUgNWcwmooKOrWzqGxvMmF6tAKNdb0taHkdKbIQFeBHm50Wgjwcmk4nsgmJW7M5m+e4DLNuVzZb0XAqKbezKKmBXVsEJn3vJjgO8NG8bwb5WTu8QTnpOIX/tOQiAj9XC+EGxLNt1gHUpOfzz41VcOzCWh0Z1wdtqqf3fCJFGTEmpWnbocDG2ctv12gV0df3ayyccSObw/iQguOoh5w4H2DXoXESal1dffRWHw1H1hfXgr7/+ws/P75Qf58knn2TMmDG1lpCS6rvnnnuIj4/nrrvuIj7+2Lk0ItI0eVm86BbWjW5h3ZjQbQJ7c/cyd/dc5u6ay57cPeSV5JFXkkcqxofFy9OWM2vbLLqFduPyTpdzftvz8fY4tTYuaRi8PCwkxLSo9LYQP0/O6x7Fed2NKqbDxaWk5xwhI7eIzLwjZOYWcaCgGB+rBT8vC/5eHvh6eZBbWMLi7Vks2ZHFwcMlfL9uX9lzmbl2YCw3D29HmL8XxaV2XvxlK28v3MXHy/by155sJp3eFl9PD3w8zXh7WLB6mDmQX0xG7hHSc4+QkXOE3COldGsZyMD4UHq3aaFEljRpSkrVskve/JNd+49m1U3WPPzbG79ekWqhA/DZvKXABdUYcn60xFRJKRFpLoKCgtwdgkt4ePgpP8bhw4d5//33+fnnn2shoqajuLgYT8+6rwIOCwtj5MiRvPnmmzz//PN1/nwi0jDFBsZyS8It3NzzZg4WHSSnKIecohxyi3PJKcphWdoyftr9ExsPbGTqn1N5YeULnBN7Dt3CutE1pCvtg9vjZdFA7KbO19OD+HB/4sOrbu+8ZmAsJTY7a5MPsWjbfgCuHhhLZLmZVJ4eZqZc0IXB7cO4+4u1bEnP495ZidWK5dfNGbw6fzueHmZ6xbSgb2ww0UHehPt7ERbgRbi/F8F+nvh5WvCo6ufKBiAj9wjh/l6YtSlR/kZJqTrmKAmlNL8jJnMR+aUB4AFRpmwARnavoq/Y2boHat8TkSZl1qxZPProo+zYsQNfX1969+7Nt99+i5+f3zHte3l5edx8883Mnj2bwMBA7rvvPr799lt69erFK6+8AhhtdjfccAPbtm3j66+/JjQ0lNdff51BgwZxww03MH/+fOLj4/nggw847bTTXHF89dVXTJ06lR07dhAdHc2//vUv7r77btftf2/f2759O5MmTWLFihXEx8fz6quvVvla586di5eXFwMHDgTAbrfTpk0bHnroIW655RbXdWvWrKFv377s3r2b2NhYXnrpJT788EN27dpFSEgIo0eP5rnnnsPfv+ZzUHbu3MnkyZNZtmwZBQUFdOnShaeffpoRI0a4rikqKmLq1Kl89tlnZGZmEhMTw5QpU5g0aRIAGzdu5P7772fhwoU4HA569erF9OnTadeuHcOHD6/w5wEwduxYWrRowfTp012/l5MmTWL79u3Mnj2bSy65hOnTp3P//ffzzTffkJKSQlRUFFdffTVTp07Faj36Ycz333/PY489xvr16/H392fo0KF88803PPbYY3zxxRds2LChwuvt1asXo0eP5vHHHwdg9OjRPPTQQ0pKiQgmk4kQ7xBCvCsO1B7dbjT3nHYP3+z4hi+2fkFqfipfbf+Kr7Z/BYCHyYP4FvFE+0XjZ/UjwDPA9bVvZF8SwhMwmxp+YkBql9ViPmaTYGWGdQxn7h1DefXX7SQfLORIic11FJXaaeHrSXSgN1FB3kQGeuNtNbMm6RDLdh3g/9u787iqqrWB478zwGGeJycmRSUHcMLQTBOvmkZpWVq8SWl6nXozs8w5K/OWQ17N7NbtahOR+mbXHCrTNEWcRxRxBBVB5nk6cPb7B3nyBCoWcgCf7+ezP3j2XmfvtdcWWDxnrWel5Zey/2IW+y9m3fT8VhZqbC212Oq06LRqNGoVapUKjbpy83KworWnHa297GntaY+3iw2Xsoo4djmHE8m5HL+SS0puMX1aexBxvzcdmztVuUZRWTkx5zLJLiqjjac9bbzsazSC61peCW9sOMmWuFS6+jjz/vBgWrjY3PZ94t4hQalatm1K72r2Dq78sncl/LiZFzpa8sKTg26/RO2NQSlZfU8IUVOKAvqiur+uhU3lsqK3kZKSwtNPP817773H0KFDyc/PZ9euXTedsjdlyhRiYmLYsGEDnp6ezJkzh8OHDxMcHGxS7v333+edd95h9uzZvP/++zz77LP06NGDUaNGsXDhQqZNm8bIkSM5efIkKpWKQ4cO8dRTT/HGG28wfPhw9uzZw4QJE3B1deW5556rUg+DwcDjjz+Op6cn+/btIzc395a5pq7btWsXXbp0Mb5Wq9U8/fTTREVFmQSlvvrqK3r27ImPj4+x3LJly/Dz8+PChQtMmDCB1157jQ8//PC21/yjgoICBg0axPz589HpdHz++eeEh4eTkJCAt7c3ACNHjiQ2NpZly5YRFBTExYsXycjIACA5OZkHH3yQPn36sH37dhwcHIiJiaG8vPxWl61i0aJFzJkzh7lz5xr32dvbs3r1apo2bcqJEycYM2YM9vb2vPbaawBs2rSJoUOHMnPmTD7//HPKysrYvHkzAKNGjWLevHkcOHCAbt26AZXBvePHj/Ptt98arxESEsKVK1dITEyUKZRCiJtytnJmVPtRRN4XyZ6rezh47SDxmfHEZ8WTU5rDmewznMk+U+17m9o2ZaDfQAb5DaK1c+vb9/PFPcfD3or5Q2ues+z5npUr+SVmFrH3QianruaRnl9KRkEp6QWlpOeXUvTbioMlegMl+jIyC8tuer4fTt7+mt8cvMw3By/TvpkDEd19CPFzYc/5TLbFX2PP+UzKyg3GsmoV+LnZEtjEgU7ezvRt64Gf2+8pDwwGha/2X+K9LafJL63sLxxMymbQP3fx9tD2xpxfQkhQqpbd8heQY+U3nirvao3+cDPmk0IFaplHLISoIX0RvNO07q874ypY3j7/UkpKCuXl5Tz++OPGAEyHDtV30vLz8/nss8+IiooiLCwMgFWrVtG0adX7GzRoEH//+98BmDNnDitXrqRbt248+eSTAEybNo3Q0FCuXbuGl5cXS5YsISwsjNmzZwPQunVrTp06xcKFC6sNSv3888+cPn2aH3/80Xj9d955h4cffviW95uUlFSlvhERESxevJhLly7h7e2NwWAgOjqaWbNmGcvcGPDy9fXl7bffZty4cX8qKBUUFERQUJDx9VtvvcX69evZsGEDkyZN4syZM6xZs4atW7caR0/dmH9pxYoVODo6Eh0dbRzB1Lp16zuuR9++fU1GogEm9+zr68vUqVOJjo42BqXmz5/PiBEjmDdvnsn9ADRv3pwBAwawatUqY1Bq1apV9O7d26T+19s/KSlJglJCiNvSqDX0at6LXs17AZWBgWtF14jPjCerJIsCfUHlVlZAenE6u67s4mrhVf4T9x/+E/cf/B396erZlfZu7Wnn1o6Wji3RSF9e/AkqlQo/N1uTYM+NSssrKCqtoOC3VQELS8spLTdgMECFomAwKOgrDFzKKuLstQLOpOVz9loBBaXl2FpqaNfMkaDmjnRo7oSTtQXrjySz6UQKccl5TP/2RJXrNXe2xtvFhoTUfDILyzifXsj59EI2Hk/hrY2n8HezpW9bD7r4OPPv3Rc5lFSZ+D2ouSOT+7Vm+fazHL6Uw0vRR9mRkM68x9rhYPX74IsKg0JZuQErC/UdBXYVReFyVjFWFmrc7XUSFG5gJChVlxx+iwbnXa1Z+RtX3pNvLCFEIxEUFERYWBgdOnRgwIAB9O/fn2HDhuHs7Fyl7IULF9Dr9YSEhBj3OTo60qZNmyplO3bsaPy3p6cnYBrsur4vLS0NLy8v4uPjeeyxx0zO0bNnT5YuXUpFRQUajekfEPHx8bRo0cIkwBQaGnrb+y0uLsbKyjRZbnBwMIGBgURFRfH666+zc+dO0tLSjAE0qAyCLViwgNOnT5OXl0d5eTklJSUUFRVhY3Nnw94LCgp444032LRpkzEoWFxczKVLlctfHz16FI1GQ+/e1Y32rTzeq1cvkyl1f8aNUyev++abb1i2bBnnz5+noKCA8vJyHBwcTK49ZsyYm55zzJgxjBo1iiVLlqBWq4mKiuL99983KWNtbQ1U5vcSQog7pVKp8LL1wsu2+tQbJeUl/HrlV7Zc3MKvV37lQu4FLuReYM2ZNQBYa63xc/QDoNxQToWhgnKlHEedI/19+jPIbxDuNn89h6G49+i0GnRaDc62NU/1oigKmYVlONtYovlDfqcHW7sz55H7+L/DV4jad4nEzEI6ezsTFuhJWKAHAR52qFQqFEUhPb+UUyl5nErJI+ZcBvsuZFWuWrj7Iv/efREAW0sNrw5ow7OhvmjUKnoFuPHBL+dYtu0s648kE3MuAzc7HbnFevKK9cYRVRYaFY7WFjhYW+BobYGXgxXtmjrQrpkj7Zs64m6vo0RfwZ7zGfxyOp3tp9NIzikGwF6nxd/dlpbudvi62aLTqlH4bQ0xRcFSo6azjzNBzR2r5OIqKC3nx7hUNh6/SrlB4X/u9+FvgZ6SB+suk6BUXXL47Q+Z/BQwVNx+9JMxKCVT94QQd8DCpnLUkjmuWwMajYatW7eyZ88efvrpJ5YvX87MmTPZt28ffn5+f/7yNwRMrn9CVt0+g8FAXXJzcyM7O7vK/oiICGNQKioqioEDB+Lq6gpAYmIijzzyCOPHj2f+/Pm4uLiwe/duRo8eTVlZ2R0HpaZOncrWrVtZtGgRrVq1wtrammHDhlFWVvl75nrQ5mZud1ytVleZfqnX66uU++NKhrGxsURERDBv3jwGDBhgHI21ePHiGl87PDwcnU7H+vXrsbS0RK/XM2zYMJMyWVmVeThqI3G9EEL8kZXWiv6+/env25/8snxir8YSlxFHXGYcJzNOUlRexKnMU9W+93j6cZYcWkJ3r+480vIRQpuE4qRzwkL6/+IuUalUuNndPGm/s60lL/TyZ/QDfpQblGoX51KpVHg4WOHhYEWfNh5M6NOKvJLKFQm3xadxIDGLDs0cmTk4kKZOv/8e12rUTO7Xml4BbrwUfZQr2cWk5ZdWOb++QiGjoIyMgt+nI26JSzX+28O+MpBVesN0QguNigqDQn5pOceu5HLsSu4t28HeSkvPlm70au2Gq60l3x9P4edT10zOuetsBgEedozr3ZJHg5tioVGTV6In9nwmu89mcDApGy8HHf3u8ySsrSdejr9/CFlUVk7s+Ux2JKRzIaOAh9p48GSXFjja/Pnv7eKyCs6m5ePtYoOTTePJOS1Bqbpk5wkqDSgVUHDt9yDVzVT8Nn1PfikJIe6ESlWjaXTmpFKp6NmzJz179mTOnDn4+Piwfv16pkyZYlLO398fCwsLDhw4YMx9lJuby5kzZ3jwwQf/Uh0CAwOJiYkx2RcTE0Pr1q2rjJK6Xv7y5cukpKTQpEkTAPbu3Xvb63Tq1Ikvv/yyyv5nnnmGWbNmcejQIdatW8dHH31kPHbo0CEMBgOLFy9Gra7sDK5Zs+aO7u9GMTExPPfccwwdOhSoHDmVmJhoPN6hQwcMBgM7d+40SX5+XceOHfnss8/Q6/XVjpZyd3cnJSXF+LqiooK4uDgeeuihW9Zrz549+Pj4MHPmTOO+pKSkKtfetm0bzz//fLXn0Gq1REZGsmrVKiwtLRkxYkSVQFZcXBwWFha0a9fulvURQoi/yt7S3higAqgwVJCYl8ilvEto1Bq0Ki1adeV2NvssGy9s5Gj6UWJTYolNiTWex9bCFiedE446R3zsfQh0DeQ+1/sIdA3EwdLhZpcXotaoVCosNDUfIeRgZcGgDk0Y1KHJbct28XHhh8kPsudcBhZaNY6/jYhytLZAp1WTV1JOXrGe3N+2y1lFxCXnciI5lwsZhcZAVlNHKx5q60Hfth6EtnRFo1aRlFnEhfQCzqcXkpRZSLlBQa1SoaKyi5xbrGfvhSxyi/X8cDKVH06mmtTN382Wx4KbUVpewRexSZxNK+CVtcdYsvUMHg46jl3OwXDD53DxKfBLQjoziaN9MwdC/V05nZrPvgtZlFX8HuCKOZfJop8SGBLcjGdDfWjX1JESfQVnrxVwOjWPhNR8CkrLcbPT4WZnibu9Fa52lqTmlnDkUjaHL+UQn5JHuUFBp1XzeOdmPNfDjzZe9jV+RvWVBKXqkloD9k0g70rlFL7bBqVumL4nhBCNxL59+9i2bRv9+/fHw8ODffv2kZ6eTmBgYJWy9vb2REZG8uqrr+Li4oKHhwdz585Frb6zXAPVeeWVV+jWrRtvvfUWw4cPJzY2lg8++OCmOZv69etH69atiYyMZOHCheTl5ZkEU25mwIABTJ8+nezsbJMpir6+vvTo0YPRo0dTUVHBo48+ajzWqlUr9Ho9y5cvJzw8nJiYGJOg1Z0KCAjg22+/JTw8HJVKxezZs01GjPn6+hIZGcmoUaOMic6TkpJIS0vjqaeeYtKkSSxfvpwRI0Ywffp0HB0d2bt3LyEhIbRp04a+ffsyZcoUNm3aRMuWLVmyZAk5OTk1qtelS5eIjo6mW7dubNq0ifXr15uUmTt3LmFhYbRs2ZIRI0ZQXl7O5s2bmTZtmrHMCy+8YPz/88dAI1Qmm+/Vq9dtR10JIURt06g1tHRqSUunllWOdfbszPC2w7mcd5lNFzex5eIWLuZeREGhUF9Iob6Q5IJkTmWeYkviFuP7mts1p41LG1o5tSLAOYAA5wC87b3RquVPO9Fw2Om09G9X/ZRYeysLmjlV/zu7sLSc06n52FtpjdMJb9Tas3KFwVupMCicSM5l15l0dp3NIKOglL5tPXgsuBntmzkYzzmuT0u+3JvEf3ZfJDmn2DhF0N/NlgcC3Oju50piZiE/x1/j6OUc4pLziEvOM16nubM1fdq44+tqy7pDVzidmk/0gctEH7hME0crruWVmAS4asJepyW/tJyv91/m6/2X6dnKlZGhvthbackoKCOzoDIZfl5xeeWqjDotdrrKlRltLDXotGp0Wg2WWjU6rRpLrZo2XvbotObLeyc/ueqaQ9PfglLJQNXcGiauB6Vk5T0hRCPi4ODAr7/+ytKlS8nLy8PHx4fFixffNGH4kiVLGDduHI888ggODg689tprXL58uUqepjvVuXNn1qxZw5w5c3jrrbdo0qQJb775ZrVJzqFyitr69esZPXo0ISEh+Pr6smzZMgYOHHjL63To0MF4reuJ2K+LiIhgwoQJjBw50iRgEhQUxJIlS3j33XeZPn06Dz74IAsWLGDkyJF/6l6XLFnCqFGj6NGjB25ubkybNo28vDyTMitXrmTGjBlMmDCBzMxMvL29mTFjBgCurq5s376dV199ld69e6PRaAgODqZnz55A5Sp4x44dY+TIkWi1Wl5++eXbjpICePTRR3n55ZeZNGkSpaWlDB48mNmzZ/PGG28Yy/Tp04e1a9fy1ltv8Y9//AMHB4cqo+QCAgLo0aMHWVlZdO/evcp1oqOjTc4phBD1SQuHFowLGse4oHEYFAP5ZfnklOaQXZJNdkk253PPcyrzFKcyT5FckMyVgitcKbjCtkvbjOfQqrW4WrniYuVi3JytnHGwdMBB51D51dIBDxsP/J38sZC/L0QDZavT0sWnah7SO6FRqwhu4URwCydeDAu4aTkHKwsm9GnFqJ5+bD6RQnmFQs8AtyoBs4kPtSKjoJTtp9M4nJRNKw87+rTxoKW7rTHANfoBPw4kZvN5bCI/xKWSklsCgIutJW087WnjZY+zjSWZhb+tsJhfSkZBGQ7WFnT2dqKztzOdvJ1o5mTNgcRsVsVc5MeTqcScyyTmXOZfao9drz1EC5c7Sw1Rm1TKzdbgbqTy8vJwdHQkNzfXJJFqnVn7HJxcDwMWQOiEW5e9vB8+/Rs4+8FLR+uidkKIBqakpISLFy/i5+f3l4M0DUVhYSHNmjVj8eLFjB492tzVqZFNmzbx6quvEhcXZ5yOJ2qPoigEBAQwYcKEKlNAt2zZwiuvvMLx48fRam/+WdytvpfM3neoJ6QdhDC/3NJc4rPiOZd9jrM5ZzmbfZZzOecoLi+u8Tl0Gh1tXNrQ3rVydcC2Lm3xcfBBp7l5niEhRO1Jyy/hQnoh/u62uNv9+dUCr2QX8XlsEptPpKDTqn+b+lc5/c/R2oKSckPlyoy/bUVlFZSVGygtN/z2tfL1d5N64mFf+39H1LTfICOl6ppxBb7k25eV6XtCCMGRI0c4ffo0ISEh5Obm8uabbwJUWTmvPhs8eDBnz54lOTmZFi1amLs6jUp6ejrR0dGkpqZWm3eqsLCQVatW3TIgJYQQDYWjzpH7m9zP/U3uN+4zKAbSitLILM4ksySTrJIsskuyySrJIr8sn7yyPPJK88gry+NK/hXy9fkcTz/O8fTjxnOoUNHUril+jn74OvgCkFWSZdzyyvLwc/AjpEkI3by60c61nUwXFOJP8rC3qpUgUHNnG2YMCmTGoKopMBoS+UlS167nkcqrwcpYsvqeEEIAsGjRIhISErC0tKRLly7s2rULNzc3c1frjkyePPmunbtdu3ZVEoRf969//YuIiIi7dm1z8/DwwM3NjY8//tgkZ9d1f1yJTwghGhu1So2XrRdettXn57mRQTFwKe8SJzNPEpcRx8nMk5zLOUd+WT7JBckkFySzO3l3te9NLUw1JmO30drQyaMTze2b42Hjgbu1Ox42Hrhau2KpscRSbWn8amNhg2UtfMiuKApF5UXYWtTvxVyEEHdGglJ17Y5GSsnqe0II0alTJw4dOmTuatRrmzdvRq/XV3vM09OzjmtTt+6xLARCCPGXqFVqfB198XX0ZbD/YKDy52hmSSaJuYlczLtIUm4SGrXGJD+VrYUtJzNPsj9lPwevHSSvLI+Yq1UXlrgZewt7XKxdjHmvXK1dcbN2w83aDXdrd9ys3Whu3xxHnWOV96YUpLDh/Aa+v/A9SXlJdPXsyvig8XTz6vaXFz0Rf42iKHx37jscdA6EeYeZuzqigZKgVF0zBqXuZKSUTN8TQghxcz4+PuaughBCiAZKpVIZA0RdvW6+EFOwRzARgREYFAMJWQmcyDhBWlFa5VacRnpROlklWZRVlKE36CmtKMWgVK70mq/PJ1+fT1Je9aN6r/Ow9qCVcytaOrXEy8aLX6/8yv7U/Sj8/gHEwWsHGf3TaDp7dGZc0Djub3K/BKfMJOp0FP/Y/w8AlvddTp8WfcxbIdEgSVCqrl2fvpefAoYKUN9i6UVZfU8IIYQQQghRj6hVagJdAwl0vX0em3JDOYX6wspcV8VZZJZkGnNfZRZnklGcQUZxBulF6aQVpxm3PVf3mJwnxCuER1s+Skf3jnx9+mv+78z/cTjtMGO3jqWNcxvcbdyx0lih0+qw0lTm6skryzPm08ovy8fe0p72bu2NW0vHlmhu9bcYUFpRSmphKi3sW6BWVb9QiaIoHE0/Skl5CSFeIbc9Z2NxLP0Yiw4uMr6euXsma8LX0MyumRlrJRoiCUrVNXsvUGnAUA4FaeDQ5OZlDTJ9TwghhBBCCNEwadVaHHWOOOoc8Xf0v2XZgrICzuee51z2Oc7lnONK/hXaubUjvGW4SaBjRvcZjG4/mlUnV7HuzDoSshNIyE6oUX3is+JZe2YtANZaawKcAmjp1NK4Nbdrzvnc8xxNO8qRtCOcyjyF3qDH3dqdfj796O/Tn04endCoNSQXJLPh/AY2nNvAlYIrAPg4+DC6/Wge8X8Eiz/8Dac36LmQcwEXKxfcbdzvpBnNQlEUisuLsbGwqXIsuySbqTunUm4op593P9KK0jiecZypO6by2cOf1UoOMXHvUCn3WDKGerGc8ZL7KnNKvbAdmne5ebkjX8J/J0LAAIhYU3f1E0I0GLdaxl4IUXO3+l6qF32HekDaQQhR32QUZ3Dw2kGK9cWUVpRSWlFKcXkxKlQ46Bywt7THwbLya1pRGiczThKXGcfJjJMUlRfV6BoalYYKpcL42s3ajeZ2zTmaftS4z0Zrg0atIb8sHwBPG0+eb/88LZ1acvjaYQ5fO8zxjOMUlxcD0MqpFaFNQwltEkoXzy7VBn7uhsziTHZc3sHxjOOEeIXwsN/D1Y4Ai8+M543YNziTdYYRbUcwLmicMd9XhaGCCdsmsOfqHnwdfPl68Nfkl+Xz5MYnyS3N5Zm2zzC9+/Q6uR9Rv9W03yAjpczBoWllUCovGbhFUEpW3xNCCCGEEEKIarlZuzHQd2CNyw/wHQBUBlaS8pI4m3OWCzkXOJdzjgu5F7icf5kW9i0I9gimk0cnOrl3wtPWk70pe/kx8Ud+ufSLccohQPcm3Xms5WOEeYehoLA2YS2fnfqMa0XXjLmWbmRnYUehvpBzOZWjwb449QVatRYnnRO2FrbYaG2w1lpjbWENSuVqiQbFgAEDatSVQTadAw6WlZuNhQ1qlRo1atTqyq+WmsoVD20tbLHV2mKhseDQtUP8nPQzh9MOG/N8fXv2Wz4/9TlTu06lm1c3AIrLi1l5bCWfn/zcGIj7Mv5LNl7YyKTgSTzR+gk+Of4Je67uwUpjxeI+i7GztMPO0o53HniHidsmEnU6is6enY1tfSeK9EWkFKbg7eCNRSNPYVNhqLjtVM9rhdew0lpVuwBAY1IvglIrVqxg4cKFpKamEhQUxPLlywkJCam27Lfffss777zDuXPn0Ov1BAQE8Morr/Dss8/Wca3/gut5pW63Ap+svieEEEIIIYQQtUqj1uDv5I+/062nFF73YPMHebD5g+gr9MSmxJJSkEKv5r1oatfUpNxz7Z/j6cCn+e+5//JV/FcUlxfTyaMTXTy70NmjM/5O/uSV5rE3dS97r+4l9mosVwuvmgS67rZ2ru1o79aejRc2cirzFKN+HEWf5n0Y6DeQFUdXcDn/MgD9ffpX7juygvO553l739t8Gf+lMVn9nNA5tHZubdJGL3R4gX+f+Ddz98zF3tIeD2sPrC2ssdZaY6O1wUpb/aj+hKwEvkn4hk0XNlFUXoSVxor2bu0J9ggm2D2YYI/guxKYySrJ4vvz33M5/zI9m/akZ7Oed33q4eW8y8yNncupzFNM6TKFJ1s/WSVRv6IoRJ2OYtHBRVhrrJnbY+6fCvI1FGafvvfNN98wcuRIPvroI7p3787SpUtZu3YtCQkJeHh4VCm/Y8cOsrOzadu2LZaWlmzcuJFXXnmFTZs2MWDA7R9UvRh6/sMM2LsCerwI/d++ebk9H8BPM6HjcHj847qrnxCiwZDpe1X5+voyefJkJk+efEfvy8zMJDAwkP379+Pr61srdVm9ejWTJ08mJyenVs7XWL3++usUFhayfPlys9VBpu/dnrSDEELULkVRuFZ0jdzSXAr1hRSVF1GkL6K4vBi1So1KpaocBaVSU66Uk1+Wb0zcnleWR3F58e+jqX7bSitKKSwvpEj/+7n8HP34m8/f6Ovd1xhIyyzOZOWxlaw7s85keqKHjQezus/iIe+HgMpk9WsS1rDi6AryyvIAGNZ6GHND51a5n3JDOS/89AKHrh2q9n7drN3wd/THz9EPf0d/LDWWrD+3nuPpx41lLNWWlBnKTN6nQkU713aENg2lR9MeBLkHgQpOZZ4ymR5pUAzYWthiZ2GHrYUt9pb2+Dn60dalLYEugfg4+KBSqdibspf/O/N/bL+8nfLreZwBe0t7+nn3Y6DfQEK8QtCqTcfwKIpCob6Q9OJ0MoozyCzJxN/RnwCngNuuAKkoCmsS1rD40GLjNE6Avi36Mq/HPJysnADIL8tn7p65bE3aavL+Ya2H8Vq317DWWt/yOhWGCi7mXiStKI3Onp1vGgisCzXtN5g9KNW9e3e6devGBx98AIDBYKBFixa8+OKLvP766zU6R+fOnRk8eDBvvfXWbcvWiw7V9WBT+ydg2H9uXm7XEtg2D4L/B4asqLv6CSEajIYalOrTpw/BwcEsXbq01s/9Z4NSU6ZMIT8/n08++aTW6iJBqZrJyMjA39+fo0eP4u9fs0+ta5sEpW5P2kEIIRqfC7kXeP/Q++y+spsnWj/B5M6TsbO0q1IupySHf5/4N0XlRUwLmYZOo6v2fOlF6czdM5dzOecoLi+muLwy39etaFVawnzCGN5mOF08u5CYm8jR9KPGhPOJeYkm5W20NigoJsGdmrDWWmNrYWsyKq29a3vaubXjl0u/kFacZtyvQoVWrcVCbWH8WlReVO01fR18+ZvP3+jv2582zm2qBKhSClKYs2cOe1P2AtDVsyv3N7mfj45/RLmhHA9rD97p9Q72lva8suMVrhRcQavWMrXrVDKLM/n3iX+joNDKqRULH1xIK+dWAOSW5pJckMyl/EucyjjFiYwTnMo8ZcyX5qhzZFjAMEa0HYGXrdcdtVVtaBA5pcrKyjh06BDTp/+eCE2tVtOvXz9iY2Nv+35FUdi+fTsJCQm8++67d7Oqtev69L30M5C4++blss5XfpXpe0IIcVcVFRXx6aef8uOPP5q7KvVKWVkZlpZ3fwUdNzc3BgwYwMqVK1m4cOFdv54QQgghKvk7+rO873LKDeVVRgbdyMnKiandpt72fO427nzY70OTfRWGCgr0BSTlJXEh9wIXcy9yIfcCWSVZ9Gneh6EBQ3Gzdvu9Tr9NrXw84HGgMrfS3pS97Lm6h70pe8kqyaqsk86JTh6d6OzRmU6enbCzsKNAX0BhWSEF+gJySnM4k32G01mnOZN9xhgks7ewZ7D/YIa1HkYblzYATA+ZzuG0w2y5uIWtSVvJKc1Bb9CjN+ir3KOdhR1u1m446Bw4nXmaxLxEPjnxCZ+c+IRmds1wtXaFG4b+nM89T6G+ECuNFZO7TObptk+jVql5sPmDTNs1jYu5Fxnz0xi0ai16g56mtk1Z3Gcx7d3aAxDSJITpu6ZzLuccIzaNwM/Rj+T8ZPL1+dU+gxuDb5/Gfcrqk6sJ8w5jkN8gygxl5JTmkFOaQ25pLjmlOcy5f06dJduvjlmDUhkZGVRUVODp6Wmy39PTk9OnT9/0fbm5uTRr1ozS0lI0Gg0ffvghf/vb36otW1paSmnp75HZvLy82qn8X+HYvPLrtROwevDty2urj0ILIURD9Nxzz7Fz50527tzJP//5TwAuXrxIixYtGDt2LNu3byc1NRVvb28mTJjASy+9ZPLenJwcHnjgARYvXkxZWRkjRoxg6dKlWFj8HsAvKipi1KhRrF27FmdnZ2bNmsXYsWNvWqfNmzej0+m4//77gcpRu97e3sycOZPx48cbyx05coQuXbpw8eJFfHx8WLJkCatWreLChQu4uLgQHh7Oe++9h51d1U8Yb+f8+fNMmTKFvXv3UlhYSGBgIAsWLKBfv37GMqWlpcyZM4eoqCjS0tJo0aIF06dPZ/To0QCcPHmSadOm8euvv6IoCsHBwaxevZqWLVtWOzptyJAhODk5sXr1aqBylNno0aM5e/Ys3333HY8//jirV69m2rRprF+/nitXruDl5UVERARz5swxafPvv/+eN998kxMnTmBnZ0evXr1Yv349b775JmvWrCEuLs7kfoODgwkPDzeOcg4PD2fmzJkSlBJCCCHM4FYBqb9Ko9bgqHOko3tHOrp3vOP3e9p68lirx3is1WMYFAPncs6hVWnxdfStdvXA6lxPbn+t6BrBHsFVpsFp1Bq6eXWjm1c3ZnSfQU5pDuWGcvQVevSKHn2FHhutDa7WriYBnEJ9ITsv72Rr0lZ2Je8iuSCZ5IKquaM7undkfs/5+Dr6GvcFugYSPTiahQcXsu7MOvQGPX1a9OHtnm+b5NC6v8n9rAtfx8yYmcQkx3A66/dYiYuVC83tmtPGpQ0d3DrQzq0d/o7+qFCx48oOouKj2J+6n5+SfuKnpJ+qbZtJwZPu3aDUn2Vvb8/Ro0cpKChg27ZtTJkyBX9/f/r06VOl7IIFC5g3b17dV/JWmnaCwPDKkVK3Y2kDHZ68+3USQjQainLnw5lrg7XW+rbz6QH++c9/cubMGdq3b8+bb74JgLu7OwaDgebNm7N27VpcXV3Zs2cPY8eOpUmTJjz11FPG9//yyy80adKEX375hXPnzjF8+HCCg4MZM2aMsczixYt56623mDFjBuvWrWP8+PH07t2bNm3aVFunXbt20aXL76uhqtVqnn76aaKiokyCUl999RU9e/bEx8fHWG7ZsmX4+flx4cIFJkyYwGuvvcaHH35Y5Rq3U1BQwKBBg5g/fz46nY7PP/+c8PBwEhIS8Pb2BmDkyJHExsaybNkygoKCuHjxIhkZlUPQk5OTefDBB+nTpw/bt2/HwcGBmJgYysvLb3XZKhYtWsScOXOYO/f3XBH29vasXr2apk2bcuLECcaMGYO9vT2vvfYaAJs2bWLo0KHMnDmTzz//nLKyMjZv3gzAqFGjmDdvHgcOHKBbt8rVfY4cOcLx48f59ttvjdcICQnhypUrJCYm1lpOLyGEEEI0LmqV2iTBek3dSXJ7rVprMnLrVmwtbBnkP4hB/oMo0hdxJO2IcbqiChUqlQpbC1s6e3SudrU9Gwsb5oZWJjLPLslmoO/AavvTrtaufBj2IXuv7qXMUEYzu2Y0s2t2y2BSmHcYYd5hnMk+Q1R8FHEZcdhb2uNs5YyjzhEnnRNOOifsLO78w9TaZNacUmVlZdjY2LBu3TqGDBli3B8ZGUlOTg7//e9/a3SeF154gcuXL1c77aK6kVItWrSQfAhCiEahujw4Rfoiukd1r/O67HtmX40/ZalpTqlJkyaRmprKunXrgMqRUjt27OD8+fNoNJW/2J966inUajXR0dFA5WifXr168cUXXwCVQTovLy/mzZvHuHHjqr3OkCFDcHV15dNPPzXuO3r0KJ07dyYxMRFvb2/j6KlZs2bd9Dzr1q1j3LhxxkDRX80p1b59e8aNG8ekSZM4c+YMbdq0YevWrSajp66bMWMG0dHRJCQkmIxguq6mI6U6derE+vXrb1mvRYsWER0dzcGDBwHo0aMH/v7+fPnll9WWHzRoEL6+vsZg3f/+7/9y4sQJfvnlF2OZ63kHduzYQe/evW95/btBckrdnrSDEEIIIWqqpv2Gmo11u0ssLS3p0qUL27ZtM+4zGAxs27aN0NDQGp/HYDCYBJ5upNPpcHBwMNmEEELUTytWrKBLly64u7tjZ2fHxx9/zKVLl0zKtGvXzhiQAmjSpAlpaWkmZTp2/H1ouEqlwsvLq0qZGxUXF1cJRAQHBxMYGEhUVBQAO3fuJC0tjSef/H306s8//0xYWBjNmjXD3t6eZ599lszMTIqKiu743gsKCpg6dSqBgYE4OTlhZ2dHfHy88f6PHj2KRqO5acDm6NGj9OrVq9qA1J3o2rVrlX3ffPMNPXv2xMvLCzs7O2bNmmXyXI4ePUpYWNhNzzlmzBi+/vprSkpKKCsrIyoqilGjRpmUsbauHEb/Z9pOCCGEEEI0TGafvjdlyhQiIyPp2rUrISEhLF26lMLCQp5//nmgcqpCs2bNWLBgAVA5Ha9r1660bNmS0tJSNm/ezBdffMHKlSvNeRtCCFFvWGut2ffMPrNc96+Ijo5m6tSpLF68mNDQUOzt7Vm4cCH79pneyx+DLiqVCoPBcMdlbuTm5kZ2dnaV/REREURFRfH6668TFRXFwIEDcXV1BSAxMZFHHnmE8ePHM3/+fFxcXNi9ezejR482jgS+E1OnTmXr1q0sWrSIVq1aYW1tzbBhwygrq1wW+XrQ5mZud1ytVvPHwdF6fdXknba2tiavY2NjiYiIYN68eQwYMABHR0eio6NZvHhxja8dHh6OTqdj/fr1WFpaotfrGTZsmEmZrKzKpKXu7u63PJcQQgghhGg8zB6UGj58OOnp6cyZM4fU1FSCg4P54YcfjMnPL126hFr9+4CuwsJCJkyYwJUrV7C2tqZt27Z8+eWXDB8+3Fy3IIQQ9YpKpTJrssKasLS0pKKiwmRfTEwMPXr0YMKECcZ958+fr5P6dOrUqdqpZ8888wyzZs3i0KFDrFu3jo8++sh47NChQxgMBhYvXmz8PbVmzZo/XYeYmBiee+45hg4dClSOnEpMTDQe79ChAwaDgZ07d1Y7fa9jx4589tln6PX6akdLubu7k5KSYnxdUVFBXFwcDz300C3rtWfPHnx8fJg5c6ZxX1JSUpVrb9u2zfiB0h9ptVoiIyNZtWoVlpaWjBgxokogKy4uDgsLC9q1a3fL+gghhBBCiMbD7EEpqMwZMmnSpGqP7dixw+T122+/zdtvv10HtRJCCHG3+Pr6sm/fPhITE7Gzs8PFxYWAgAA+//xzfvzxR/z8/Pjiiy84cOAAfn5+d70+AwYMYPr06WRnZ+Ps7GxSzx49ejB69GgqKip49NFHjcdatWqFXq9n+fLlhIeHExMTYxK0ulMBAQF8++23hIeHo1KpmD17tsnoLl9fXyIjIxk1apQx0XlSUhJpaWk89dRTTJo0ieXLlzNixAimT5+Oo6Mje/fuJSQkhDZt2tC3b1+mTJnCpk2baNmyJUuWLKlRrquAgAAuXbpEdHQ03bp1Y9OmTVVyTs2dO5ewsDBatmzJiBEjKC8vZ/PmzUybNs1Y5oUXXiAwMBCoDMD90a5du+jVq9dtR10JIYQQQojGw6w5pYQQQtybpk6dikaj4b777sPd3Z1Lly7x97//nccff5zhw4fTvXt3MjMzTUZN3U0dOnSgc+fO1Y50ioiI4NixYwwdOtQkYBIUFMSSJUt49913ad++PV999ZVxqvmfsWTJEpydnenRowfh4eEMGDCAzp07m5RZuXIlw4YNY8KECbRt25YxY8ZQWFgIgKurK9u3b6egoIDevXvTpUsXPvnkE+OoqVGjRhEZGcnIkSPp3bs3/v7+tx0lBfDoo4/y8ssvM2nSJIKDg9mzZw+zZ882KdOnTx/Wrl3Lhg0bCA4Opm/fvuzfv9+kTEBAAD169KBt27Z07141EX90dLTJCopCCCGEEKLxM+vqe+YgK8cIIRqTW60YJu7Mpk2bePXVV4mLizOZNi5qh6IoBAQEMGHCBKZMmWJybMuWLbzyyiscP34crdY8g7gb4up7K1asYOHChaSmphIUFMTy5csJCQm5afm1a9cye/ZsEhMTCQgI4N1332XQoEE1vl59bQchhBBC1D8NYvU9IYQQor4YPHgwY8eOJTk52dxVaXTS09P54IMPSE1NrTbvVGFhIatWrTJbQKoh+uabb5gyZQpz587l8OHDBAUFMWDAgJuuMrlnzx6efvppRo8ezZEjRxgyZAhDhgwhLi6ujmsuhBBCCPE7GSklhBANmIyUajjatWtXJUH4df/617+IiIio4xrVHZVKhZubG//85z955plnzF2dajW0kVLdu3enW7dufPDBBwAYDAZatGjBiy++yOuvv16l/PDhwyksLGTjxo3Gfffffz/BwcE1zoVWH9tBCCGEEPVTTfsN8pGkEEIIUQc2b96MXq+v9tj1FWcbq3vs86+7rqysjEOHDjF9+nTjPrVaTb9+/YiNja32PbGxsVWmTQ4YMIDvvvvublZVCCGEEOKWJCglhBBC1AEfHx9zV0E0EhkZGVRUVFQJZnp6enL69Olq35Oamlpt+dTU1Jtep7S0lNLSUuPrvLy8v1BrIYQQQoiqJKeUEEIIIYSoYsGCBTg6Ohq3Fi1amLtKQgghhGhkJCglhBCNgEyPEuKvaUjfQ25ubmg0Gq5du2ay/9q1a3h5eVX7Hi8vrzsqDzB9+nRyc3ON2+XLl/965YUQQgghbiBBKSGEaMAsLCwAKCoqMnNNhGjYrn8PXf+eqs8sLS3p0qUL27ZtM+4zGAxs27aN0NDQat8TGhpqUh5g69atNy0PoNPpcHBwMNmEEEIIIWqT5JQSQogGTKPR4OTkZFwG3sbGBpVKZeZaCdFwKIpCUVERaWlpODk5odFozF2lGpkyZQqRkZF07dqVkJAQli5dSmFhIc8//zwAI0eOpFmzZixYsACAl156id69e7N48WIGDx5MdHQ0Bw8e5OOPPzbnbQghhBDiHidBKSGEaOCuT7+5HpgSQtw5JyenW05lq2+GDx9Oeno6c+bMITU1leDgYH744QdjMvNLly6hVv8+IL5Hjx5ERUUxa9YsZsyYQUBAAN999x3t27c31y0IIYQQQqBSGlIShVqQl5eHo6Mjubm5MgxdCNGoVFRUoNfrzV0NIRocCwuLW46Qkr5DJWkHIYQQQtRUTfsNMlJKCCEaCY1G02CmHgkhhBBCCCGEJDoXQgghhBBCCCGEEHVOglJCCCGEEEIIIYQQos5JUEoIIYQQQgghhBBC1Ll7LqfU9bzueXl5Zq6JEEIIIRqC632Ge2xtmCqkDyWEEEKImqpp/+meC0rl5+cD0KJFCzPXRAghhBANSX5+Po6OjuauhtlIH0oIIYQQd+p2/SeVco997GcwGLh69Sr29vaoVKpaP39eXh4tWrTg8uXLslyymcgzMC9pf/OTZ2B+8gzMq7bbX1EU8vPzadq0KWr1vZv5QPpQjZu0v/nJMzA/eQbmJe1vfrX5DGraf7rnRkqp1WqaN29+16/j4OAg30hmJs/AvKT9zU+egfnJMzCv2mz/e3mE1HXSh7o3SPubnzwD85NnYF7S/uZXW8+gJv2ne/fjPiGEEEIIIYQQQghhNhKUEkIIIYQQQgghhBB1ToJStUyn0zF37lx0Op25q3LPkmdgXtL+5ifPwPzkGZiXtH/DJM/NvKT9zU+egfnJMzAvaX/zM8czuOcSnQshhBBCCCGEEEII85ORUkIIIYQQQgghhBCizklQSgghhBBCCCGEEELUOQlKCSGEEEIIIYQQQog6J0GpWrZixQp8fX2xsrKie/fu7N+/39xVapQWLFhAt27dsLe3x8PDgyFDhpCQkGBSpqSkhIkTJ+Lq6oqdnR1PPPEE165dM1ONG7d//OMfqFQqJk+ebNwn7X/3JScn8z//8z+4urpibW1Nhw4dOHjwoPG4oijMmTOHJk2aYG1tTb9+/Th79qwZa9y4VFRUMHv2bPz8/LC2tqZly5a89dZb3JiqUZ5B7fr1118JDw+nadOmqFQqvvvuO5PjNWnvrKwsIiIicHBwwMnJidGjR1NQUFCHdyGqI/2nuiN9qPpF+lDmIX0o85H+U92r7/0nCUrVom+++YYpU6Ywd+5cDh8+TFBQEAMGDCAtLc3cVWt0du7cycSJE9m7dy9bt25Fr9fTv39/CgsLjWVefvllvv/+e9auXcvOnTu5evUqjz/+uBlr3TgdOHCAf/3rX3Ts2NFkv7T/3ZWdnU3Pnj2xsLBgy5YtnDp1isWLF+Ps7Gws895777Fs2TI++ugj9u3bh62tLQMGDKCkpMSMNW883n33XVauXMkHH3xAfHw87777Lu+99x7Lly83lpFnULsKCwsJCgpixYoV1R6vSXtHRERw8uRJtm7dysaNG/n1118ZO3ZsXd2CqIb0n+qW9KHqD+lDmYf0ocxL+k91r973nxRRa0JCQpSJEycaX1dUVChNmzZVFixYYMZa3RvS0tIUQNm5c6eiKIqSk5OjWFhYKGvXrjWWiY+PVwAlNjbWXNVsdPLz85WAgABl69atSu/evZWXXnpJURRp/7owbdo05YEHHrjpcYPBoHh5eSkLFy407svJyVF0Op3y9ddf10UVG73Bgwcro0aNMtn3+OOPKxEREYqiyDO42wBl/fr1xtc1ae9Tp04pgHLgwAFjmS1btigqlUpJTk6us7oLU9J/Mi/pQ5mH9KHMR/pQ5iX9J/Oqj/0nGSlVS8rKyjh06BD9+vUz7lOr1fTr14/Y2Fgz1uzekJubC4CLiwsAhw4dQq/XmzyPtm3b4u3tLc+jFk2cOJHBgwebtDNI+9eFDRs20LVrV5588kk8PDzo1KkTn3zyifH4xYsXSU1NNXkGjo6OdO/eXZ5BLenRowfbtm3jzJkzABw7dozdu3fz8MMPA/IM6lpN2js2NhYnJye6du1qLNOvXz/UajX79u2r8zoL6T/VB9KHMg/pQ5mP9KHMS/pP9Ut96D9p//IZBAAZGRlUVFTg6elpst/T05PTp0+bqVb3BoPBwOTJk+nZsyft27cHIDU1FUtLS5ycnEzKenp6kpqaaoZaNj7R0dEcPnyYAwcOVDkm7X/3XbhwgZUrVzJlyhRmzJjBgQMH+N///V8sLS2JjIw0tnN1P5PkGdSO119/nby8PNq2bYtGo6GiooL58+cTEREBIM+gjtWkvVNTU/Hw8DA5rtVqcXFxkWdiJtJ/Mi/pQ5mH9KHMS/pQ5iX9p/qlPvSfJCglGryJEycSFxfH7t27zV2Ve8bly5d56aWX2Lp1K1ZWVuauzj3JYDDQtWtX3nnnHQA6depEXFwcH330EZGRkWau3b1hzZo1fPXVV0RFRdGuXTuOHj3K5MmTadq0qTwDIUSDIH2ouid9KPOTPpR5Sf9J/JFM36slbm5uaDSaKitjXLt2DS8vLzPVqvGbNGkSGzdu5JdffqF58+bG/V5eXpSVlZGTk2NSXp5H7Th06BBpaWl07twZrVaLVqtl586dLFu2DK1Wi6enp7T/XdakSRPuu+8+k32BgYFcunQJwNjO8jPp7nn11Vd5/fXXGTFiBB06dODZZ5/l5ZdfZsGCBYA8g7pWk/b28vKqkjy7vLycrKwseSZmIv0n85E+lHlIH8r8pA9lXtJ/ql/qQ/9JglK1xNLSki5durBt2zbjPoPBwLZt2wgNDTVjzRonRVGYNGkS69evZ/v27fj5+Zkc79KlCxYWFibPIyEhgUuXLsnzqAVhYWGcOHGCo0ePGreuXbsSERFh/Le0/93Vs2fPKkt4nzlzBh8fHwD8/Pzw8vIyeQZ5eXns27dPnkEtKSoqQq02/TWq0WgwGAyAPIO6VpP2Dg0NJScnh0OHDhnLbN++HYPBQPfu3eu8zkL6T+YgfSjzkj6U+Ukfyryk/1S/1Iv+019OlS6MoqOjFZ1Op6xevVo5deqUMnbsWMXJyUlJTU01d9UanfHjxyuOjo7Kjh07lJSUFONWVFRkLDNu3DjF29tb2b59u3Lw4EElNDRUCQ0NNWOtG7cbV45RFGn/u23//v2KVqtV5s+fr5w9e1b56quvFBsbG+XLL780lvnHP/6hODk5Kf/973+V48ePK4899pji5+enFBcXm7HmjUdkZKTSrFkzZePGjcrFixeVb7/9VnFzc1Nee+01Yxl5BrUrPz9fOXLkiHLkyBEFUJYsWaIcOXJESUpKUhSlZu09cOBApVOnTsq+ffuU3bt3KwEBAcrTTz9trlsSivSf6pr0oeof6UPVLelDmZf0n+pefe8/SVCqli1fvlzx9vZWLC0tlZCQEGXv3r3mrlKjBFS7rVq1ylimuLhYmTBhguLs7KzY2NgoQ4cOVVJSUsxX6Ubujx0qaf+77/vvv1fat2+v6HQ6pW3btsrHH39sctxgMCizZ89WPD09FZ1Op4SFhSkJCQlmqm3jk5eXp7z00kuKt7e3YmVlpfj7+yszZ85USktLjWXkGdSuX375pdqf/ZGRkYqi1Ky9MzMzlaefflqxs7NTHBwclOeff17Jz883w92IG0n/qe5IH6r+kT5U3ZM+lPlI/6nu1ff+k0pRFOWvj7cSQgghhBBCCCGEEKLmJKeUEEIIIYQQQgghhKhzEpQSQgghhBBCCCGEEHVOglJCCCGEEEIIIYQQos5JUEoIIYQQQgghhBBC1DkJSgkhhBBCCCGEEEKIOidBKSGEEEIIIYQQQghR5yQoJYQQQgghhBBCCCHqnASlhBBCCCGEEEIIIUSdk6CUEELcIZVKxXfffWfuagghhBBCNBjSfxJCVEeCUkKIBuW5555DpVJV2QYOHGjuqgkhhBBC1EvSfxJC1Fdac1dACCHu1MCBA1m1apXJPp1OZ6baCCGEEELUf9J/EkLURzJSSgjR4Oh0Ory8vEw2Z2dnoHJo+MqVK3n44YextrbG39+fdevWmbz/xIkT9O3bF2tra1xdXRk7diwFBQUmZf7zn//Qrl07dDodTZo0YdKkSSbHMzIyGDp0KDY2NgQEBLBhwwbjsezsbCIiInB3d8fa2pqAgIAqnUAhhBBCiLok/SchRH0kQSkhRKMze/ZsnnjiCY4dO0ZERAQjRowgPj4egMLCQgYMGICzszMHDhxg7dq1/PzzzyadppUrVzJx4kTGjh3LiRMn2LBhA61atTK5xrx583jqqac4fvw4gwYNIiIigqysLOP1T506xZYtW4iPj2flypW4ubnVXQMIIYQQQtwh6T8JIcxCEUKIBiQyMlLRaDSKra2tyTZ//nxFURQFUMaNG2fynu7duyvjx49XFEVRPv74Y8XZ2VkpKCgwHt+0aZOiVquV1NRURVEUpWnTpsrMmTNvWgdAmTVrlvF1QUGBAihbtmxRFEVRwsPDleeff752blgIIYQQ4i+S/pMQor6SnFJCiAbnoYceYuXKlSb7XFxcjP8ODQ01ORYaGsrRo0cBiI+PJygoCFtbW+Pxnj17YjAYSEhIQKVScfXqVcLCwm5Zh44dOxr/bWtri4ODA2lpaQCMHz+eJ554gsOHD9O/f3+GDBlCjx49/tS9CiGEEELUBuk/CSHqIwlKCSEaHFtb2yrDwWuLtbV1jcpZWFiYvFapVBgMBgAefvhhkpKS2Lx5M1u3biUsLIyJEyeyaNGiWq+vEEIIIURNSP9JCFEfSU4pIUSjs3fv3iqvAwMDAQgMDOTYsWMUFhYaj8fExKBWq2nTpg329vb4+vqybdu2v1QHd3d3IiMj+fLLL1m6dCkff/zxXzqfEEIIIcTdJP0nIYQ5yEgpIUSDU1paSmpqqsk+rVZrTIa5du1aunbtygMPPMBXX33F/v37+fTTTwGIiIhg7ty5REZG8sYbb5Cens6LL77Is88+i6enJwBvvPEG48aNw8PDg4cffpj8/HxiYmJ48cUXa1S/OXPm0KVLF9q1a0dpaSkbN240duqEEEIIIcxB+k9CiPpIglJCiAbnhx9+oEmTJib72rRpw+nTp4HKlV2io6OZMGECTZo04euvv+a+++4DwMbGhh9//JGXXnqJbt26YWNjwxNPPMGSJUuM54qMjKSkpIT333+fqVOn4ubmxrBhw2pcP0tLS6ZPn05iYiLW1tb06tWL6OjoWrhzIYQQQog/R/pPQoj6SKUoimLuSgghRG1RqVSsX7+eIUOGmLsqQgghhBANgvSfhBDmIjmlhBBCCCGEEEIIIUSdk6CUEEIIIYQQQgghhKhzMn1PCCGEEEIIIYQQQtQ5GSklhBBCCCGEEEIIIeqcBKWEEEIIIYQQQgghRJ2ToJQQQgghhBBCCCGEqHMSlBJCCCGEEEIIIYQQdU6CUkIIIYQQQgghhBCizklQSgghhBBCCCGEEELUOQlKCSGEEEIIIYQQQog6J0EpIYQQQgghhBBCCFHnJCglhBBCCCGEEEIIIerc/wM5r/GrLFjiYwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss Functions assignment questions"
      ],
      "metadata": {
        "id": "Qoxsyqn9F4Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Explain the concept of a loss function in the context of deep learning. Why are loss functions important in training neural networks?\n",
        "# Ans: A loss function (also called a cost function or objective function) is a mathematical function that measures the difference between the predicted output of a neural network and the true target value (ground truth). It quantifies how well the model's predictions match the actual results. The goal of training a neural network is to minimize this loss function, thereby improving the model's accuracy and performance.\n",
        "\n",
        "# Why Are Loss Functions Important in Training Neural Networks?\n",
        "# Loss functions play a crucial role in the training process of neural networks for the following reasons:\n",
        "\n",
        "# Guides Optimization:\n",
        "\n",
        "# Neural networks learn by adjusting their weights based on feedback from the loss function. During training, the optimization algorithm (usually Gradient Descent or its variants) uses the loss function to compute the gradients (partial derivatives) with respect to the weights. These gradients are then used to update the weights in the direction that reduces the loss. The goal is to minimize the loss function iteratively.\n",
        "# Quantifies the Error:\n",
        "\n",
        "# The loss function helps to measure how well the network is performing. By calculating the difference between the predicted output and the true label, it tells the network how far off its predictions are. This error is essential for learning, as it tells the network what to adjust.\n",
        "# Optimizing Model Performance:\n",
        "\n",
        "# The choice of loss function significantly affects the learning process and, ultimately, the model's performance. For example, in classification tasks, using cross-entropy loss (which penalizes wrong predictions) helps improve classification accuracy. In regression tasks, mean squared error (MSE) can be used to reduce the difference between predicted and actual continuous values.\n",
        "# Determines Model Behavior:\n",
        "\n",
        "# The type of loss function chosen directly impacts the model's behavior during training. For instance, Mean Squared Error (MSE) is commonly used for regression because it penalizes large errors more than small ones, promoting a smoother and more accurate regression model. On the other hand, categorical cross-entropy loss is suited for classification tasks where the output is categorical."
      ],
      "metadata": {
        "id": "gq4Y6agYF5bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Compare and contrast commonly used loss functions in deep learning, such as Mean Squared Error (MSE), Binary Cross-Entropy, and Categorical Cross-Entropy. When would you choose one over the other?\n",
        "# Ans: In deep learning, the choice of loss function depends on the type of problem you're trying to solve, such as regression or classification. Below is a comparison of three commonly used loss functions: Mean Squared Error (MSE), Binary Cross-Entropy, and Categorical Cross-Entropy, along with guidance on when to choose one over the other.\n",
        "\n",
        "# 1. Mean Squared Error (MSE)\n",
        "# Use Case:\n",
        "# Regression problems (e.g., predicting continuous numerical values).\n",
        "# Examples: Predicting house prices, temperature, stock prices, etc.\n",
        "\n",
        "# 2. Binary Cross-Entropy (Log Loss)\n",
        "# Use Case:\n",
        "# Binary classification problems (e.g., distinguishing between two classes, such as spam vs. non-spam, positive vs. negative sentiment).\n",
        "# Examples: Email spam detection, sentiment analysis, etc.\n",
        "\n",
        "# 3. Categorical Cross-Entropy (Multi-Class Cross-Entropy)\n",
        "# Use Case:\n",
        "# Multi-class classification problems (e.g., classifying an image into one of several categories, such as identifying animal species, object recognition in images).\n",
        "# Examples: Image classification, multi-class sentiment analysis, etc."
      ],
      "metadata": {
        "id": "_OY2rBbqGT5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Discuss the challenges associated with selecting an appropriate loss function for a given deep learning task. How might the choice of loss function affect the training process and model performance?\n",
        "# Ans: Choosing the correct loss function for a deep learning task is a critical aspect of model development. The choice can significantly impact the training process and the overall performance of the model. Here are the key challenges associated with selecting a loss function:\n",
        "\n",
        "# 1. Nature of the Problem (Regression vs. Classification)\n",
        "# Challenge: The most fundamental challenge is identifying whether the task is a regression or classification problem. For instance, Mean Squared Error (MSE) is used for regression, while Cross-Entropy Loss is used for classification. Choosing an inappropriate loss function can lead to ineffective training and poor model performance.\n",
        "\n",
        "# Solution: Carefully define the problem (continuous vs. categorical outputs) to determine the appropriate loss function. Regression tasks typically require loss functions like MSE, while classification tasks require either binary cross-entropy (for binary classification) or categorical cross-entropy (for multi-class classification).\n",
        "# 2. Imbalanced Data\n",
        "# Challenge: In classification tasks, when classes are imbalanced (i.e., one class has significantly more samples than another), a standard loss function like binary cross-entropy or categorical cross-entropy can lead to biased models that favor the majority class. This results in poor performance on the minority class.\n",
        "\n",
        "# Solution: Modify the loss function to account for class imbalance, such as using weighted loss functions where more weight is assigned to underrepresented classes, or apply techniques like oversampling or undersampling in the dataset.\n",
        "# 3. Optimization and Convergence Issues\n",
        "# Challenge: Some loss functions can lead to slow convergence or cause optimization issues during training. For example, loss functions like MSE might struggle with gradient descent when the predictions are very far from the target values, as the gradients can become too small or unstable (a known issue with vanishing gradients).\n",
        "\n",
        "# Solution: Choosing a loss function that is more stable and easier to optimize, such as Huber Loss, which combines the benefits of both MSE and mean absolute error (MAE), can help prevent issues like vanishing gradients and speed up convergence.\n",
        "# 4. Overfitting and Underfitting\n",
        "# Challenge: Certain loss functions may make the model overly sensitive to outliers or noise in the training data. For instance, MSE tends to be sensitive to large outliers, which can lead to overfitting.\n",
        "\n",
        "# Solution: Using a robust loss function, such as Huber loss or mean absolute error (MAE), which is less sensitive to outliers, can mitigate overfitting and underfitting. Additionally, incorporating regularization techniques such as L2 regularization can help balance the model's ability to generalize.\n",
        "# 5. Loss Function Mismatch with Performance Metrics\n",
        "# Challenge: Sometimes, the loss function chosen doesn't align well with the performance metric you care about. For example, a model trained with MSE might not perform well according to accuracy or F1-score, even if the MSE is minimized.\n",
        "\n",
        "# Solution: It's important to align the loss function with the evaluation metric. For instance, focal loss is a modification of cross-entropy that helps improve performance on class imbalance tasks, as it focuses more on hard-to-classify examples. Additionally, evaluating the model with multiple metrics (e.g., accuracy, precision, recall) during training can give a more complete view of performance.\n",
        "# 6. Multi-task Learning\n",
        "# Challenge: In multi-task learning, where the model is expected to solve several tasks simultaneously (e.g., predicting both bounding boxes and class labels in object detection), it is difficult to define a single loss function that is optimal for all tasks.\n",
        "\n",
        "# Solution: Use a weighted sum of loss functions where each task has its own loss function, and the total loss is a combination of them. The weights for each loss term can be tuned based on the importance of each task.\n",
        "# 7. Type of Model Output (Probability vs. Raw Score)\n",
        "# Challenge: Different models produce different types of outputs. For example, models might output raw scores or probabilities, and some loss functions, like cross-entropy, expect probabilities as input. If the model outputs raw logits, using a loss function like binary cross-entropy directly on those can lead to incorrect training behavior.\n",
        "\n",
        "# Solution: Ensure compatibility between the model output and the loss function. If the model produces logits, use a softmax or sigmoid activation layer before the loss function (depending on the task). If probabilities are produced directly, you can use cross-entropy directly without the need for a softmax layer."
      ],
      "metadata": {
        "id": "CNc3m5RhG4Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Implement a neural network for binary classification using TensorFlow or PyTorch. Choose an appropriate loss function for this task and explain your reasoning. Evaluate the performance of your model on a test dataset.\n",
        "# Ans: Below is an example implementation of a neural network for binary classification using TensorFlow. The task is to classify whether a given input belongs to class 0 or class 1. We'll use the Binary Cross-Entropy loss function, which is the most appropriate loss function for binary classification tasks. After training the model, we'll evaluate its performance on a test dataset.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "import numpy as np\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# Build a neural network model\n",
        "model = Sequential([\n",
        "    Dense(64, input_dim=X_train.shape[1], activation='relu'),  # Input layer and first hidden layer\n",
        "    Dense(32, activation='relu'),  # Second hidden layer\n",
        "    Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary output\n",
        "])\n",
        "\n",
        "# Compile the model with the binary cross-entropy loss function and the Adam optimizer\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Explanation of the Loss Function\n",
        "# We use Binary Cross-Entropy as the loss function because:\n",
        "\n",
        "# In binary classification, the model outputs a probability (after applying the sigmoid activation function), which is a value between 0 and 1. The binary cross-entropy loss function measures the difference between the predicted probability and the true class label (0 or 1).\n",
        "# It is the most commonly used loss function for binary classification problems, as it directly correlates the predicted probability with the true label, encouraging the model to output probabilities that are as close as possible to the true values.\n",
        "# Key Advantages of Binary Cross-Entropy:\n",
        "# Works well when the classes are imbalanced (e.g., using a weighted version).\n",
        "# Provides meaningful gradients for optimization.\n",
        "# Suitable for tasks where the output represents a probability (using sigmoid activation).\n",
        "# Evaluating Performance\n",
        "# After training, we evaluate the model's performance on the test set. The model's accuracy and loss values provide insight into how well the model has learned to classify binary labels.\n",
        "\n",
        "# Final Notes\n",
        "# Model Architecture: This is a simple feedforward neural network, with two hidden layers. More complex architectures could involve more layers, dropout, batch normalization, etc., depending on the complexity of the task.\n",
        "# Loss Function: Binary Cross-Entropy is used because this is a binary classification task (output is 0 or 1). If we were to handle multi-class classification, we would use Categorical Cross-Entropy.\n",
        "# Evaluation: The test accuracy and loss provide the final performance metrics, but additional metrics like precision, recall, and F1-score might be useful in imbalanced datasets.\n",
        "# This should give you a complete, working example of implementing and evaluating a binary classification neural network using TensorFlow."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GbKGjP79HaXx",
        "outputId": "e5284921-be6a-4c8f-de54-05f46f61b31c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.5363 - loss: 0.7163 - val_accuracy: 0.7937 - val_loss: 0.5014\n",
            "Epoch 2/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8496 - loss: 0.4721 - val_accuracy: 0.8500 - val_loss: 0.3907\n",
            "Epoch 3/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8720 - loss: 0.3722 - val_accuracy: 0.8938 - val_loss: 0.3216\n",
            "Epoch 4/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9241 - loss: 0.2731 - val_accuracy: 0.9125 - val_loss: 0.2760\n",
            "Epoch 5/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9321 - loss: 0.2346 - val_accuracy: 0.9125 - val_loss: 0.2490\n",
            "Epoch 6/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9173 - loss: 0.2366 - val_accuracy: 0.9250 - val_loss: 0.2207\n",
            "Epoch 7/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9499 - loss: 0.1754 - val_accuracy: 0.9250 - val_loss: 0.2103\n",
            "Epoch 8/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9353 - loss: 0.1818 - val_accuracy: 0.9312 - val_loss: 0.1883\n",
            "Epoch 9/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9471 - loss: 0.1505 - val_accuracy: 0.9250 - val_loss: 0.1906\n",
            "Epoch 10/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9373 - loss: 0.1642 - val_accuracy: 0.9438 - val_loss: 0.1667\n",
            "Epoch 11/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9527 - loss: 0.1434 - val_accuracy: 0.9250 - val_loss: 0.1740\n",
            "Epoch 12/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9660 - loss: 0.1199 - val_accuracy: 0.9500 - val_loss: 0.1561\n",
            "Epoch 13/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9666 - loss: 0.1108 - val_accuracy: 0.9438 - val_loss: 0.1463\n",
            "Epoch 14/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9786 - loss: 0.0846 - val_accuracy: 0.9438 - val_loss: 0.1458\n",
            "Epoch 15/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9633 - loss: 0.1075 - val_accuracy: 0.9500 - val_loss: 0.1349\n",
            "Epoch 16/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9741 - loss: 0.0874 - val_accuracy: 0.9500 - val_loss: 0.1292\n",
            "Epoch 17/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9735 - loss: 0.0856 - val_accuracy: 0.9563 - val_loss: 0.1260\n",
            "Epoch 18/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9852 - loss: 0.0718 - val_accuracy: 0.9563 - val_loss: 0.1255\n",
            "Epoch 19/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9879 - loss: 0.0584 - val_accuracy: 0.9500 - val_loss: 0.1215\n",
            "Epoch 20/20\n",
            "\u001b[1m20/20\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9884 - loss: 0.0673 - val_accuracy: 0.9563 - val_loss: 0.1183\n",
            "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9553 - loss: 0.1211  \n",
            "Test Loss: 0.1532\n",
            "Test Accuracy: 94.00%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Consider a regression problem where the target variable has outliers. How might the choice of loss function impact the model's ability to handle outliers? Propose a strategy for dealing with outliers in the context of deep learning.\n",
        "# Ans: In regression problems, outliers can significantly affect the modelâ€™s performance, especially when using traditional loss functions like Mean Squared Error (MSE). The choice of loss function plays a critical role in determining how the model handles outliers. Below is an explanation of how the choice of loss function impacts the modelâ€™s ability to handle outliers and a proposed strategy for mitigating their influence.\n",
        "\n",
        "# Impact of Loss Functions on Outliers\n",
        "# Mean Squared Error (MSE)\n",
        "\n",
        "# Impact: MSE is highly sensitive to outliers. Since the loss is computed as the square of the difference between the predicted and true values, large errors (such as those caused by outliers) have a disproportionately large impact on the total loss. This means that a few extreme outliers can dominate the optimization process and cause the model to focus more on minimizing the errors for those outliers rather than learning the underlying distribution of the data.\n",
        "# Example: If the model predicts y_pred = 100 for a target value of y = 2000, the error is 2000 - 100 = 1900. The MSE will be 1900^2 = 3,610,000, which can overshadow the smaller errors in the majority of the dataset.\n",
        "# Mean Absolute Error (MAE)\n",
        "\n",
        "# Impact: MAE computes the absolute difference between predicted and true values, rather than squaring it. This makes it less sensitive to large errors than MSE. As a result, MAE is more robust to outliers, as the loss function does not amplify the impact of extreme values as much as MSE does.\n",
        "# Example: For the same example where y_pred = 100 and y = 2000, the error is |2000 - 100| = 1900. The MAE loss would simply be 1900, which, while large, is not as drastically exaggerated as MSE would make it.\n",
        "# Huber Loss\n",
        "\n",
        "# Impact: Huber loss is a hybrid loss function that combines the benefits of MSE and MAE. It uses MSE for smaller errors and MAE for larger errors. The idea is to have the benefits of squaring small errors (which encourages better convergence) but avoid the extreme sensitivity of MSE to large outliers by using the MAE approach for larger errors. This makes Huber loss more robust to outliers compared to MSE while still promoting good gradient flow during optimization.\n",
        "# Example: For smaller errors, Huber loss behaves like MSE, but for large errors, it behaves like MAE, effectively reducing the influence of outliers.\n",
        "# Strategy for Dealing with Outliers in Deep Learning\n",
        "# Using Robust Loss Functions:\n",
        "\n",
        "# Huber Loss: A primary strategy for handling outliers is to use Huber Loss, which balances between MSE and MAE. This loss function ensures that outliers have less influence on the overall loss, allowing the model to focus more on the majority of the data points.\n",
        "\n",
        "from tensorflow.keras.losses import Huber\n",
        "\n",
        "model.compile(optimizer='adam', loss=Huber(), metrics=['mae'])\n",
        "\n",
        "\n",
        "# MAE (Mean Absolute Error): In some cases, when outliers are particularly problematic and the relationship between the variables is linear, MAE may be a good alternative, as it treats all errors equally and doesnâ€™t exaggerate large ones.\n",
        "# Data Preprocessing:\n",
        "\n",
        "# Outlier Detection and Removal: Before training the model, it's often helpful to identify and remove or transform outliers. Common techniques for this include:\n",
        "# Z-Score Method: Identify outliers that have a z-score greater than a certain threshold (e.g., 3 standard deviations from the mean).\n",
        "# IQR Method: Use the Interquartile Range (IQR) to detect and remove values that fall outside of the range defined by Q1 - 1.5 * IQR and Q3 + 1.5 * IQR.\n",
        "# Transformation: Another strategy is to transform the target variable or the features to reduce the impact of outliers. For example, applying a log transformation to the target variable can reduce the influence of extreme values, making the distribution more symmetric.\n",
        "\n",
        "import numpy as np\n",
        "y_train_transformed = np.log1p(y_train)\n",
        "\n",
        "\n",
        "# Regularization:\n",
        "\n",
        "# Regularization methods like L1 (Lasso) or L2 (Ridge) can help mitigate the impact of outliers by preventing the model from fitting noise or extreme values in the data. L1 regularization encourages sparsity and may help ignore outliers by assigning them very small coefficients, while L2 regularization penalizes large weights, potentially reducing the impact of outliers.\n",
        "\n",
        "\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))  # Example of L2 regularization\n",
        "\n",
        "\n",
        "# Outlier Detection during Training:\n",
        "\n",
        "# Another advanced strategy involves training the model with outlier detection built into the pipeline. For example, using a robust regression technique, like RANSAC (Random Sample Consensus), to fit the model while rejecting outliers during training. This method iteratively refines the model by selecting inliers (points close to the model) and ignoring outliers.\n",
        "# Ensemble Methods:\n",
        "\n",
        "# If the dataset contains a lot of noise or outliers, ensemble methods like Random Forest or Gradient Boosting can be more robust compared to neural networks. These models are less sensitive to individual outliers and tend to average over many trees or boosting iterations, thus reducing the impact of any single outlier."
      ],
      "metadata": {
        "id": "8O8SUhzDIQDM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Explore the concept of weighted loss functions in deep learning. When and why might you use weighted loss functions? Provide examples of scenarios where weighted loss functions could be beneficial.\n",
        "# Ans: In deep learning, a weighted loss function is a loss function where the individual errors (losses) for different samples are multiplied by a weight before being summed or averaged. These weights are typically used to assign different importance to different data points. This is particularly useful in cases where some data points are more important or require more attention than others.\n",
        "\n",
        "# The basic idea behind weighted loss functions is to adjust the contribution of each sample to the overall loss, based on the importance or significance of the sample. The weight values are typically non-negative, and in most cases, higher weights imply greater importance.\n",
        "\n",
        "# Why Use Weighted Loss Functions?\n",
        "# There are several reasons why you might use weighted loss functions in deep learning:\n",
        "\n",
        "# Class Imbalance: In many real-world tasks (such as classification), the dataset might have an unequal distribution of classes. For example, in fraud detection, the number of fraudulent transactions (positive class) is much lower than legitimate transactions (negative class). In this case, the model might be biased toward the majority class and fail to learn how to predict the minority class accurately. By applying higher weights to the minority class, the model can be forced to focus more on correctly predicting the minority class.\n",
        "\n",
        "# Outliers: In regression tasks, there may be certain data points (outliers) that have a significant influence on the model's learning process. If outliers are more important (e.g., rare but critical events), you may want to apply higher weights to these points to make sure the model focuses on learning from them.\n",
        "\n",
        "# Importance of Samples: In some scenarios, the importance of each sample may differ based on external factors or the specific problem. For instance, in medical diagnosis, misclassifying a high-risk patient might be more costly than misclassifying a low-risk one. In such cases, you can apply higher weights to high-risk patients.\n",
        "\n",
        "# Handling Noisy Labels: When you have noisy labels, you can assign lower weights to the samples with noisy labels, reducing their impact on model training.\n"
      ],
      "metadata": {
        "id": "CVx6o5nqJQb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Investigate how the choice of activation function interacts with the choice of loss function in deep learning models. Are there any combinations of activation functions and loss functions that are particularly effective or problematic?\n",
        "# Ans: In deep learning, the choice of activation function and loss function are crucial to the model's performance and convergence. These two components interact in important ways, and certain combinations can either improve training or create challenges. Here's an exploration of how these components work together and some considerations for selecting them.\n",
        "\n",
        "# 1. The Role of Activation Functions\n",
        "# Activation functions in neural networks determine the output of each neuron. They introduce non-linearity, which allows the model to learn complex patterns in the data. The choice of activation function affects the gradients during backpropagation and can influence training stability, convergence speed, and model expressiveness.\n",
        "\n",
        "# ReLU (Rectified Linear Unit): Commonly used in hidden layers because it helps mitigate the vanishing gradient problem and speeds up training. It is particularly useful in deep networks.\n",
        "\n",
        "# Sigmoid and Tanh: Often used for binary classification tasks or when outputs are expected to be in a bounded range. They suffer from vanishing gradients when used in deep networks.\n",
        "\n",
        "# Softmax: Used for multi-class classification tasks in the output layer, as it transforms the network's outputs into a probability distribution over multiple classes.\n",
        "\n",
        "# 2. The Role of Loss Functions\n",
        "# Loss functions quantify how well the model's predictions match the true labels. The type of loss function should correspond to the nature of the problem (e.g., regression, binary classification, multi-class classification). Some common loss functions include:\n",
        "\n",
        "# Mean Squared Error (MSE): Used in regression problems, it measures the squared difference between predicted and true values.\n",
        "\n",
        "# Binary Cross-Entropy: Used for binary classification tasks, it calculates the loss for each sample in a two-class classification problem.\n",
        "\n",
        "# Categorical Cross-Entropy: Used for multi-class classification tasks when the output is a probability distribution over multiple classes.\n",
        "\n",
        "# 3. Key Interactions Between Activation and Loss Functions\n",
        "# The combination of activation and loss functions is important because their characteristics must align to ensure proper learning and convergence. Let's explore some common combinations:\n",
        "\n",
        "# Combination 1: ReLU + MSE (Regression)\n",
        "# Activation Function: ReLU\n",
        "# Loss Function: Mean Squared Error (MSE)\n",
        "# This combination is typical for regression tasks where the target variable is continuous. ReLU is effective in handling non-linearities, while MSE measures how close the predicted value is to the actual value. This combination generally works well, but ReLU can be problematic when dealing with negative target values because it clips all negative outputs to zero. To handle negative outputs, other activation functions, like linear, may be preferred in the output layer.\n",
        "\n",
        "# Combination 2: Sigmoid + Binary Cross-Entropy (Binary Classification)\n",
        "# Activation Function: Sigmoid\n",
        "# Loss Function: Binary Cross-Entropy\n",
        "# This combination is one of the most commonly used for binary classification problems. Sigmoid produces an output between 0 and 1, which is interpreted as a probability. Binary Cross-Entropy computes the log loss based on the predicted probability of the positive class. The interaction here works well because the sigmoid output can be directly interpreted as a probability, which is exactly what Binary Cross-Entropy needs.\n",
        "\n",
        "# However, Sigmoid can suffer from the vanishing gradient problem for deep networks, especially when the activations are close to 0 or 1. This can slow down training. To alleviate this, ReLU or Leaky ReLU might be used in the hidden layers, but the Sigmoid and Binary Cross-Entropy pairing in the output layer remains effective for binary classification.\n",
        "\n",
        "# Combination 3: Softmax + Categorical Cross-Entropy (Multi-Class Classification)\n",
        "# Activation Function: Softmax\n",
        "# Loss Function: Categorical Cross-Entropy\n",
        "# For multi-class classification, this combination is highly effective. Softmax transforms the network's output into a probability distribution over the different classes, and Categorical Cross-Entropy computes the loss between the predicted probability distribution and the actual one-hot encoded target. This pairing ensures that the output is interpreted as a class probability, and the loss is calculated based on the correct class.\n",
        "\n",
        "# This combination works well as long as the classes are mutually exclusive (i.e., the target variable can only take one class at a time).\n",
        "\n",
        "# Combination 4: ReLU + Categorical Cross-Entropy (Multi-Class Classification)\n",
        "# Activation Function: ReLU (for hidden layers)\n",
        "# Loss Function: Categorical Cross-Entropy\n",
        "# For multi-class classification, ReLU is commonly used in the hidden layers to help the network learn non-linear mappings. However, the Softmax activation function is typically used in the output layer for multi-class classification problems, not ReLU. ReLU is not suitable for the output layer in this case, because its output range is [0, âˆž), whereas Softmax ensures that the output is a probability distribution that sums to 1.\n",
        "\n",
        "# Thus, while ReLU works well in the hidden layers, for multi-class classification, Softmax is preferred in the output layer.\n",
        "\n",
        "# Combination 5: Tanh + MSE (Regression)\n",
        "# Activation Function: Tanh\n",
        "# Loss Function: Mean Squared Error (MSE)\n",
        "# The Tanh activation function is similar to Sigmoid but has a range of [-1, 1] rather than [0, 1]. It can be useful in regression problems where the target variable is expected to have both positive and negative values. However, Tanh is also prone to the vanishing gradient problem, especially in deep networks.\n",
        "\n",
        "# When used with MSE, Tanh can work reasonably well, but it may still suffer from slow convergence for deep networks. Additionally, ReLU might perform better in certain cases, as it doesn't saturate at large positive values.\n",
        "\n",
        "# Combination 6: Sigmoid + MSE (Regression)\n",
        "# Activation Function: Sigmoid\n",
        "# Loss Function: Mean Squared Error (MSE)\n",
        "# This combination can be problematic in regression tasks. The Sigmoid activation function outputs values in the range (0, 1), which is fine for binary outputs but restrictive for regression problems where the target can take any real value. When used with MSE, the model is forced to learn a regression task with outputs limited to the [0, 1] range, which can lead to poor performance unless the target variable is specifically normalized or constrained to that range.\n",
        "\n",
        "# Problematic Combinations\n",
        "# Sigmoid + Categorical Cross-Entropy: This combination is problematic because Sigmoid is typically used for binary classification, whereas Categorical Cross-Entropy requires a probability distribution across multiple classes, which is best provided by Softmax.\n",
        "\n",
        "# ReLU + Binary Cross-Entropy: ReLU outputs can be unbounded, and while it works well for regression tasks, it can be problematic for binary classification when used in the output layer. Sigmoid should be used in the output layer for binary classification to map the outputs to the (0, 1) range.\n",
        "\n",
        "# Choosing the Right Combination\n",
        "# For binary classification, the combination of Sigmoid activation and Binary Cross-Entropy loss is almost always the best choice.\n",
        "# For multi-class classification, Softmax in the output layer with Categorical Cross-Entropy loss is ideal.\n",
        "# For regression tasks, ReLU or Linear activation in the output layer with MSE loss works well, though care must be taken with negative targets when using ReLU."
      ],
      "metadata": {
        "id": "CAZBBbn1KgIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizers"
      ],
      "metadata": {
        "id": "DqmjwAPhK_EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the concept of optimization in the context of training neural networks. Why are optimizers important for the training process?\n",
        "# Ans: Optimization, in the context of training neural networks, refers to the process of adjusting the model's parameters (weights and biases) to minimize the loss function during training. The goal is to find the best set of parameters that allows the network to make accurate predictions or classifications, thus improving the model's performance. This process is fundamental to training any machine learning model, especially neural networks, as it directly influences how well the model will generalize to unseen data.\n",
        "\n",
        "# Why Optimization is Important in Training Neural Networks\n",
        "# Optimizers are crucial because they guide the network in updating its weights and biases efficiently, allowing the network to learn from the data and improve its predictions over time. The key goals of optimization in neural network training are:\n",
        "\n",
        "# Minimizing the Loss Function: The loss function quantifies how well the model is performing. The optimizer helps in minimizing this function by adjusting the model parameters to reduce the error between the predicted output and the actual target.\n",
        "\n",
        "# Efficient Learning: Optimizers help neural networks learn effectively by determining the step size (learning rate) and direction to take when updating parameters. Without proper optimization, training could be very slow, or the network might fail to converge to the optimal solution.\n",
        "\n",
        "# Avoiding Overfitting or Underfitting: By optimizing the loss function, optimizers help ensure that the model generalizes well to unseen data. If optimization is poor, the model may overfit (learn the noise in the training data) or underfit (fail to capture underlying patterns).\n",
        "\n",
        "# Key Concepts in Neural Network Optimization\n",
        "# Gradient Descent: The most commonly used optimization method in neural networks is Gradient Descent, which computes the gradient of the loss function with respect to each parameter and updates the parameters in the opposite direction of the gradient. This process is repeated iteratively until the loss function is minimized.\n",
        "\n",
        "# Stochastic Gradient Descent (SGD): A variation of gradient descent where the model parameters are updated using a randomly selected subset (mini-batch) of the data at each iteration. This speeds up training compared to using the entire dataset.\n",
        "\n",
        "# Batch Gradient Descent: In contrast to SGD, batch gradient descent computes the gradient over the entire dataset before updating the parameters. While it gives a more precise update, it can be computationally expensive.\n",
        "\n",
        "# Learning Rate: The learning rate controls the step size taken towards the minimum of the loss function. If the learning rate is too large, the model may overshoot the optimal solution, and if it's too small, training may be very slow.\n",
        "\n",
        "# Momentum: Momentum is a technique used to accelerate gradient descent by adding a fraction of the previous update to the current update. This helps the optimizer navigate plateaus and avoid getting stuck in local minima.\n",
        "\n",
        "# Adaptive Methods: Optimizers like AdaGrad, RMSProp, and Adam adjust the learning rate during training based on the gradient history. These methods are designed to adaptively scale the learning rate for each parameter, improving convergence in many cases, especially for non-convex loss functions.\n",
        "\n",
        "# Common Optimizers in Deep Learning\n",
        "# Stochastic Gradient Descent (SGD): The basic form of gradient descent, where parameters are updated using a small batch of data points. It can be slow but effective when combined with techniques like momentum.\n",
        "\n",
        "# Momentum: An extension of SGD, momentum adds a fraction of the previous update to the current one, which helps accelerate convergence and smoothens the updates.\n",
        "\n",
        "# RMSProp (Root Mean Square Propagation): An adaptive learning rate optimizer that divides the learning rate by a moving average of recent gradients. It helps stabilize the training process, particularly in tasks with noisy gradients.\n",
        "\n",
        "# Adam (Adaptive Moment Estimation): One of the most popular optimizers, Adam combines the advantages of both Momentum and RMSProp. It adapts the learning rate for each parameter by using both the first moment (mean) and the second moment (variance) of the gradients. Adam is widely used due to its efficiency and good performance across a variety of tasks.\n",
        "\n",
        "# AdaGrad: An adaptive learning rate algorithm that adjusts the learning rate for each parameter based on the past gradients. It can perform well on sparse data but might become too aggressive in reducing the learning rate during training."
      ],
      "metadata": {
        "id": "_zIgOks7LBcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Compare and contrast commonly used optimizers in deep learning, such as Stochastic Gradient Descent (SGD), Adam, RMSprop, and AdaGrad. What are the key differences between these optimizers, and when might you choose one over the others?\n",
        "# Ans: In deep learning, optimizers are responsible for updating the weights and biases of a model to minimize the loss function during training. Different optimizers use various strategies to adjust the learning rate and improve the training process. Below is a comparison of four commonly used optimizers: Stochastic Gradient Descent (SGD), Adam, RMSprop, and AdaGrad.\n",
        "\n",
        "# 1. Stochastic Gradient Descent (SGD)\n",
        "# Description:\n",
        "\n",
        "# SGD updates the model's parameters by calculating the gradient of the loss function using a single random sample (or mini-batch) of data.\n",
        "# The weights are updated after each data point (or mini-batch) instead of after going through the entire dataset (as in batch gradient descent).\n",
        "# Key Characteristics:\n",
        "\n",
        "# Learning Rate: Fixed learning rate.\n",
        "# Efficiency: Faster updates, but noisier convergence.\n",
        "# Convergence: Can oscillate and take a longer time to converge to the global minimum, especially if the learning rate is too large.\n",
        "# Pros:\n",
        "\n",
        "# Faster compared to batch gradient descent because updates are made after processing a single sample or mini-batch.\n",
        "# Less memory-intensive, as it doesn't need to calculate gradients over the entire dataset.\n",
        "# Cons:\n",
        "\n",
        "# The noisy updates make convergence slower and less stable.\n",
        "# It can get stuck in local minima due to its noisy nature.\n",
        "# When to Use:\n",
        "\n",
        "# Suitable when dealing with large datasets.\n",
        "# Works well when you combine it with techniques like Momentum to help stabilize convergence.\n",
        "# 2. Adam (Adaptive Moment Estimation)\n",
        "# Description:\n",
        "\n",
        "# Adam combines the benefits of Momentum and RMSprop. It calculates adaptive learning rates for each parameter by considering both the first moment (mean) and the second moment (variance) of the gradients.\n",
        "# Key Characteristics:\n",
        "\n",
        "# Learning Rate: Adaptive learning rates for each parameter.\n",
        "# Efficiency: Efficient, works well on sparse gradients, and is widely used.\n",
        "# Convergence: Generally converges faster than SGD due to adaptive learning rates.\n",
        "# Pros:\n",
        "\n",
        "# Combines the advantages of Momentum (using past gradients) and RMSprop (adapting to the scale of the gradients).\n",
        "# Works well for non-stationary problems and problems with sparse data (like NLP tasks or text).\n",
        "# Popular in deep learning and widely used in practice.\n",
        "# Cons:\n",
        "\n",
        "# May overfit in some cases, especially on small datasets.\n",
        "# The default hyperparameters (e.g., learning rates) may need adjustment based on the task.\n",
        "# When to Use:\n",
        "\n",
        "# General-purpose optimizer and often the first choice for most tasks.\n",
        "# Particularly useful in tasks involving large datasets and non-convex loss functions.\n",
        "# Sparse gradient problems (e.g., text or image classification).\n",
        "# 3. RMSprop (Root Mean Square Propagation)\n",
        "# Description:\n",
        "\n",
        "# RMSprop is an adaptive learning rate method that divides the learning rate by a moving average of recent gradients. It helps in stabilizing the training process and reducing the oscillations in the gradient descent process.\n",
        "# Key Characteristics:\n",
        "\n",
        "# Learning Rate: Adaptive, depending on the average of recent gradients.\n",
        "# Efficiency: Suitable for non-stationary objectives (e.g., recurrent neural networks).\n",
        "# Convergence: Faster convergence than plain SGD due to adaptive learning rates.\n",
        "# Pros:\n",
        "\n",
        "# Particularly effective for recurrent neural networks (RNNs) and tasks where the gradients change drastically (e.g., time-series data).\n",
        "# It is more stable than SGD, as the learning rate adapts to the scale of the gradients.\n",
        "# Works well on non-convex problems and noisy gradients.\n",
        "# Cons:\n",
        "\n",
        "# Can be sensitive to the choice of the learning rate parameter.\n",
        "# Less effective on tasks where the data is sparse or when dealing with long-term dependencies in data.\n",
        "# When to Use:\n",
        "\n",
        "# RNNs or time-series data.\n",
        "# Problems with highly fluctuating gradients (e.g., reinforcement learning).\n",
        "# Typically better than SGD when training on noisy data or non-stationary objectives.\n",
        "# 4. AdaGrad (Adaptive Gradient Algorithm)\n",
        "# Description:\n",
        "\n",
        "# AdaGrad adjusts the learning rate for each parameter based on the sum of the squared gradients. Parameters that have larger gradients will have smaller updates, while those with smaller gradients will have larger updates.\n",
        "# Key Characteristics:\n",
        "\n",
        "# Learning Rate: Adaptive per parameter.\n",
        "# Efficiency: Well-suited for sparse datasets (e.g., text data) because it adapts the learning rate for each parameter based on the historical gradient information.\n",
        "# Convergence: Can become too aggressive and diminish the learning rate too quickly, potentially leading to premature convergence.\n",
        "# Pros:\n",
        "\n",
        "# Effective for sparse data (e.g., in NLP or image recognition tasks).\n",
        "# It allows for faster learning for infrequent features while stabilizing learning for frequently occurring features.\n",
        "# Cons:\n",
        "\n",
        "# The learning rate can decay too quickly, causing the model to stop learning before it reaches the optimal solution.\n",
        "# Not ideal for non-sparse datasets, as it can lead to ineffective learning over time.\n",
        "# When to Use:\n",
        "\n",
        "# Suitable for tasks involving sparse data such as text mining, or when working with data that contains many infrequent features (e.g., high-dimensional datasets).\n",
        "# Not commonly used for dense data because the learning rate decays too quickly."
      ],
      "metadata": {
        "id": "sFoZVSw0LaE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Discuss the challenges associated with selecting an appropriate optimizer for a given deep learning task. How might the choice of optimizer affect the training dynamics and convergence of the neural network?\n",
        "# Ans: Selecting an appropriate optimizer for a deep learning task is a critical decision that can significantly impact the performance and convergence of a neural network. The choice of optimizer influences the speed of learning, the stability of training, and the model's ability to reach a global or satisfactory local minimum. Below are some key challenges associated with choosing an optimizer, as well as how this choice affects the training dynamics and convergence:\n",
        "\n",
        "# 1. Sensitivity to Learning Rate\n",
        "# Challenge: Optimizers like Stochastic Gradient Descent (SGD) and even Adam can be sensitive to the choice of the learning rate. An improperly tuned learning rate can cause the optimizer to either:\n",
        "# Converge too slowly (if the learning rate is too small),\n",
        "# Overshoot the minimum (if the learning rate is too large), or\n",
        "# Become unstable (if the learning rate is too variable).\n",
        "# Impact on Training: If the learning rate is not tuned correctly, the optimizer might get stuck in a local minimum or fail to converge, leading to poor model performance. Some optimizers, like Adam and RMSprop, adaptively adjust the learning rate, which can mitigate this challenge, but they also introduce hyperparameters (like decay rates) that require careful tuning.\n",
        "# 2. Convergence Behavior and Stability\n",
        "# Challenge: Different optimizers exhibit varied convergence behaviors:\n",
        "\n",
        "# SGD may exhibit slow convergence, especially with large datasets or noisy gradients.\n",
        "# Adam tends to converge faster in many cases but may sometimes overfit the model or converge to suboptimal points due to the nature of adaptive learning rates.\n",
        "# RMSprop stabilizes the learning process for non-stationary problems (e.g., RNNs) but can suffer from excessively small learning rates if the gradient scale becomes too small.\n",
        "# Impact on Training: The optimizer affects how quickly and stably the model reaches the minimum. SGD might be slower but can eventually reach a global minimum if tuned properly, while Adam might offer faster convergence but run the risk of overfitting in some cases. Optimizers that adapt the learning rate, like RMSprop and AdaGrad, can help achieve more stable training but might get stuck if the adaptive rate diminishes too much.\n",
        "\n",
        "# 3. Type of Data and Task (Task-Specific Requirements)\n",
        "# Challenge: The nature of the data and the task can make a particular optimizer more suitable. For example:\n",
        "\n",
        "# Sparse Data: For problems involving sparse data (such as NLP or image classification tasks with large vocabulary sizes), AdaGrad and Adam are preferable because they adjust the learning rate based on each parameterâ€™s gradient history.\n",
        "# Time-Series or Sequential Data: For sequential data like time-series forecasting or training recurrent neural networks (RNNs), RMSprop and Adam tend to perform better because they stabilize the learning process and adapt to changing gradients.\n",
        "# Impact on Training: Choosing an optimizer that is well-suited to the specific nature of the task ensures that the model can learn effectively and efficiently. For instance, using Adam in a classification task may help the model quickly converge, while RMSprop may be the better choice for sequence data due to its ability to handle fluctuating gradients.\n",
        "\n",
        "# 4. Impact of Momentum and Adaptive Learning Rates\n",
        "# Challenge: Some optimizers, like SGD with momentum, help smooth out updates, preventing the model from oscillating in the gradient descent process. Optimizers like Adam and RMSprop automatically adjust learning rates, making them more flexible. However, there is always the risk that the momentum or adaptive learning rates could lead the optimizer to overshoot or underadjust in certain situations, especially when gradients are sparse or highly variable.\n",
        "\n",
        "# Impact on Training: If the momentum is not properly tuned, it could either cause the model to overshoot minima (if the momentum is too high) or result in very slow convergence (if it is too low). Similarly, adaptive learning rates can become excessively small over time, leading to premature convergence or a failure to find the global minimum.\n",
        "\n",
        "# 5. Overfitting and Generalization\n",
        "# Challenge: Some optimizers, like Adam, can lead to faster convergence but at the cost of overfitting, especially on small datasets. This is because adaptive learning rates can cause the optimizer to settle into sharp minima, which might not generalize well to unseen data.\n",
        "\n",
        "# Impact on Training: Optimizers like SGD with momentum or RMSprop are less prone to overfitting as they explore the solution space more thoroughly. Adam, on the other hand, may converge quickly, but it could potentially overfit the model if the training time is not monitored or regularization techniques (such as dropout or weight decay) are not used.\n",
        "\n",
        "# 6. Computational Efficiency\n",
        "# Challenge: Optimizers that use momentum (such as SGD with momentum) or adaptive learning rates (such as Adam) require additional computation compared to simple SGD. Adam needs to compute and store first and second moment estimates for each parameter, increasing memory usage. This could be problematic for large models or on machines with limited memory.\n",
        "\n",
        "# Impact on Training: More complex optimizers may take more time per iteration and consume more memory, potentially slowing down training in resource-constrained environments. SGD, although simpler and computationally cheaper, may require more epochs to converge compared to Adam or RMSprop.\n",
        "\n",
        "# 7. Hyperparameter Tuning\n",
        "# Challenge: Optimizers, especially Adam and RMSprop, come with hyperparameters (e.g., decay rates, beta values, epsilon) that can significantly influence performance. Selecting appropriate hyperparameters requires experimentation, which can be time-consuming.\n",
        "\n",
        "# Impact on Training: A poor choice of hyperparameters can lead to slow or failed convergence. For example, the learning rate for Adam is often set to 0.001 by default, but this may not work well for every task. The learning rate for SGD needs to be carefully tuned, especially when combined with momentum."
      ],
      "metadata": {
        "id": "yeMsUfM7L63c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Implement a neural network for image classification using TensorFlow or PyTorch. Experiment with different optimizers and evaluate their impact on the training process and model performance. Provide insights into the advantages and disadvantages of each optimizer.\n",
        "# Ans: To implement a neural network for image classification and experiment with different optimizers, I'll use TensorFlow as an example, focusing on a simple image classification task using the Fashion MNIST dataset. We will experiment with different optimizers: SGD, Adam, and RMSprop, and evaluate their impact on training speed, accuracy, and loss.\n",
        "\n",
        "# Steps:\n",
        "# Load the Dataset\n",
        "# Preprocess the Data\n",
        "# Define the Neural Network\n",
        "# Train the Network Using Different Optimizers\n",
        "# Evaluate and Compare the Performance\n",
        "# 1. Load the Dataset\n",
        "# We'll use the Fashion MNIST dataset, which consists of 60,000 training images and 10,000 test images of 10 clothing categories.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Reshape the images to have a channel dimension\n",
        "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
        "test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))\n",
        "\n",
        "\n",
        "# 2. Define the Neural Network\n",
        "# We'll create a simple Convolutional Neural Network (CNN) for image classification.\n",
        "\n",
        "def create_model(optimizer):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# 3. Experiment with Different Optimizers\n",
        "# Now, we'll experiment with different optimizers: SGD, Adam, and RMSprop.\n",
        "\n",
        "# Define different optimizers\n",
        "optimizers = {\n",
        "    'SGD': tf.keras.optimizers.SGD(learning_rate=0.01),\n",
        "    'Adam': tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    'RMSprop': tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "}\n",
        "\n",
        "# Train and evaluate the model using each optimizer\n",
        "results = {}\n",
        "for opt_name, optimizer in optimizers.items():\n",
        "    print(f\"Training with {opt_name} optimizer...\")\n",
        "    model = create_model(optimizer)\n",
        "    history = model.fit(train_images, train_labels, epochs=10, batch_size=64, validation_data=(test_images, test_labels), verbose=2)\n",
        "    results[opt_name] = history\n",
        "\n",
        "\n",
        "# 4. Evaluate and Compare the Performance\n",
        "# After training the models with different optimizers, we can evaluate and compare their performance in terms of accuracy and loss.\n",
        "\n",
        "# Plot the training and validation accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for opt_name, history in results.items():\n",
        "    plt.plot(history.history['accuracy'], label=f'{opt_name} - Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label=f'{opt_name} - Validation Accuracy')\n",
        "\n",
        "plt.title('Comparison of Optimizers - Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for opt_name, history in results.items():\n",
        "    plt.plot(history.history['loss'], label=f'{opt_name} - Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label=f'{opt_name} - Validation Loss')\n",
        "\n",
        "plt.title('Comparison of Optimizers - Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 5. Insights into Optimizers\n",
        "# SGD:\n",
        "\n",
        "# Advantages: Simple and effective, especially for convex problems. Can converge to a global minimum if the learning rate is tuned properly.\n",
        "# Disadvantages: May converge slowly and get stuck in local minima in non-convex problems, such as deep learning tasks.\n",
        "# Use Case: Best used when you have a good understanding of the data and the model, and you can tune the learning rate effectively.\n",
        "# Adam:\n",
        "\n",
        "# Advantages: Combines the advantages of both AdaGrad and RMSprop. It adjusts the learning rate based on the first and second moments of the gradients, making it more robust to noisy data. Faster convergence compared to SGD.\n",
        "# Disadvantages: May overfit or converge too quickly to sharp minima.\n",
        "# Use Case: Often the go-to optimizer for general deep learning tasks, especially when dealing with sparse data, noisy gradients, or complex models like CNNs.\n",
        "# RMSprop:\n",
        "\n",
        "# Advantages: Good for problems with non-stationary objectives, such as training RNNs or when the data is highly noisy. Adapts the learning rate for each parameter.\n",
        "# Disadvantages: Might get stuck in a suboptimal solution if the learning rate becomes too small.\n",
        "# Use Case: Great for sequential data (RNNs, LSTMs) or other models where gradient magnitudes can vary significantly."
      ],
      "metadata": {
        "id": "Win1fqq4MbzY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Investigate the concept of learning rate scheduling and its relationship with optimizers in deep learning. How does learning rate scheduling influence the training process and model convergence? Provide examples of different learning rate scheduling techniques and their practical implications.\n",
        "# Ans: Learning rate scheduling refers to the strategy of adjusting the learning rate during the training process. The learning rate controls how much the model's weights are updated in response to the computed gradients. A high learning rate might lead to overshooting the optimal solution, while a low learning rate could slow down convergence, potentially getting stuck in suboptimal minima.\n",
        "\n",
        "# Learning rate scheduling is crucial in deep learning as it allows the model to converge more efficiently by dynamically adjusting the learning rate, often starting with a larger value and decreasing it gradually as the model approaches the optimal solution. This can lead to faster convergence and better overall performance by balancing the need for fast progress (at the beginning) and fine-tuning (as the model approaches the minimum).\n",
        "\n",
        "# Relationship Between Learning Rate and Optimizers\n",
        "# The choice of optimizer significantly affects how the learning rate interacts with the training process:\n",
        "\n",
        "# SGD (Stochastic Gradient Descent) is sensitive to the learning rate, and the scheduling of the learning rate can make a substantial difference in its performance.\n",
        "# Adam, RMSprop, and other adaptive optimizers adjust learning rates for individual parameters, but learning rate scheduling can still improve their performance, especially in tasks that require fine-tuning.\n",
        "# While adaptive optimizers (like Adam or RMSprop) inherently adjust the learning rates per parameter, global learning rate schedules can still help in improving convergence speed and avoiding overfitting, especially in more complex tasks.\n",
        "\n",
        "# How Learning Rate Scheduling Influences Training Process and Convergence\n",
        "# Faster Initial Training: By using a higher learning rate at the beginning of training, the optimizer can take large steps toward the minimum, speeding up the convergence process.\n",
        "# Better Fine-tuning: As the model begins to converge, lowering the learning rate allows the optimizer to make smaller, more refined updates to the weights, improving the precision of the model and helping to avoid overshooting the minimum.\n",
        "# Avoiding Overfitting: A decaying learning rate can act as a form of regularization, especially in later stages of training. It prevents the model from overfitting by gradually reducing the step size as the optimization nears convergence.\n",
        "# Examples of Learning Rate Scheduling Techniques\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "\n",
        "# Load and preprocess CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize images to [0, 1]\n",
        "y_train, y_test = tf.keras.utils.to_categorical(y_train, 10), tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Build a simple CNN model for classification\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define a learning rate scheduling function (Step Decay Example)\n",
        "def step_decay(epoch, lr):\n",
        "    drop = 0.5\n",
        "    epoch_drop = 5.0\n",
        "    return lr * drop ** (epoch // epoch_drop)\n",
        "\n",
        "# Define learning rate scheduler callback\n",
        "lr_scheduler = LearningRateScheduler(step_decay)\n",
        "\n",
        "# Train the model with learning rate scheduling\n",
        "history = model.fit(x_train, y_train, epochs=20,\n",
        "                    batch_size=64,\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    callbacks=[lr_scheduler])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f'Test accuracy: {test_acc:.4f}')\n"
      ],
      "metadata": {
        "id": "ttxYmpm1NcrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Explore the role of momentum in optimization algorithms, such as SGD with momentum and Adam. How does momentum affect the optimization process, and under what circumstances might it be beneficial or detrimental?\n",
        "# Ans: Momentum is a technique used in optimization algorithms to improve the speed and stability of training, particularly in gradient-based optimization methods like Stochastic Gradient Descent (SGD). It helps accelerate the convergence, especially in regions where the gradient changes slowly, and it also dampens oscillations during optimization. Momentum can be particularly useful in deep learning tasks where the optimization process might otherwise be slow or stuck in local minima.\n",
        "\n",
        "# Momentum Concept\n",
        "# Momentum involves accumulating a moving average of past gradients to smooth out the optimization process. This is similar to the physical concept of momentum, where an object in motion continues to move even after an external force is removed.\n",
        "\n",
        "# SGD with Momentum\n",
        "# In Stochastic Gradient Descent (SGD) with Momentum, the momentum term (ð‘£ð‘¡) helps the optimization to \"remember\" the direction of previous gradients, accelerating updates in directions that are consistent across multiple iterations.\n",
        "\n",
        "# Effect on Optimization: Momentum reduces the oscillations that can happen when the gradient is noisy or changes direction rapidly. It helps the model converge faster by carrying forward information from previous updates.\n",
        "\n",
        "# Benefit: It is particularly beneficial when the optimization landscape has steep, narrow regions (e.g., ravines) where gradients may be highly variable. Momentum can prevent the optimizer from getting stuck in small local minima and guide the optimization process more efficiently.\n",
        "\n",
        "# Detrimental Effect: If momentum is too high, the optimizer can overshoot the global minimum, especially in areas where the gradient is very steep, causing the training process to diverge. In such cases, a smaller value of momentum should be chosen.\n",
        "\n",
        "# Adam Optimizer and Momentum\n",
        "# Adam (short for Adaptive Moment Estimation) combines the benefits of both momentum and adaptive learning rates. It keeps track of two moving averages: the first moment (mean of gradients) and the second moment (uncentered variance of gradients).\n",
        "\n",
        "# When is Momentum Beneficial or Detrimental?\n",
        "# When is Momentum Beneficial?\n",
        "# In complex or noisy environments: Momentum can help navigate through noisy gradients, especially in problems like deep learning, where the gradients may be inconsistent.\n",
        "# In the presence of ravines: In regions with steep slopes and narrow valleys, like those often found in deep learning problems, momentum can help the optimizer avoid oscillating and reach the global minimum faster.\n",
        "# When training large models: With deep or complex models (e.g., CNNs or RNNs), momentum helps to stabilize and speed up convergence by leveraging past gradient information.\n",
        "# When is Momentum Detrimental?\n",
        "# When gradients change too rapidly: If the momentum is too high, it may cause the optimizer to overshoot the optimal solution and fail to converge properly, especially in regions with very steep gradients.\n",
        "# In shallow or simple models: For simple optimization landscapes, using too much momentum might lead to unnecessary complexity and overfitting.\n",
        "# When choosing hyperparameters: Fine-tuning the momentum parameter is important because if it is set too high (e.g., near 1.0), the model might fail to converge, while if it is too low, it might not benefit from the advantages of momentum."
      ],
      "metadata": {
        "id": "hvfzeSMHOfmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Discuss the importance of hyperparameter tuning in optimizing deep learning models. How do hyperparameters, such as learning rate and momentum, interact with the choice of optimizer? Propose a systematic approach for hyperparameter tuning in the context of deep learning optimization.\n",
        "# Ans: Hyperparameter tuning is a crucial step in the development and optimization of deep learning models. Hyperparameters are parameters that are set before the learning process begins and cannot be learned from the data directly (e.g., learning rate, momentum, batch size, number of layers, etc.). Proper tuning of these hyperparameters can significantly improve model performance, accelerate convergence, and help avoid overfitting or underfitting.\n",
        "\n",
        "# The choice of hyperparameters affects:\n",
        "\n",
        "# Model performance: Hyperparameters, like learning rate and momentum, directly influence the speed of learning and the ability to generalize.\n",
        "# Optimization efficiency: A well-tuned optimizer can speed up convergence and improve the stability of the training process.\n",
        "# Avoiding common issues: Incorrect settings can lead to issues like slow convergence, gradient vanishing/explosion, or oscillations in training.\n",
        "# Hyperparameters must be selected carefully because they impact the optimizerâ€™s effectiveness, which is responsible for adjusting the model's weights during training.\n",
        "\n",
        "# Interaction Between Hyperparameters and Optimizer\n",
        "# The choice of optimizer plays a significant role in how hyperparameters interact during training.\n",
        "\n",
        "# Learning Rate:\n",
        "\n",
        "# Role: The learning rate controls the size of the steps the optimizer takes when updating model parameters. A high learning rate can cause the optimizer to overshoot the optimal solution, while a low learning rate might make the training process very slow.\n",
        "# Interaction with Optimizer: Different optimizers handle the learning rate in various ways:\n",
        "# SGD: The learning rate is constant or adjusted with learning rate schedules.\n",
        "# Adam and RMSprop: These optimizers adapt the learning rate based on past gradients, so they can handle varying gradient magnitudes better than SGD.\n",
        "# Momentum:\n",
        "\n",
        "# Role: Momentum helps the optimizer maintain direction when the gradients are noisy or change rapidly, leading to smoother and faster convergence.\n",
        "# Interaction with Optimizer: Momentum works by accumulating a weighted moving average of the gradients from previous steps.\n",
        "# SGD with Momentum: Here, momentum helps avoid oscillations in the gradient direction and accelerates convergence.\n",
        "# Adam: Momentum in Adam is controlled by the first moment of the gradients (mean of the gradients), which is adaptive over time.\n",
        "# Batch Size:\n",
        "\n",
        "# Role: The batch size defines how many samples are processed before the model's parameters are updated. A larger batch size leads to more accurate gradient estimates but at the cost of more computation.\n",
        "# Interaction with Optimizer: A larger batch size may stabilize the optimizer, but it can increase training time. Smaller batches provide more noisy gradient estimates but tend to converge faster and might escape local minima more easily.\n",
        "# Weight Decay (L2 Regularization):\n",
        "\n",
        "# Role: Weight decay penalizes large weights and helps prevent overfitting by encouraging simpler models.\n",
        "# Interaction with Optimizer: Different optimizers handle regularization differently. For example, Adam implicitly adapts to regularization by adjusting the learning rate for each parameter, while SGD requires separate tuning of weight decay."
      ],
      "metadata": {
        "id": "6ueXTsgdPA0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assignment Questions on Forward and Backward Propagation"
      ],
      "metadata": {
        "id": "W0sfOJFdPZoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Explain the concept of forward propagation in a neural network.\n",
        "# Ans: Forward propagation is the process of passing input data through a neural network in order to generate an output. It is the fundamental operation that occurs during the training and inference phases of a neural network. The goal of forward propagation is to compute the predicted output of the model based on the input data and the current weights and biases of the network.\n",
        "\n",
        "# Steps in Forward Propagation:\n",
        "# Input Layer:\n",
        "\n",
        "# The input data is fed into the network. Each neuron in the input layer represents a feature of the dataset.\n",
        "# The input layer does not perform any computation; it simply passes the data to the next layer.\n",
        "# Weighted Sum:\n",
        "\n",
        "# Each neuron in a layer receives inputs from the previous layer, and these inputs are multiplied by a set of weights specific to that neuron.\n",
        "# The weighted sum of the inputs is calculated for each neuron in the layer.\n",
        "\n",
        "# Activation Function:\n",
        "\n",
        "# After computing the weighted sum, the output of the neuron is passed through an activation function.\n",
        "# The activation function introduces non-linearity to the network, allowing it to model complex relationships in the data.\n",
        "\n",
        "# Hidden Layers:\n",
        "\n",
        "# The process of calculating the weighted sum and applying the activation function is repeated for each hidden layer in the network.\n",
        "# The number of hidden layers and the number of neurons per layer depend on the architecture of the neural network.\n",
        "# Each hidden layer transforms the data and passes it to the next layer.\n",
        "# Output Layer:\n",
        "\n",
        "# The final hidden layer passes its output to the output layer.\n",
        "# For a classification task, the output layer typically uses a softmax activation function (for multi-class problems) or sigmoid (for binary classification) to map the output to probabilities.\n",
        "# For regression tasks, the output layer may not have an activation function or use a linear activation function to produce continuous values.\n",
        "# Final Output:\n",
        "\n",
        "# The output of the network is the prediction for the input data.\n",
        "# In a binary classification task, the output is a probability value between 0 and 1 (if using a sigmoid activation). For multi-class classification, the output is a vector of probabilities (if using softmax).\n"
      ],
      "metadata": {
        "id": "_2oh5l7hPdCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. What is the purpose of the activation function in forward propagation?\n",
        "# Ans: The purpose of the activation function in forward propagation is to introduce non-linearity into the neural network, enabling it to model complex relationships and patterns in the data. Without an activation function, the neural network would essentially behave like a linear regression model, regardless of the number of layers, which severely limits its ability to learn from complex datasets.\n",
        "\n",
        "# Key Functions of the Activation Function:\n",
        "# Introduce Non-Linearity:\n",
        "\n",
        "# Real-world data is often non-linear, and many tasks such as image recognition, language modeling, and classification require the ability to model these complex, non-linear relationships.\n",
        "# The activation function allows the network to learn and represent these non-linearities. Without it, the network would be unable to capture complex patterns in data.\n",
        "# Enabling Deep Learning:\n",
        "\n",
        "# In deep neural networks, where multiple layers are used, the activation function helps each layer to transform the input data in such a way that deeper layers can learn more abstract features.\n",
        "# A deep network without activation functions would just perform a series of linear transformations, limiting its ability to learn hierarchical features.\n",
        "# Control Output Range:\n",
        "\n",
        "# Many activation functions, such as sigmoid and tanh, constrain the output of each neuron within a fixed range, which can help with numerical stability during training.\n",
        "# For example, the sigmoid function compresses its output to a range between 0 and 1, which is particularly useful in classification tasks (e.g., binary classification) when representing probabilities.\n",
        "# Sparsity and Efficiency (for ReLU):\n",
        "\n",
        "# The ReLU (Rectified Linear Unit) activation function, which outputs zero for negative inputs and the input itself for positive inputs, promotes sparsity in the network (many activations being zero).\n",
        "# This can lead to more efficient computation, as well as help the model focus on the most important features.\n",
        "# Gradient Propagation (for Training):\n",
        "\n",
        "# In the training phase, during backpropagation, the gradients are computed with respect to the activation function. Some activation functions, like ReLU, help avoid the vanishing gradient problem by maintaining larger gradients for positive inputs.\n",
        "# Activation functions allow the model to adjust weights in the right direction during gradient descent, ultimately improving the model's performance."
      ],
      "metadata": {
        "id": "aoofTM41P0ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Describe the steps involved in the backward propagation (backpropagation) algorithm.\n",
        "# Ans: Backpropagation is the process used to optimize the weights of a neural network by calculating the gradient of the loss function with respect to each weight using the chain rule of calculus. This enables the model to adjust its weights to minimize the error, improving the modelâ€™s performance over time.\n",
        "\n",
        "# The backpropagation algorithm is essential for training deep neural networks and involves the following steps:\n",
        "\n",
        "# 1. Forward Propagation (Initial Step)\n",
        "# Before backpropagation begins, forward propagation must be performed to calculate the output of the network, which will later be compared to the actual target to compute the error (loss).\n",
        "\n",
        "# Input: The input features are passed through the network layer by layer, where weighted sums are calculated at each neuron, followed by the activation function.\n",
        "# Output: The final output is generated, which is compared to the true label using a loss function (e.g., Mean Squared Error, Cross-Entropy).\n",
        "# 2. Calculate the Loss (Error)\n",
        "# Once the forward pass is completed, the output of the network is compared with the actual target to calculate the error or loss. The loss function quantifies how far the model's predictions are from the actual labels."
      ],
      "metadata": {
        "id": "g_JS1RfUQywi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. What is the purpose of the chain rule in backpropagation?\n",
        "# Ans: The chain rule plays a crucial role in backpropagation because it allows us to compute the gradient of the loss function with respect to each weight in the network, especially when dealing with the multiple layers and non-linearities inherent in deep neural networks.\n",
        "\n",
        "# Purpose of the Chain Rule in Backpropagation\n",
        "# Calculating Gradients Efficiently:\n",
        "\n",
        "# Backpropagation involves computing gradients of the loss function with respect to all the weights in the network. The chain rule enables the calculation of these gradients layer by layer, starting from the output layer and propagating backward through the hidden layers.\n",
        "# Each layer's output depends on the previous layer's output, and the chain rule allows us to break down complex dependencies into simpler, manageable parts.\n",
        "# Breaking Down Complex Derivatives:\n",
        "\n",
        "# In a neural network, the output of each neuron is a function of the weighted sum of the inputs and an activation function. To update the weights, we need the derivative of the loss function with respect to each weight.\n",
        "# The chain rule helps in decomposing the derivative of the loss into simpler parts by relating the gradients of the loss function at each layer to the gradients of the activations and weights at that layer."
      ],
      "metadata": {
        "id": "VDOOYAQfRHFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Implement the forward propagation process for a simple neural network with one hidden layer using NumPy.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of sigmoid (for backpropagation, not needed for forward propagation but included for completeness)\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Initialize neural network parameters\n",
        "input_size = 3  # number of input features\n",
        "hidden_size = 4  # number of hidden neurons\n",
        "output_size = 1  # single output for binary classification\n",
        "\n",
        "# Randomly initialize weights and biases\n",
        "np.random.seed(42)  # for reproducibility\n",
        "weights_input_hidden = np.random.rand(input_size, hidden_size)  # weights from input to hidden\n",
        "bias_hidden = np.random.rand(1, hidden_size)  # bias for hidden layer\n",
        "weights_hidden_output = np.random.rand(hidden_size, output_size)  # weights from hidden to output\n",
        "bias_output = np.random.rand(1, output_size)  # bias for output layer\n",
        "\n",
        "# Example input\n",
        "X = np.array([[0.5, 0.2, 0.8]])  # Single training example with 3 features\n",
        "\n",
        "# Forward Propagation Process\n",
        "# 1. Input to Hidden Layer\n",
        "hidden_input = np.dot(X, weights_input_hidden) + bias_hidden  # weighted sum + bias\n",
        "hidden_output = sigmoid(hidden_input)  # Apply sigmoid activation\n",
        "\n",
        "# 2. Hidden to Output Layer\n",
        "output_input = np.dot(hidden_output, weights_hidden_output) + bias_output  # weighted sum + bias\n",
        "output = sigmoid(output_input)  # Apply sigmoid activation\n",
        "\n",
        "# Print the results\n",
        "print(\"Input to Hidden Layer (weighted sum + bias):\\n\", hidden_input)\n",
        "print(\"Output of Hidden Layer (after activation):\\n\", hidden_output)\n",
        "print(\"Input to Output Layer (weighted sum + bias):\\n\", output_input)\n",
        "print(\"Final Output (after activation):\\n\", output)\n"
      ],
      "metadata": {
        "id": "5Zeo6xexRWI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assignment on weight initialization techniques"
      ],
      "metadata": {
        "id": "VPlB_4OqRoBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. What is the vanishing gradient problem in deep neural networks? How does it affect training?\n",
        "# Ans: The vanishing gradient problem is a significant issue that arises during the training of deep neural networks, especially when using activation functions like sigmoid or tanh in deep networks. It refers to the situation where the gradients (or the derivatives of the loss function with respect to the model parameters) become exceedingly small as they are propagated backward through the layers during backpropagation.\n",
        "\n",
        "# How the Vanishing Gradient Problem Affects Training:\n",
        "# Small Gradients: During backpropagation, the gradients of the loss with respect to the weights are calculated by applying the chain rule. When using certain activation functions, such as sigmoid or tanh, the derivatives of these functions are small (for example, sigmoid's derivative is between 0 and 0.25). In deep networks, the gradients for layers near the input tend to get smaller and smaller as they are passed backward through each layer, leading to vanishing gradients.\n",
        "\n",
        "# Slow or Stalled Training: As the gradients shrink, the model's weights for the earlier layers (closer to the input) are updated very little. This means that the weights of these layers do not change significantly, and as a result, the network fails to learn effectively. The model may stop improving and converge too slowly or not at all.\n",
        "\n",
        "# Poor Convergence: In deep networks, this issue becomes especially pronounced, as the network depth increases. The further a layer is from the output, the smaller the gradient becomes, making it hard for the network to learn the appropriate representations."
      ],
      "metadata": {
        "id": "btbJT18GRsyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Explain how Xavier initialization addresses the vanishing gradient problem.\n",
        "# Ans: Xavier Initialization (also known as Glorot Initialization) is a technique designed to help address the vanishing gradient problem in deep neural networks by ensuring that the gradients neither vanish nor explode as they are propagated through the network during backpropagation.\n",
        "# How Xavier Initialization Works:\n",
        "# Xavier initialization aims to balance the scale of the input and output of each layer in a neural network so that the variance of the activations (outputs) and the gradients remain the same throughout the layers. This prevents the gradients from becoming too small (vanishing) or too large (exploding), which can otherwise hinder the training of deep networks.\n",
        "\n",
        "# The Key Idea Behind Xavier Initialization:\n",
        "# In deep neural networks, weights are the parameters that need to be updated during training, and their initialization can significantly affect how well the network trains. If weights are initialized poorly (e.g., too small or too large), it can lead to problems with gradient flow during backpropagation.\n",
        "\n",
        "# Small weights can cause gradients to vanish (especially with activation functions like sigmoid and tanh) because the output of each layer becomes too small.\n",
        "# Large weights can cause gradients to explode, leading to unstable training and large updates.\n",
        "# Xavier initialization seeks to prevent both vanishing and exploding gradients by choosing an appropriate variance for the initial weights."
      ],
      "metadata": {
        "id": "nenQZOZpSLAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. What are some common activation functions that are prone to causing vanishing gradients?\n",
        "# Ans: Some common activation functions that are prone to causing vanishing gradients, particularly in deep neural networks, include:\n",
        "\n",
        "# 1. Sigmoid Activation Function:\n",
        "# Problem: The sigmoid function squashes its input to a range between 0 and 1, which means that for very large positive or negative inputs, the gradient of the function becomes very small (near 0). This is especially problematic when used in deep networks, where the gradients diminish as they propagate backward, leading to slow or stalled training (vanishing gradients).\n",
        "# Effect on Training: In deep networks, the gradients can vanish during backpropagation, making it difficult for the model to update the weights in the earlier layers effectively.\n",
        "\n",
        "# 2. Tanh Activation Function:\n",
        "# Problem: Like the sigmoid function, the tanh function squashes its input, but to a range of -1 to 1. For large positive or negative values of the input, the gradient approaches 0. While tanh typically has a wider output range than sigmoid, it still suffers from the vanishing gradient problem when inputs are far from 0.\n",
        "# Effect on Training: In deep networks, tanh can still cause vanishing gradients, especially when the network has many layers. This can result in very small weight updates during training, preventing the network from learning effectively.\n",
        "\n",
        "# 3. Softmax Activation Function (in output layer for multi-class classification):\n",
        "# Problem: The softmax activation function normalizes the output of a network to produce probabilities for multi-class classification. While it is not prone to vanishing gradients in the same way as sigmoid and tanh, the gradient can become very small when the predicted probability is near 0 or 1 (i.e., when the network is very confident in its prediction). This can lead to very slow learning if the network is highly confident but incorrect, which can cause training difficulties.\n",
        "# Effect on Training: The vanishing gradients can occur during backpropagation, particularly in cases where the network is already highly confident (close to 0 or 1) in its predictions, leading to slow or unresponsive updates."
      ],
      "metadata": {
        "id": "cAgqe4A-Saym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Define the exploding gradient problem in deep neural networks. How does it impact training?\n",
        "# Ans: The exploding gradient problem occurs when the gradients during backpropagation become excessively large, causing unstable updates to the weights during the training process of a neural network. This issue is most commonly observed in deep neural networks with many layers or in networks with certain activation functions and weight initialization schemes.\n",
        "\n",
        "# How It Happens:\n",
        "# During backpropagation, the gradients are propagated backward through the layers to update the weights. If the gradients are very large, they cause large updates to the weights. This makes the network's weights grow too quickly, leading to instability. Over time, the network's weights can grow so large that the model's behavior becomes unpredictable, and it fails to converge.\n",
        "\n",
        "# The exploding gradient problem is often caused by:\n",
        "\n",
        "# Deep networks: When training deep neural networks with many layers, gradients can multiply during backpropagation. If the network has certain activation functions or weight initialization schemes that cause large values, the gradients can quickly grow larger and larger as they propagate backward, resulting in unstable training.\n",
        "\n",
        "# Activation functions: Some activation functions (like ReLU) can cause large gradients for large input values, which might lead to the explosion of gradients.\n",
        "\n",
        "# Improper weight initialization: If the initial weights of the neural network are too large, it can cause the gradients to explode during the forward and backward passes. Large weights lead to large activations, and as the gradients propagate backward, they can multiply and grow exponentially.\n",
        "\n",
        "# Impact on Training:\n",
        "# The exploding gradient problem significantly impacts training in the following ways:\n",
        "\n",
        "# Unstable Weights: During backpropagation, excessively large gradients cause weight updates that are too large. These large updates can result in weights that grow rapidly, making it difficult for the network to converge to an optimal solution.\n",
        "\n",
        "# Loss Divergence: The loss function, which is minimized during training, can \"diverge\" or increase drastically instead of decreasing. This happens because the large updates lead to extreme predictions, which cause the loss to become very large.\n",
        "\n",
        "# Numerical Instability: Large gradients can cause numerical issues, such as overflow or invalid values (NaN, Not a Number), leading to computation errors. This makes the model impossible to train effectively.\n",
        "\n",
        "# Slow Convergence: Even if the network doesn't completely diverge, the overly large gradients can cause the model to converge very slowly or not at all, as the optimizer makes inconsistent updates that don't lead to stable improvement in the loss function."
      ],
      "metadata": {
        "id": "3efhb3D2S1bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. What is the role of proper weight initialization in training deep neural networks?\n",
        "# Ans: Weight initialization plays a crucial role in the successful training of deep neural networks. The values assigned to the initial weights determine the starting point of the training process, which can have a significant impact on the network's ability to converge to an optimal solution. Proper initialization helps avoid common issues like vanishing gradients and exploding gradients, which can otherwise severely hinder the learning process.\n",
        "\n",
        "# Key Roles of Proper Weight Initialization\n",
        "# Avoiding Vanishing and Exploding Gradients:\n",
        "\n",
        "# In deep neural networks, especially with many layers, improper weight initialization can cause gradients to either vanish (become too small) or explode (become too large) during backpropagation.\n",
        "# Vanishing gradients occur when small weights lead to very small gradient values, which decay as they propagate backward, making it hard for the model to learn in the deeper layers. This can result in very slow or stalled training.\n",
        "# Exploding gradients occur when large weights cause large gradient values, leading to very large weight updates and causing instability or divergence in training.\n",
        "# Ensuring Symmetry Breaking:\n",
        "\n",
        "# Symmetry breaking is essential to avoid the issue where all neurons in a layer learn the same features during training, which would prevent the network from learning diverse and useful patterns.\n",
        "# If all the weights are initialized to the same value (e.g., zero), the neurons in the layer would receive the same gradients during backpropagation and would update identically, making them indistinguishable from one another. Proper initialization ensures that each neuron learns different features, enabling effective learning.\n",
        "# Improving Convergence Speed:\n",
        "\n",
        "# Proper weight initialization helps in achieving faster convergence by allowing the network to start from a good point in the optimization landscape.\n",
        "# Starting from well-chosen initial weights can reduce the number of iterations needed for the network to find a good local minimum, speeding up the overall training process.\n",
        "# Preventing Dead Neurons:\n",
        "\n",
        "# Some activation functions (e.g., ReLU) can suffer from the \"dead neuron\" problem when large or small initial weights cause certain neurons to always output the same value (e.g., 0 for ReLU). Proper initialization prevents such situations, ensuring that neurons are active and contribute to learning."
      ],
      "metadata": {
        "id": "658Hsjd8TfCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Explain the concept of batch normalization and its impact on weight initialization techniques.\n",
        "# Ans: Batch Normalization (BN) is a technique used to improve the training of deep neural networks by normalizing the inputs to each layer. The main idea behind batch normalization is to reduce internal covariate shift, which occurs when the distribution of inputs to a layer changes during training as the parameters of the previous layers are updated. By normalizing the inputs to each layer, BN stabilizes the training process, speeds up convergence, and improves model performance.\n",
        "\n",
        "# Impact of Batch Normalization on Weight Initialization\n",
        "# Batch normalization has a significant impact on weight initialization strategies because it mitigates some of the issues caused by poor initialization. Specifically:\n",
        "\n",
        "# Alleviating the Vanishing and Exploding Gradient Problems:\n",
        "\n",
        "# Batch normalization reduces the impact of the vanishing gradients and exploding gradients problems. By ensuring that the inputs to each layer have a standard distribution (zero mean and unit variance), it stabilizes the gradient flow during backpropagation, making training more stable.\n",
        "# As a result, the model can tolerate larger initial weights, making techniques like Xavier or He initialization more effective even in deep networks.\n",
        "# Relaxing Initialization Requirements:\n",
        "\n",
        "# Without batch normalization, careful weight initialization is critical to prevent poor training dynamics, especially for deep networks where small gradients can vanish across layers.\n",
        "# However, with batch normalization, the reliance on precise weight initialization is reduced because BN ensures that the inputs to each layer are normalized. This means that you can use a wider range of initializations (e.g., random initialization) without worrying as much about training stability.\n",
        "# Effect on Convergence Speed:\n",
        "\n",
        "# Batch normalization can lead to faster convergence because it reduces internal covariate shift and stabilizes the optimization process. This reduces the need for careful tuning of the learning rate, and the optimizer can make more effective updates, leading to quicker progress during training."
      ],
      "metadata": {
        "id": "sWFZ_hf4UC4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Implement He initialization in Python using TensorFlow or PyTorch.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define a simple neural network using He initialization\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal(), input_shape=(128,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu', kernel_initializer=tf.keras.initializers.HeNormal()),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define a simple neural network using He initialization\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(128, 64)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(64, 32)   # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(32, 1)    # Output layer for binary classification\n",
        "\n",
        "        # Apply He initialization (kaiming_normal_)\n",
        "        torch.nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
        "        torch.nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='relu')\n",
        "        torch.nn.init.kaiming_normal_(self.fc3.weight, nonlinearity='relu')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))  # Output layer for binary classification\n",
        "        return x\n",
        "\n",
        "# Create an instance of the model\n",
        "model = SimpleNN()\n",
        "\n",
        "# Print the model summary (requires torchsummary library for summary function)\n",
        "from torchsummary import summary\n",
        "summary(model, (128,))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "NckOxjKaUZXR",
        "outputId": "ca6a2e50-efcf-4a48-9246-fcf667d0b5ba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ dense_22 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  â”‚           \u001b[38;5;34m8,256\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_23 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  â”‚           \u001b[38;5;34m2,080\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_24 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   â”‚              \u001b[38;5;34m33\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,369\u001b[0m (40.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,369</span> (40.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,369\u001b[0m (40.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,369</span> (40.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                   [-1, 64]           8,256\n",
            "            Linear-2                   [-1, 32]           2,080\n",
            "            Linear-3                    [-1, 1]              33\n",
            "================================================================\n",
            "Total params: 10,369\n",
            "Trainable params: 10,369\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 0.04\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assignment questions on Vanishing Gradient Problem:"
      ],
      "metadata": {
        "id": "cJxnishBUwH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the vanishing gradient problem and the exploding gradient problem in the context of training deep neural networks. What are the underlying causes of each problem?\n",
        "# Ans: The vanishing gradient problem occurs during the training of deep neural networks when the gradients (used to update the weights) become very small as they are propagated backward through the network. This makes it difficult for the model to learn, especially for layers near the input.\n",
        "\n",
        "# Causes of the Vanishing Gradient Problem:\n",
        "# Saturated Activation Functions:\n",
        "\n",
        "# Activation functions like the Sigmoid or Tanh are prone to saturating when the input values are too large or too small (i.e., their outputs approach the extreme ends of their range: 0 or 1 for Sigmoid, and -1 or 1 for Tanh). When this happens, their gradients become very small (close to 0), leading to diminishing updates during backpropagation, especially for deep networks.\n",
        "# Deep Networks:\n",
        "\n",
        "# In very deep networks, gradients can shrink exponentially as they are propagated back through many layers, especially if the weights are initialized poorly. This leads to a situation where the earlier layers receive almost no gradient and, therefore, their weights donâ€™t update significantly.\n",
        "# Weight Initialization:\n",
        "\n",
        "# Poor weight initialization can also contribute to the vanishing gradient problem. For example, initializing weights too small can cause the gradients to become smaller with each layer, making it harder for the model to learn effectively.\n",
        "# Impact:\n",
        "# The model struggles to update the weights of the earlier layers, making it difficult for the network to learn useful features, leading to slow convergence or failure to converge.\n",
        "# Exploding Gradient Problem\n",
        "# The exploding gradient problem is the opposite of the vanishing gradient problem. It happens when the gradients become excessively large during backpropagation, leading to extremely large weight updates and causing the model to fail to converge.\n",
        "\n",
        "# Causes of the Exploding Gradient Problem:\n",
        "# Large Gradients:\n",
        "\n",
        "# If the weights are initialized with very large values or if the network is too deep, the gradients can grow exponentially during backpropagation, causing the weight updates to become extremely large. This causes the model to behave erratically and may result in NaN values or overflow in the learning process.\n",
        "# Uncontrolled Activation Functions:\n",
        "\n",
        "# Certain activation functions (e.g., ReLU) may not inherently cause exploding gradients but can become problematic when used in deep networks without proper normalization or when the learning rate is too high.\n",
        "# Improper Weight Initialization:\n",
        "\n",
        "# Initializing the weights with large values or using poor initialization strategies (e.g., random values without any scaling) can cause gradients to grow exponentially during training.\n",
        "# Impact:\n",
        "# The model's weights can grow too large, causing numerical instability. This leads to exploding gradients, making the training process unstable, and the model may not converge or even diverge completely.\n",
        "\n",
        "# Key Differences Between Vanishing and Exploding Gradients\n",
        "# Vanishing Gradients:\n",
        "# Gradients become very small as they propagate backward through the network.\n",
        "# Leads to very small updates to weights, especially in deeper layers.\n",
        "# Makes it hard for the network to learn, especially for deep networks.\n",
        "# Exploding Gradients:\n",
        "# Gradients grow exponentially as they propagate backward.\n",
        "# Causes large updates to weights, leading to instability in the learning process.\n",
        "# Results in numerical issues (NaN values or overflow) and makes the network difficult to train."
      ],
      "metadata": {
        "id": "m-jeK1tOUzxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Discuss the implications of the vanishing gradient problem and the exploding gradient problem on the training process of deep neural networks. How do these problems affect the convergence and stability of the optimization process?\n",
        "# Ans: The vanishing gradient problem significantly affects the convergence and stability of the training process in deep neural networks. It arises when gradients shrink as they are propagated backward through the network, leading to smaller and smaller updates to the weights, particularly in the earlier layers of the network.\n",
        "\n",
        "# Impact on Convergence:\n",
        "# Slow or Stagnant Learning: Since the gradients become too small, the updates to the weights become negligible, especially for layers that are further from the output layer. This causes the network to converge very slowly, or in some cases, the learning process can completely stagnate, making it impossible to learn useful features from the data.\n",
        "# Inefficient Training: In deep networks, especially those with many layers, the vanishing gradient problem leads to inefficient training. While the later layers (closer to the output) can still learn, the earlier layers stop improving, which prevents the network from learning complex representations effectively.\n",
        "# Impact on Stability:\n",
        "# Unstable Training: The diminishing gradients may cause instability in the weight updates, as weights in the earlier layers are not updated adequately. This leads to poor generalization and model performance.\n",
        "# Difficulty in Fine-Tuning: In tasks that require fine-tuning (such as transfer learning), the vanishing gradient problem exacerbates the issue, as smaller weight updates prevent the network from learning small but critical details of new data.\n",
        "\n",
        "# How These Problems Affect the Optimization Process\n",
        "# Vanishing Gradient:\n",
        "# Gradient Descent Inefficiency: Standard gradient descent relies on the gradient to update model parameters. With vanishing gradients, the model fails to make meaningful updates, resulting in extremely slow or halted learning. In the case of deep networks, the optimization process may never converge to the optimal solution.\n",
        "# Saturation of Activation Functions: Saturation in activation functions like Sigmoid and Tanh is a common cause of vanishing gradients, especially when these functions output values near 0 or 1. These functions have small gradients in these regions, which contributes to the vanishing gradient problem.\n",
        "# Difficulty in Training Deep Networks: As the network deepens, the gradients become smaller with each layer, causing optimization to become more difficult. This is especially problematic in recurrent neural networks (RNNs), where the vanishing gradient problem can occur across time steps.\n",
        "# Exploding Gradient:\n",
        "# Large Weight Updates: The optimizer will make massive updates to weights during backpropagation, resulting in excessively large changes in the model parameters. This often leads to instability, as the learning rate becomes too large, and the optimizer cannot effectively find the minimum of the loss function.\n",
        "# Divergence of Training: The exploding gradient problem can lead to divergence rather than convergence. In this case, the optimization process is not guided toward the optimal solution, and instead, the loss function continues to increase or oscillate wildly.\n",
        "# Infeasible Loss Function Behavior: The instability caused by exploding gradients results in unpredictable behavior in the loss function, which may cause the model to fail to achieve any meaningful improvement during training."
      ],
      "metadata": {
        "id": "sJMYiaejVPmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Explore the role of activation functions in mitigating the vanishing gradient problem and the exploding gradient problem. How do activation functions such as ReLU, sigmoid, and tanh influence gradient flow during backpropagation?\n",
        "# Ans: Activation functions play a crucial role in mitigating both the vanishing gradient problem and the exploding gradient problem in deep neural networks. Their influence on gradient flow during backpropagation significantly affects the training process and stability of the optimization.\n",
        "\n",
        "# 1. ReLU (Rectified Linear Unit)\n",
        "# ReLU is one of the most commonly used activation functions, and it plays a key role in mitigating the vanishing gradient problem, while it can sometimes be involved in the exploding gradient problem if used with inappropriate weight initialization or high learning rates.\n",
        "\n",
        "# 2. Sigmoid Activation Function\n",
        "# The Sigmoid activation function is another common activation used in neural networks, especially in the output layer for binary classification problems. However, it is more prone to the vanishing gradient problem compared to ReLU.\n",
        "\n",
        "# 3. Tanh (Hyperbolic Tangent) Activation Function\n",
        "# The tanh activation function is similar to sigmoid, but it outputs values in the range of -1 to 1, rather than 0 to 1. While tanh mitigates some of the limitations of the sigmoid, it can still suffer from the vanishing gradient problem.\n",
        "\n",
        "# Role of Activation Functions in Mitigating the Gradient Problems\n",
        "# ReLU (and its variants like Leaky ReLU, Parametric ReLU): These activation functions are effective at mitigating the vanishing gradient problem due to their non-saturating nature (for positive inputs). By maintaining a constant gradient for positive inputs, ReLU ensures efficient weight updates and faster convergence. However, it can still face the exploding gradient problem in certain situations, which can be mitigated through careful weight initialization and gradient clipping.\n",
        "\n",
        "# Sigmoid and Tanh: These functions are prone to the vanishing gradient problem, especially for deep networks. As the gradients shrink, the learning process becomes slower and less effective. They are less effective for deep networks but might still be used in simpler or smaller networks, or in the output layers for binary classification tasks (sigmoid) and regression tasks (tanh)."
      ],
      "metadata": {
        "id": "zw9MxZazV3Kx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}